{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NiFi Processor Usage Analyzer - Databricks Edition\n",
    "\n",
    "This notebook analyzes NiFi processor execution counts to identify unused or underutilized processors.\n",
    "\n",
    "**Features:**\n",
    "- Fast execution count analysis (~5-10 seconds)\n",
    "- Snapshot mode for time-series tracking\n",
    "- Delta Lake integration for historical analysis\n",
    "- Standalone - no external files needed\n",
    "\n",
    "**Setup:**\n",
    "1. Edit the configuration in Cell 3\n",
    "2. Run all cells\n",
    "3. View results in output and Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "# This installs packages for the current notebook session\n",
    "\n",
    "%pip install requests rich --quiet\n",
    "\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "# Databricks-specific imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "# Disable SSL warnings if verify_ssl=False\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('nifi_analyzer')\n",
    "\n",
    "# Initialize Rich console for pretty output\n",
    "console = Console()\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "# EDIT THESE VALUES FOR YOUR NIFI INSTANCE\n",
    "\n",
    "CONFIG = {\n",
    "    # NiFi Connection\n",
    "    'nifi_url': 'https://thbnk01hdpnp002.th-bnk01.nxp.com:8443/nifi',  # Your NiFi URL (without /api)\n",
    "    'username': 'nxg16670',                                              # Your NiFi username\n",
    "    'password': 'your-password-here',                                    # Your NiFi password\n",
    "    'verify_ssl': False,                                                 # Set to True if you have valid SSL\n",
    "    \n",
    "    # Analysis Parameters\n",
    "    'process_group_id': '8c8677c4-29d6-36...',                          # Process group ID to analyze\n",
    "    \n",
    "    # Snapshot Storage (optional)\n",
    "    'enable_snapshots': True,                                            # Save snapshots to Delta Lake?\n",
    "    'delta_table_name': 'nifi_processor_snapshots',                     # Delta table name\n",
    "    'delta_database': 'default',                                         # Database name\n",
    "}\n",
    "\n",
    "console.print(\"[green]✓ Configuration loaded![/green]\")\n",
    "console.print(f\"  NiFi URL: {CONFIG['nifi_url']}\")\n",
    "console.print(f\"  Username: {CONFIG['username']}\")\n",
    "console.print(f\"  Process Group ID: {CONFIG['process_group_id'][:16]}...\")\n",
    "console.print(f\"  Snapshots enabled: {CONFIG['enable_snapshots']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: NiFi Client Class\n",
    "# Handles all NiFi REST API interactions\n",
    "\n",
    "class NiFiClient:\n",
    "    \"\"\"\n",
    "    Client for interacting with Apache NiFi REST API.\n",
    "    Handles authentication and common API operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, username: str, password: str, verify_ssl: bool = True):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.api_url = f\"{self.base_url}-api\"\n",
    "        self.verify_ssl = verify_ssl\n",
    "        self.session = requests.Session()\n",
    "        self.token = None\n",
    "        \n",
    "        # Authenticate\n",
    "        self._authenticate(username, password)\n",
    "        \n",
    "    def _authenticate(self, username: str, password: str) -> None:\n",
    "        \"\"\"Authenticate with NiFi using username/password.\"\"\"\n",
    "        logger.debug(f\"Attempting token authentication at {self.api_url}/access/token\")\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/access/token\",\n",
    "            data={'username': username, 'password': password},\n",
    "            verify=self.verify_ssl\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 201:\n",
    "            self.token = response.text\n",
    "            self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n",
    "            logger.info(\"Successfully authenticated with token-based auth\")\n",
    "        else:\n",
    "            raise Exception(f\"Authentication failed: {response.status_code} - {response.text}\")\n",
    "    \n",
    "    def _request(self, method: str, endpoint: str, **kwargs) -> requests.Response:\n",
    "        \"\"\"Make authenticated request to NiFi API.\"\"\"\n",
    "        url = f\"{self.api_url}/{endpoint.lstrip('/')}\"\n",
    "        kwargs.setdefault('verify', self.verify_ssl)\n",
    "        \n",
    "        response = self.session.request(method, url, **kwargs)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    \n",
    "    def list_processors(self, process_group_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get all processors in a process group (recursive).\"\"\"\n",
    "        response = self._request(\"GET\", f\"/flow/process-groups/{process_group_id}/processors\")\n",
    "        data = response.json()\n",
    "        return data.get('processors', [])\n",
    "    \n",
    "    def get_process_group_status(self, group_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get live execution statistics for all processors in a process group.\"\"\"\n",
    "        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}/status\")\n",
    "        return response.json()\n",
    "    \n",
    "    def get_processor_invocation_counts(self, group_id: str) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract invocation counts for all processors in a process group (recursive).\n",
    "        Returns dictionary mapping processor ID to {name, type, invocations}.\n",
    "        \"\"\"\n",
    "        status_data = self.get_process_group_status(group_id)\n",
    "        processor_stats = {}\n",
    "        \n",
    "        logger.debug(f\"Status data top-level keys: {list(status_data.keys())}\")\n",
    "        \n",
    "        pg_status = status_data.get(\"processGroupStatus\", {})\n",
    "        \n",
    "        if not pg_status:\n",
    "            logger.warning(f\"No 'processGroupStatus' key in response for group {group_id}\")\n",
    "            return processor_stats\n",
    "        \n",
    "        logger.debug(f\"processGroupStatus keys: {list(pg_status.keys())}\")\n",
    "        \n",
    "        # Extract processor stats from current group\n",
    "        proc_status_list = pg_status.get(\"processorStatus\", [])\n",
    "        logger.debug(f\"Found {len(proc_status_list)} processors in current group {group_id[:8]}\")\n",
    "        \n",
    "        for proc_status in proc_status_list:\n",
    "            try:\n",
    "                proc_id = proc_status[\"id\"]\n",
    "                proc_name = proc_status[\"name\"]\n",
    "                proc_type = proc_status[\"type\"].split('.')[-1]\n",
    "                invocations = proc_status.get(\"aggregateSnapshot\", {}).get(\"invocations\", 0)\n",
    "                \n",
    "                processor_stats[proc_id] = {\n",
    "                    \"name\": proc_name,\n",
    "                    \"type\": proc_type,\n",
    "                    \"invocations\": invocations\n",
    "                }\n",
    "                logger.debug(f\"  Processor: {proc_name} - {invocations} invocations\")\n",
    "            except KeyError as e:\n",
    "                logger.error(f\"Missing key in processor status: {e}\")\n",
    "        \n",
    "        # Recursively get from child process groups\n",
    "        child_groups = pg_status.get(\"processGroupStatus\", [])\n",
    "        logger.debug(f\"Found {len(child_groups)} child process groups in {group_id[:8]}\")\n",
    "        \n",
    "        for child_pg_status in child_groups:\n",
    "            try:\n",
    "                child_id = child_pg_status[\"id\"]\n",
    "                child_name = child_pg_status.get(\"name\", \"unknown\")\n",
    "                logger.debug(f\"Recursing into child group: {child_name} ({child_id[:8]})\")\n",
    "                child_stats = self.get_processor_invocation_counts(child_id)\n",
    "                processor_stats.update(child_stats)\n",
    "                logger.debug(f\"Added {len(child_stats)} processors from child group {child_name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing child group: {e}\")\n",
    "        \n",
    "        logger.info(f\"Group {group_id[:8]}: collected {len(processor_stats)} total processor stats\")\n",
    "        return processor_stats\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the session.\"\"\"\n",
    "        self.session.close()\n",
    "\n",
    "console.print(\"[green]✓ NiFiClient class defined![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Analyzer Class\n",
    "# Analyzes processor execution counts\n",
    "\n",
    "class ProcessorUsageAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes processor execution frequency using NiFi Status API.\n",
    "    Identifies unused and low-usage processors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client: NiFiClient):\n",
    "        self.client = client\n",
    "        self.console = Console()\n",
    "        \n",
    "        # Analysis results\n",
    "        self.process_group_id: Optional[str] = None\n",
    "        self.processor_data: Dict[str, Dict] = {}\n",
    "        self.target_processors: List[Dict] = []\n",
    "        self.snapshot_timestamp: datetime = None\n",
    "    \n",
    "    def analyze(self, process_group_id: str) -> None:\n",
    "        \"\"\"Analyze processor execution counts for a process group.\"\"\"\n",
    "        self.process_group_id = process_group_id\n",
    "        self.snapshot_timestamp = datetime.now()\n",
    "        \n",
    "        self.console.print(f\"\\n[yellow]Analyzing processor execution counts:[/yellow]\")\n",
    "        self.console.print(f\"  Process Group: {process_group_id[:16]}...\")\n",
    "        self.console.print(f\"  Timestamp: {self.snapshot_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Phase 1: Get processors in target group\n",
    "        self.console.print(f\"\\n[yellow]Phase 1:[/yellow] Getting processors from target process group...\")\n",
    "        \n",
    "        try:\n",
    "            self.target_processors = self.client.list_processors(process_group_id)\n",
    "            self.console.print(f\"[green]OK[/green] Found {len(self.target_processors)} processors\")\n",
    "            \n",
    "            # Display processor list (first 10)\n",
    "            if self.target_processors:\n",
    "                self.console.print(\"\\n[cyan]Processors in target group:[/cyan]\")\n",
    "                for proc in self.target_processors[:10]:\n",
    "                    proc_name = proc['component']['name']\n",
    "                    proc_type = proc['component']['type'].split('.')[-1]\n",
    "                    self.console.print(f\"  • {proc_name} ({proc_type})\")\n",
    "                if len(self.target_processors) > 10:\n",
    "                    self.console.print(f\"  ... and {len(self.target_processors) - 10} more\")\n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[red]ERROR[/red] Failed to get processors: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Phase 2: Get processor execution counts\n",
    "        self.console.print(f\"\\n[yellow]Phase 2:[/yellow] Fetching execution statistics...\")\n",
    "        \n",
    "        try:\n",
    "            exec_stats = self.client.get_processor_invocation_counts(process_group_id)\n",
    "            \n",
    "            if len(exec_stats) == 0 and len(self.target_processors) > 0:\n",
    "                self.console.print(\n",
    "                    f\"[yellow]WARNING[/yellow] Retrieved execution counts for {len(exec_stats)} processors \"\n",
    "                    f\"(expected {len(self.target_processors)})\"\n",
    "                )\n",
    "            else:\n",
    "                self.console.print(\n",
    "                    f\"[green]OK[/green] Retrieved execution counts for {len(exec_stats)} processors\"\n",
    "                )\n",
    "            \n",
    "            # Build processor data\n",
    "            for proc in self.target_processors:\n",
    "                proc_id = proc['id']\n",
    "                proc_name = proc['component']['name']\n",
    "                proc_type = proc['component']['type'].split('.')[-1]\n",
    "                \n",
    "                invocations = exec_stats.get(proc_id, {}).get('invocations', 0)\n",
    "                \n",
    "                self.processor_data[proc_name] = {\n",
    "                    'id': proc_id,\n",
    "                    'type': proc_type,\n",
    "                    'invocations': invocations\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[red]ERROR[/red] Failed to fetch execution counts: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_results_dataframe(self):\n",
    "        \"\"\"Convert results to Spark DataFrame.\"\"\"\n",
    "        if not self.processor_data:\n",
    "            return None\n",
    "        \n",
    "        # Create data for DataFrame\n",
    "        rows = []\n",
    "        for name, data in self.processor_data.items():\n",
    "            rows.append((\n",
    "                self.snapshot_timestamp,\n",
    "                self.process_group_id,\n",
    "                data['id'],\n",
    "                name,\n",
    "                data['type'],\n",
    "                data['invocations']\n",
    "            ))\n",
    "        \n",
    "        # Define schema\n",
    "        schema = StructType([\n",
    "            StructField(\"snapshot_timestamp\", TimestampType(), False),\n",
    "            StructField(\"process_group_id\", StringType(), False),\n",
    "            StructField(\"processor_id\", StringType(), False),\n",
    "            StructField(\"processor_name\", StringType(), False),\n",
    "            StructField(\"processor_type\", StringType(), False),\n",
    "            StructField(\"invocations\", LongType(), False)\n",
    "        ])\n",
    "        \n",
    "        # Create DataFrame\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        df = spark.createDataFrame(rows, schema)\n",
    "        return df\n",
    "    \n",
    "    def display_summary(self):\n",
    "        \"\"\"Display analysis summary.\"\"\"\n",
    "        if not self.processor_data:\n",
    "            self.console.print(\"[red]No analysis results available.[/red]\")\n",
    "            return\n",
    "        \n",
    "        # Sort by execution count\n",
    "        sorted_processors = sorted(\n",
    "            self.processor_data.items(),\n",
    "            key=lambda x: x[1]['invocations'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Calculate stats\n",
    "        total_invocations = sum(data['invocations'] for _, data in sorted_processors)\n",
    "        unused_count = sum(1 for _, data in sorted_processors if data['invocations'] == 0)\n",
    "        low_usage_count = sum(1 for _, data in sorted_processors if 0 < data['invocations'] < 10)\n",
    "        \n",
    "        # Display summary\n",
    "        self.console.print(f\"\\n[cyan]Summary:[/cyan]\")\n",
    "        self.console.print(f\"  Total processors: {len(self.target_processors)}\")\n",
    "        self.console.print(f\"  Total executions (all time): {total_invocations:,}\")\n",
    "        self.console.print(f\"  Never executed: {unused_count} processors\")\n",
    "        self.console.print(f\"  Low usage (<10 executions): {low_usage_count} processors\")\n",
    "        \n",
    "        # Display table of top processors\n",
    "        table = Table(title=\"\\nTop 10 Active Processors\")\n",
    "        table.add_column(\"Processor Name\", style=\"cyan\")\n",
    "        table.add_column(\"Type\", style=\"green\")\n",
    "        table.add_column(\"Executions\", justify=\"right\", style=\"yellow\")\n",
    "        \n",
    "        for name, data in sorted_processors[:10]:\n",
    "            table.add_row(name, data['type'], f\"{data['invocations']:,}\")\n",
    "        \n",
    "        self.console.print(table)\n",
    "        \n",
    "        # Show pruning candidates\n",
    "        if unused_count > 0:\n",
    "            self.console.print(\n",
    "                f\"\\n[yellow]WARNING: Processors with 0 executions (candidates for pruning):[/yellow]\"\n",
    "            )\n",
    "            for name, data in sorted_processors:\n",
    "                if data['invocations'] == 0:\n",
    "                    self.console.print(f\"  • {name} ({data['type']})\")\n",
    "        \n",
    "        # Show low usage processors\n",
    "        if low_usage_count > 0:\n",
    "            self.console.print(\n",
    "                f\"\\n[yellow]WARNING: Processors with low execution count (<10 invocations):[/yellow]\"\n",
    "            )\n",
    "            for name, data in sorted_processors:\n",
    "                if 0 < data['invocations'] < 10:\n",
    "                    self.console.print(f\"  • {name} ({data['type']}): {data['invocations']} executions\")\n",
    "        \n",
    "        self.console.print(f\"\\n[green]OK[/green] Analysis complete!\")\n",
    "\n",
    "console.print(\"[green]✓ ProcessorUsageAnalyzer class defined![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run Analysis\n",
    "# This is the main execution cell\n",
    "\n",
    "console.print(\"\\n[cyan]Starting NiFi Processor Analysis...[/cyan]\\n\")\n",
    "\n",
    "# Connect to NiFi\n",
    "console.print(\"[yellow]Connecting to NiFi...[/yellow]\")\n",
    "client = NiFiClient(\n",
    "    base_url=CONFIG['nifi_url'],\n",
    "    username=CONFIG['username'],\n",
    "    password=CONFIG['password'],\n",
    "    verify_ssl=CONFIG['verify_ssl']\n",
    ")\n",
    "console.print(\"[green]OK[/green] Connected successfully\\n\")\n",
    "\n",
    "# Create analyzer and run analysis\n",
    "analyzer = ProcessorUsageAnalyzer(client)\n",
    "analyzer.analyze(CONFIG['process_group_id'])\n",
    "\n",
    "# Display results\n",
    "analyzer.display_summary()\n",
    "\n",
    "# Cleanup\n",
    "client.close()\n",
    "\n",
    "console.print(\"\\n[green]✓ Analysis complete![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save Snapshot to Delta Lake (Optional)\n",
    "# Run this cell to save the snapshot for historical tracking\n",
    "\n",
    "if CONFIG['enable_snapshots']:\n",
    "    console.print(\"\\n[yellow]Saving snapshot to Delta Lake...[/yellow]\")\n",
    "    \n",
    "    # Get results as DataFrame\n",
    "    df = analyzer.get_results_dataframe()\n",
    "    \n",
    "    if df is not None:\n",
    "        # Create table if it doesn't exist and append data\n",
    "        table_name = f\"{CONFIG['delta_database']}.{CONFIG['delta_table_name']}\"\n",
    "        \n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(table_name)\n",
    "        \n",
    "        console.print(f\"[green]OK[/green] Snapshot saved to {table_name}\")\n",
    "        console.print(f\"  Timestamp: {analyzer.snapshot_timestamp}\")\n",
    "        console.print(f\"  Processors: {len(analyzer.processor_data)}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        console.print(f\"\\n[cyan]Sample data saved:[/cyan]\")\n",
    "        display(df.limit(5))\n",
    "    else:\n",
    "        console.print(\"[red]ERROR[/red] No data to save\")\n",
    "else:\n",
    "    console.print(\"\\n[yellow]Snapshots disabled in configuration[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Query Historical Snapshots (Optional)\n",
    "# Run this cell to analyze historical snapshot data\n",
    "\n",
    "if CONFIG['enable_snapshots']:\n",
    "    console.print(\"\\n[cyan]Querying historical snapshots...[/cyan]\\n\")\n",
    "    \n",
    "    table_name = f\"{CONFIG['delta_database']}.{CONFIG['delta_table_name']}\"\n",
    "    \n",
    "    # Check if table exists\n",
    "    try:\n",
    "        # Show total snapshots\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                DATE(snapshot_timestamp) as date,\n",
    "                COUNT(DISTINCT snapshot_timestamp) as snapshots,\n",
    "                COUNT(*) as total_records\n",
    "            FROM {table_name}\n",
    "            GROUP BY DATE(snapshot_timestamp)\n",
    "            ORDER BY date DESC\n",
    "            LIMIT 10\n",
    "        \"\"\").show()\n",
    "        \n",
    "        # Find processors with 0 activity in last 7 days\n",
    "        console.print(\"\\n[yellow]Processors with 0 activity in last 7 days:[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            WITH recent_snapshots AS (\n",
    "                SELECT \n",
    "                    processor_name,\n",
    "                    processor_type,\n",
    "                    MAX(invocations) - MIN(invocations) as delta_invocations,\n",
    "                    MIN(snapshot_timestamp) as first_snapshot,\n",
    "                    MAX(snapshot_timestamp) as last_snapshot\n",
    "                FROM {table_name}\n",
    "                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "                GROUP BY processor_name, processor_type\n",
    "            )\n",
    "            SELECT \n",
    "                processor_name,\n",
    "                processor_type,\n",
    "                delta_invocations,\n",
    "                first_snapshot,\n",
    "                last_snapshot\n",
    "            FROM recent_snapshots\n",
    "            WHERE delta_invocations = 0\n",
    "            ORDER BY processor_name\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]ERROR[/red] Failed to query snapshots: {e}\")\n",
    "        console.print(\"[yellow]Hint:[/yellow] Table may not exist yet. Run Cell 7 first to create it.\")\n",
    "else:\n",
    "    console.print(\"\\n[yellow]Snapshots disabled in configuration[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Export Results to CSV (Optional)\n",
    "# Run this cell to export current results to DBFS/cloud storage\n",
    "\n",
    "console.print(\"\\n[yellow]Exporting results to CSV...[/yellow]\")\n",
    "\n",
    "# Get current timestamp for filename\n",
    "timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_path = f\"/dbfs/nifi_analysis/processor_usage_{timestamp_str}.csv\"\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = analyzer.get_results_dataframe()\n",
    "if df is not None:\n",
    "    # Convert to Pandas and save as CSV\n",
    "    pdf = df.toPandas()\n",
    "    pdf.to_csv(output_path, index=False)\n",
    "    \n",
    "    console.print(f\"[green]OK[/green] Results exported to {output_path}\")\n",
    "    console.print(f\"  Rows: {len(pdf)}\")\n",
    "    \n",
    "    # Show sample\n",
    "    console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n",
    "    display(pdf.head(10))\n",
    "else:\n",
    "    console.print(\"[red]ERROR[/red] No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### To Schedule This Notebook:\n",
    "\n",
    "1. **Create a Databricks Job:**\n",
    "   - Go to **Workflows** → **Jobs** → **Create Job**\n",
    "   - Type: **Notebook**\n",
    "   - Notebook path: Select this notebook\n",
    "   - Compute: **Serverless** (recommended)\n",
    "   - Schedule: `0 9 * * *` (daily at 9 AM)\n",
    "\n",
    "2. **Query Historical Data:**\n",
    "   ```sql\n",
    "   -- Find processors never used in last 30 days\n",
    "   SELECT \n",
    "       processor_name,\n",
    "       processor_type,\n",
    "       MAX(invocations) - MIN(invocations) as activity\n",
    "   FROM default.nifi_processor_snapshots\n",
    "   WHERE snapshot_timestamp >= current_date() - INTERVAL 30 DAYS\n",
    "   GROUP BY processor_name, processor_type\n",
    "   HAVING activity = 0;\n",
    "   ```\n",
    "\n",
    "3. **Create Alerts:**\n",
    "   - Set up Databricks alerts on the Delta table\n",
    "   - Get notified when new unused processors are detected\n",
    "\n",
    "### Tips:\n",
    "\n",
    "- Run Cell 6 anytime for ad-hoc analysis\n",
    "- Run Cell 7 to collect snapshots over time\n",
    "- Run Cell 8 to analyze trends and find inactive processors\n",
    "- Edit CONFIG in Cell 3 to analyze different process groups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
