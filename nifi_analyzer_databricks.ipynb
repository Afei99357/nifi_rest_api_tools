{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NiFi Processor Usage Analyzer - Databricks Edition\n",
    "\n",
    "This notebook analyzes NiFi processor execution counts to identify unused or underutilized processors.\n",
    "\n",
    "**Features:**\n",
    "- Fast execution count analysis (~5-10 seconds)\n",
    "- Snapshot mode for time-series tracking\n",
    "- Delta Lake integration for historical analysis\n",
    "- Standalone - no external files needed\n",
    "\n",
    "**Setup:**\n",
    "1. Edit the configuration in Cell 3\n",
    "2. Run all cells\n",
    "3. View results in output and Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "# This installs packages for the current notebook session\n",
    "\n",
    "%pip install requests rich --quiet\n",
    "\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "# Databricks-specific imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "# Disable SSL warnings if verify_ssl=False\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('nifi_analyzer')\n",
    "\n",
    "# Initialize Rich console for pretty output\n",
    "console = Console()\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Configuration\n# EDIT THESE VALUES FOR YOUR NIFI INSTANCE\n\nCONFIG = {\n    # NiFi Connection\n    'nifi_url': 'https://thbnk01hdpnp002.th-bnk01.nxp.com:8443/nifi',  # Your NiFi URL (without /api)\n    'username': 'nxg16670',                                              # Your NiFi username\n    'password': 'your-password-here',                                    # Your NiFi password\n    'verify_ssl': False,                                                 # Set to True if you have valid SSL\n    \n    # Server Identifier (for tracking multiple NiFi servers)\n    'server': 'prod-nifi-01',                                            # Server name (e.g., hostname, environment)\n    \n    # Analysis Parameters\n    'process_group_id': '8c8677c4-29d6-36...',                          # Process group ID to analyze\n    \n    # Snapshot Storage (Unity Catalog - 3-level naming)\n    'enable_snapshots': True,                                            # Save snapshots to Delta Lake?\n    'delta_table_path': 'main.default.nifi_processor_snapshots',        # catalog.schema.table\n}\n\nconsole.print(\"[green]✓ Configuration loaded![/green]\")\nconsole.print(f\"  NiFi URL: {CONFIG['nifi_url']}\")\nconsole.print(f\"  Username: {CONFIG['username']}\")\nconsole.print(f\"  Server: {CONFIG['server']}\")\nconsole.print(f\"  Process Group ID: {CONFIG['process_group_id'][:16]}...\")\nconsole.print(f\"  Delta table: {CONFIG['delta_table_path']}\")\nconsole.print(f\"  Snapshots enabled: {CONFIG['enable_snapshots']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: NiFi Client Class\n# Handles all NiFi REST API interactions\n\nclass NiFiClient:\n    \"\"\"\n    Client for interacting with Apache NiFi REST API.\n    Handles authentication and common API operations.\n    \"\"\"\n    \n    def __init__(self, base_url: str, username: str, password: str, verify_ssl: bool = True):\n        self.base_url = base_url.rstrip('/')\n        if not self.base_url.endswith('/nifi'):\n            self.base_url += '/nifi'\n        self.api_url = f\"{self.base_url}-api\"\n        self.verify_ssl = verify_ssl\n        self.session = requests.Session()\n        self.token = None\n        self.username = username\n        self.password = password\n        \n        # Authenticate\n        self._authenticate(username, password)\n        \n    def _authenticate(self, username: str, password: str) -> None:\n        \"\"\"Authenticate with NiFi using username/password.\"\"\"\n        try:\n            response = requests.post(\n                f\"{self.api_url}/access/token\",\n                data={'username': username, 'password': password},\n                verify=self.verify_ssl\n            )\n            \n            if response.status_code == 201:\n                self.token = response.text\n                self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n                logger.info(\"Successfully authenticated with token\")\n            else:\n                logger.warning(f\"Token auth failed with status {response.status_code}\")\n                logger.warning(\"Falling back to basic auth\")\n                from requests.auth import HTTPBasicAuth\n                self.session.auth = HTTPBasicAuth(username, password)\n        except Exception as e:\n            logger.warning(f\"Token auth error: {e}, falling back to basic auth\")\n            from requests.auth import HTTPBasicAuth\n            self.session.auth = HTTPBasicAuth(username, password)\n    \n    def _request(self, method: str, endpoint: str, **kwargs) -> requests.Response:\n        \"\"\"Make authenticated request with 401 retry.\"\"\"\n        url = f\"{self.api_url}/{endpoint.lstrip('/')}\"\n        kwargs.setdefault('verify', self.verify_ssl)\n        \n        response = self.session.request(method, url, **kwargs)\n        \n        # Handle 401 by re-authenticating once\n        if response.status_code == 401:\n            logger.warning(\"Received 401, attempting re-authentication\")\n            self._authenticate(self.username, self.password)\n            response = self.session.request(method, url, **kwargs)\n            if response.status_code == 401:\n                raise Exception(\"Authentication failed: Unauthorized\")\n        \n        response.raise_for_status()\n        return response\n    \n    def get_process_group(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get process group details including all processors.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}\")\n        return response.json()\n    \n    def list_processors(self, process_group_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Get all processors in a process group (recursive).\"\"\"\n        pg_data = self.get_process_group(process_group_id)\n        processors = pg_data[\"processGroupFlow\"][\"flow\"][\"processors\"]\n        \n        # Recursively get processors from child groups\n        child_groups = pg_data[\"processGroupFlow\"][\"flow\"][\"processGroups\"]\n        for child in child_groups:\n            processors.extend(self.list_processors(child[\"id\"]))\n        \n        return processors\n    \n    def get_process_group_status(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get execution statistics for process group.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}/status\")\n        return response.json()\n    \n    def get_connection_statistics(self, group_id: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extract ALL connection statistics from Status API (connection-level, not aggregated).\n        \n        This method extracts all 15+ fields from connectionStatusSnapshot for maximum\n        granularity. Returns raw connection-level data without processor aggregation.\n        \n        Returns:\n            List of connection dictionaries with all available fields\n        \"\"\"\n        status_data = self.get_process_group_status(group_id)\n        all_connections = []\n        \n        pg_status = status_data.get(\"processGroupStatus\", {})\n        if not pg_status:\n            return all_connections\n        \n        # Get all connections from current group\n        connection_statuses = pg_status.get(\"aggregateSnapshot\", {}).get(\"connectionStatusSnapshots\", [])\n        \n        # Extract ALL fields from each connection (15+ fields)\n        for conn_status in connection_statuses:\n            conn_snap = conn_status.get(\"connectionStatusSnapshot\", {})\n            \n            connection_data = {\n                # Connection identity (7 fields)\n                'id': conn_snap.get('id'),\n                'groupId': conn_snap.get('groupId'),\n                'name': conn_snap.get('name', ''),\n                'sourceId': conn_snap.get('sourceId'),\n                'sourceName': conn_snap.get('sourceName'),\n                'destinationId': conn_snap.get('destinationId'),\n                'destinationName': conn_snap.get('destinationName'),\n                \n                # Flow metrics - 5-minute window (6 fields)\n                'flowFilesIn': conn_snap.get('flowFilesIn', 0),\n                'flowFilesOut': conn_snap.get('flowFilesOut', 0),\n                'bytesIn': conn_snap.get('bytesIn', 0),\n                'bytesOut': conn_snap.get('bytesOut', 0),\n                'input': conn_snap.get('input', ''),\n                'output': conn_snap.get('output', ''),\n                \n                # Queue metrics - current state (4 fields)\n                'queuedCount': conn_snap.get('queuedCount', 0),\n                'queuedBytes': conn_snap.get('queuedBytes', 0),\n                'queued': conn_snap.get('queued', ''),\n                'queuedSize': conn_snap.get('queuedSize', ''),\n                \n                # Status indicators (2 fields)\n                'percentUseCount': conn_snap.get('percentUseCount', 0),\n                'percentUseBytes': conn_snap.get('percentUseBytes', 0),\n                \n                # Timestamps (1 field)\n                'statsLastRefreshed': conn_snap.get('statsLastRefreshed', ''),\n            }\n            \n            all_connections.append(connection_data)\n        \n        # Recurse into child groups\n        child_groups = pg_status.get(\"processGroupStatus\", [])\n        for child_pg in child_groups:\n            try:\n                child_id = child_pg[\"id\"]\n                child_connections = self.get_connection_statistics(child_id)\n                all_connections.extend(child_connections)\n            except Exception as e:\n                logger.error(f\"Error processing child group: {e}\")\n        \n        return all_connections\n    \n    def close(self):\n        \"\"\"Close the session.\"\"\"\n        self.session.close()\n\nconsole.print(\"[green]✓ NiFiClient class defined![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Analyzer Class\n# Analyzes processor execution counts using connection-level data\n\nclass ProcessorUsageAnalyzer:\n    \"\"\"\n    Analyzes processor execution frequency using NiFi Status API.\n    Captures ALL connection-level attributes for comprehensive analysis.\n    \"\"\"\n    \n    def __init__(self, client: NiFiClient, server: str = 'unknown'):\n        self.client = client\n        self.console = Console()\n        self.server = server\n        \n        # Analysis results\n        self.process_group_id: Optional[str] = None\n        self.connection_statistics: List[Dict] = []\n        self.target_processors: List[Dict] = []\n        self.snapshot_timestamp: datetime = None\n    \n    def analyze(self, process_group_id: str) -> None:\n        \"\"\"Analyze processor execution counts for a process group.\"\"\"\n        self.process_group_id = process_group_id\n        self.snapshot_timestamp = datetime.now()\n        \n        self.console.print(f\"\\n[yellow]Analyzing processor execution counts:[/yellow]\")\n        self.console.print(f\"  Process Group: {process_group_id[:16]}...\")\n        self.console.print(f\"  Server: {self.server}\")\n        self.console.print(f\"  Timestamp: {self.snapshot_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n        \n        # Phase 1: Get processors in target group\n        self.console.print(f\"\\n[yellow]Phase 1:[/yellow] Getting processors from target process group...\")\n        \n        try:\n            self.target_processors = self.client.list_processors(process_group_id)\n            self.console.print(f\"[green]OK[/green] Found {len(self.target_processors)} processors\")\n            \n            # Display processor list (first 10)\n            if self.target_processors:\n                self.console.print(\"\\n[cyan]Processors in target group:[/cyan]\")\n                for proc in self.target_processors[:10]:\n                    proc_name = proc['component']['name']\n                    proc_type = proc['component']['type'].split('.')[-1]\n                    self.console.print(f\"  • {proc_name} ({proc_type})\")\n                if len(self.target_processors) > 10:\n                    self.console.print(f\"  ... and {len(self.target_processors) - 10} more\")\n        except Exception as e:\n            self.console.print(f\"[red]ERROR[/red] Failed to get processors: {e}\")\n            raise\n        \n        # Phase 2: Get connection statistics (ALL fields)\n        self.console.print(f\"\\n[yellow]Phase 2:[/yellow] Fetching connection statistics (all fields)...\")\n        \n        try:\n            connection_stats = self.client.get_connection_statistics(process_group_id)\n            \n            if len(connection_stats) == 0:\n                self.console.print(\n                    f\"[yellow]WARNING[/yellow] No connections found in process group\"\n                )\n            else:\n                self.console.print(\n                    f\"[green]OK[/green] Retrieved {len(connection_stats)} connections\"\n                )\n            \n            # Store raw connection data\n            self.connection_statistics = connection_stats\n                \n        except Exception as e:\n            self.console.print(f\"[red]ERROR[/red] Failed to fetch connection statistics: {e}\")\n            raise\n    \n    def get_results_dataframe(self):\n        \"\"\"Convert results to Spark DataFrame with 24-field connection schema.\"\"\"\n        if not self.connection_statistics:\n            return None\n        \n        # Create data for DataFrame (24 fields)\n        rows = []\n        for conn in self.connection_statistics:\n            rows.append((\n                self.snapshot_timestamp,\n                self.server,\n                self.process_group_id,\n                conn.get('id'),\n                conn.get('name', ''),\n                conn.get('groupId'),\n                conn.get('sourceId'),\n                conn.get('sourceName'),\n                conn.get('destinationId'),\n                conn.get('destinationName'),\n                conn.get('flowFilesIn', 0),\n                conn.get('flowFilesOut', 0),\n                conn.get('bytesIn', 0),\n                conn.get('bytesOut', 0),\n                conn.get('input', ''),\n                conn.get('output', ''),\n                conn.get('queuedCount', 0),\n                conn.get('queuedBytes', 0),\n                conn.get('queued', ''),\n                conn.get('queuedSize', ''),\n                conn.get('percentUseCount', 0),\n                conn.get('percentUseBytes', 0),\n                conn.get('statsLastRefreshed', '')\n            ))\n        \n        # Define schema with ALL 24 fields (connection-level)\n        schema = StructType([\n            # Metadata (3 fields)\n            StructField(\"snapshot_timestamp\", TimestampType(), False),\n            StructField(\"server\", StringType(), False),\n            StructField(\"process_group_id\", StringType(), False),\n            \n            # Connection identity (3 fields)\n            StructField(\"connection_id\", StringType(), True),\n            StructField(\"connection_name\", StringType(), True),\n            StructField(\"connection_group_id\", StringType(), True),\n            \n            # Source processor (2 fields)\n            StructField(\"source_id\", StringType(), True),\n            StructField(\"source_name\", StringType(), True),\n            \n            # Destination processor (2 fields)\n            StructField(\"destination_id\", StringType(), True),\n            StructField(\"destination_name\", StringType(), True),\n            \n            # Flow metrics (6 fields)\n            StructField(\"flow_files_in\", LongType(), False),\n            StructField(\"flow_files_out\", LongType(), False),\n            StructField(\"bytes_in\", LongType(), False),\n            StructField(\"bytes_out\", LongType(), False),\n            StructField(\"input\", StringType(), True),\n            StructField(\"output\", StringType(), True),\n            \n            # Queue metrics (4 fields)\n            StructField(\"queued_count\", LongType(), False),\n            StructField(\"queued_bytes\", LongType(), False),\n            StructField(\"queued\", StringType(), True),\n            StructField(\"queued_size\", StringType(), True),\n            \n            # Status indicators (2 fields)\n            StructField(\"percent_use_count\", LongType(), False),\n            StructField(\"percent_use_bytes\", LongType(), False),\n            \n            # Timestamps (1 field)\n            StructField(\"stats_last_refreshed\", StringType(), True)\n        ])\n        \n        # Create DataFrame\n        spark = SparkSession.builder.getOrCreate()\n        df = spark.createDataFrame(rows, schema)\n        return df\n    \n    def display_summary(self):\n        \"\"\"Display analysis summary.\"\"\"\n        if not self.connection_statistics:\n            self.console.print(\"[red]No analysis results available.[/red]\")\n            return\n        \n        # Aggregate connections to processor-level for summary\n        processor_activity = {}\n        for conn in self.connection_statistics:\n            source_name = conn.get('sourceName', 'Unknown')\n            if source_name not in processor_activity:\n                processor_activity[source_name] = {\n                    'flowFilesOut': 0,\n                    'bytesOut': 0,\n                    'queuedCount': 0\n                }\n            processor_activity[source_name]['flowFilesOut'] += conn.get('flowFilesOut', 0)\n            processor_activity[source_name]['bytesOut'] += conn.get('bytesOut', 0)\n            processor_activity[source_name]['queuedCount'] += conn.get('queuedCount', 0)\n        \n        # Sort by flowfile output\n        sorted_processors = sorted(\n            processor_activity.items(),\n            key=lambda x: x[1]['flowFilesOut'],\n            reverse=True\n        )\n        \n        # Calculate stats\n        total_flowfiles = sum(data['flowFilesOut'] for _, data in sorted_processors)\n        total_bytes = sum(data['bytesOut'] for _, data in sorted_processors)\n        unused_count = sum(1 for _, data in sorted_processors if data['flowFilesOut'] == 0)\n        low_usage_count = sum(1 for _, data in sorted_processors if 0 < data['flowFilesOut'] < 10)\n        queued_connections = sum(1 for conn in self.connection_statistics if conn.get('queuedCount', 0) > 0)\n        \n        # Display summary\n        self.console.print(f\"\\n[cyan]Summary:[/cyan]\")\n        self.console.print(f\"  Total processors: {len(sorted_processors)}\")\n        self.console.print(f\"  Total connections: {len(self.connection_statistics)}\")\n        self.console.print(f\"  Total flowfiles output (snapshot): {total_flowfiles:,}\")\n        self.console.print(f\"  Total bytes output (snapshot): {total_bytes:,}\")\n        self.console.print(f\"  No output: {unused_count} processors\")\n        self.console.print(f\"  Low output (<10 flowfiles): {low_usage_count} processors\")\n        self.console.print(f\"  Connections with queue: {queued_connections}\")\n        \n        # Display table of top processors\n        table = Table(title=\"\\nTop 10 Active Processors\")\n        table.add_column(\"Processor Name\", style=\"cyan\")\n        table.add_column(\"FlowFiles Out\", justify=\"right\", style=\"yellow\")\n        table.add_column(\"Bytes Out\", justify=\"right\", style=\"green\")\n        table.add_column(\"Queued\", justify=\"right\", style=\"red\")\n        \n        for name, data in sorted_processors[:10]:\n            table.add_row(\n                name, \n                f\"{data['flowFilesOut']:,}\",\n                f\"{data['bytesOut']:,}\",\n                f\"{data['queuedCount']:,}\"\n            )\n        \n        self.console.print(table)\n        \n        # Show connections with backpressure\n        if queued_connections > 0:\n            self.console.print(\n                f\"\\n[yellow]WARNING: Connections with queued flowfiles (backpressure):[/yellow]\"\n            )\n            for conn in sorted(self.connection_statistics, key=lambda x: x.get('queuedCount', 0), reverse=True)[:10]:\n                if conn.get('queuedCount', 0) > 0:\n                    self.console.print(\n                        f\"  • {conn.get('sourceName')} → {conn.get('destinationName')}: \"\n                        f\"{conn.get('queuedCount', 0):,} flowfiles queued \"\n                        f\"({conn.get('percentUseCount', 0)}% full)\"\n                    )\n        \n        # Show pruning candidates (processors with no output)\n        if unused_count > 0:\n            self.console.print(\n                f\"\\n[yellow]WARNING: Processors with 0 flowfile output (candidates for pruning):[/yellow]\"\n            )\n            count = 0\n            for name, data in sorted_processors:\n                if data['flowFilesOut'] == 0 and count < 10:\n                    self.console.print(f\"  • {name}\")\n                    count += 1\n            if unused_count > 10:\n                self.console.print(f\"  ... and {unused_count - 10} more\")\n        \n        self.console.print(f\"\\n[green]OK[/green] Analysis complete!\")\n\nconsole.print(\"[green]✓ ProcessorUsageAnalyzer class defined![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Run Analysis\n# This is the main execution cell\n\nconsole.print(\"\\n[cyan]Starting NiFi Processor Analysis...[/cyan]\\n\")\n\n# Connect to NiFi\nconsole.print(\"[yellow]Connecting to NiFi...[/yellow]\")\nclient = NiFiClient(\n    base_url=CONFIG['nifi_url'],\n    username=CONFIG['username'],\n    password=CONFIG['password'],\n    verify_ssl=CONFIG['verify_ssl']\n)\nconsole.print(\"[green]OK[/green] Connected successfully\\n\")\n\n# Create analyzer and run analysis\nanalyzer = ProcessorUsageAnalyzer(client=client, server=CONFIG['server'])\nanalyzer.analyze(CONFIG['process_group_id'])\n\n# Display results\nanalyzer.display_summary()\n\n# Cleanup\nclient.close()\n\nconsole.print(\"\\n[green]✓ Analysis complete![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Save Snapshot to Delta Lake (Optional)\n# Run this cell to save the snapshot for historical tracking\n\nif CONFIG['enable_snapshots']:\n    console.print(\"\\n[yellow]Saving snapshot to Delta Lake...[/yellow]\")\n    \n    # Get results as DataFrame\n    df = analyzer.get_results_dataframe()\n    \n    if df is not None:\n        table_name = CONFIG['delta_table_path']\n        \n        # IMPORTANT: Drop existing table to start fresh with new 24-field connection-level schema\n        # This is necessary because we changed from old schema to 24-field connection-level\n        try:\n            spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n            console.print(f\"[yellow]Existing table dropped to start fresh with new schema[/yellow]\")\n        except Exception as e:\n            console.print(f\"[yellow]No existing table to drop (this is fine): {e}[/yellow]\")\n        \n        # Create new table with 24-field schema\n        console.print(f\"[yellow]Creating table with new schema: {table_name}[/yellow]\")\n        df.write \\\n            .format(\"delta\") \\\n            .mode(\"overwrite\") \\\n            .option(\"overwriteSchema\", \"true\") \\\n            .saveAsTable(table_name)\n        console.print(f\"[green]OK[/green] Table created successfully with 24-field connection-level schema\")\n        \n        console.print(f\"  Timestamp: {analyzer.snapshot_timestamp}\")\n        console.print(f\"  Total connections: {len(analyzer.connection_statistics)}\")\n        console.print(f\"  Schema: 23 fields (connection-level with ALL attributes)\")\n        \n        # Show sample data\n        console.print(f\"\\n[cyan]Sample data saved:[/cyan]\")\n        display(df.limit(5))\n        \n        # Show schema\n        console.print(f\"\\n[cyan]Schema:[/cyan]\")\n        df.printSchema()\n    else:\n        console.print(\"[red]ERROR[/red] No data to save\")\nelse:\n    console.print(\"\\n[yellow]Snapshots disabled in configuration[/yellow]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Query Historical Snapshots (Connection-Level Analysis)\n# Run this cell to analyze historical snapshot data\n\nif CONFIG['enable_snapshots']:\n    console.print(\"\\n[cyan]Querying connection-level snapshots...[/cyan]\\n\")\n    \n    table_name = CONFIG['delta_table_path']\n    \n    # Check if table exists\n    try:\n        # Show total snapshots by server\n        console.print(\"[yellow]Snapshots by server:[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                COUNT(DISTINCT snapshot_timestamp) as snapshots,\n                COUNT(*) as total_connections,\n                MAX(snapshot_timestamp) as last_snapshot\n            FROM {table_name}\n            GROUP BY server\n            ORDER BY server\n        \"\"\").show(truncate=False)\n        \n        # NEW: Find connections with high queue depth (backpressure detection)\n        console.print(\"\\n[yellow]Connections with queued flowfiles (backpressure):[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                source_name,\n                destination_name,\n                MAX(queued_count) as max_queued_flowfiles,\n                MAX(queued_bytes) as max_queued_bytes,\n                MAX(percent_use_count) as max_percent_full\n            FROM {table_name}\n            WHERE queued_count > 0\n            GROUP BY server, source_name, destination_name\n            ORDER BY max_queued_flowfiles DESC\n            LIMIT 20\n        \"\"\").show(truncate=False)\n        \n        # NEW: Identify connections approaching queue limits\n        console.print(\"\\n[yellow]Connections approaching queue limits (>50% full):[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                source_name,\n                destination_name,\n                MAX(percent_use_count) as max_percent_full,\n                MAX(queued_count) as max_queued_count\n            FROM {table_name}\n            WHERE percent_use_count > 50\n            GROUP BY server, source_name, destination_name\n            ORDER BY max_percent_full DESC\n            LIMIT 20\n        \"\"\").show(truncate=False)\n        \n        # Find inactive connections (no flow for 7 days)\n        console.print(\"\\n[yellow]Inactive connections (no flowfiles for 7 days):[/yellow]\")\n        spark.sql(f\"\"\"\n            WITH connection_activity AS (\n                SELECT \n                    server,\n                    source_name,\n                    destination_name,\n                    MAX(flow_files_out) - MIN(flow_files_out) as delta_flowfiles,\n                    MIN(snapshot_timestamp) as first_snapshot,\n                    MAX(snapshot_timestamp) as last_snapshot,\n                    COUNT(DISTINCT snapshot_timestamp) as num_snapshots\n                FROM {table_name}\n                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n                GROUP BY server, source_name, destination_name\n            )\n            SELECT \n                server,\n                source_name,\n                destination_name,\n                delta_flowfiles,\n                num_snapshots\n            FROM connection_activity\n            WHERE delta_flowfiles = 0\n            ORDER BY server, source_name\n            LIMIT 50\n        \"\"\").show(truncate=False)\n        \n        # Aggregate to processor level (still possible!)\n        console.print(\"\\n[yellow]Inactive processors (aggregated from connections):[/yellow]\")\n        spark.sql(f\"\"\"\n            WITH processor_activity AS (\n                SELECT \n                    server,\n                    source_name as processor_name,\n                    MAX(flow_files_out) - MIN(flow_files_out) as delta_flowfiles\n                FROM {table_name}\n                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n                GROUP BY server, source_name\n            )\n            SELECT \n                server,\n                processor_name,\n                delta_flowfiles\n            FROM processor_activity\n            WHERE delta_flowfiles = 0\n            ORDER BY server, processor_name\n            LIMIT 50\n        \"\"\").show(truncate=False)\n        \n        # NEW: Bidirectional flow analysis\n        console.print(\"\\n[yellow]Flow balance (input vs output by connection):[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                source_name,\n                destination_name,\n                SUM(flow_files_in) as total_flowfiles_in,\n                SUM(flow_files_out) as total_flowfiles_out,\n                SUM(flow_files_in) - SUM(flow_files_out) as net_change\n            FROM {table_name}\n            WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n            GROUP BY server, source_name, destination_name\n            HAVING ABS(SUM(flow_files_in) - SUM(flow_files_out)) > 100\n            ORDER BY ABS(net_change) DESC\n            LIMIT 20\n        \"\"\").show(truncate=False)\n        \n    except Exception as e:\n        console.print(f\"[red]ERROR[/red] Failed to query snapshots: {e}\")\n        console.print(\"[yellow]Hint:[/yellow] Table may not exist yet. Run Cell 7 first to create it.\")\n        import traceback\n        traceback.print_exc()\nelse:\n    console.print(\"\\n[yellow]Snapshots disabled in configuration[/yellow]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Export Results to CSV (Optional)\n",
    "# Run this cell to export current results to DBFS/cloud storage\n",
    "\n",
    "console.print(\"\\n[yellow]Exporting results to CSV...[/yellow]\")\n",
    "\n",
    "# Get current timestamp for filename\n",
    "timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_path = f\"/dbfs/nifi_analysis/processor_usage_{timestamp_str}.csv\"\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = analyzer.get_results_dataframe()\n",
    "if df is not None:\n",
    "    # Convert to Pandas and save as CSV\n",
    "    pdf = df.toPandas()\n",
    "    pdf.to_csv(output_path, index=False)\n",
    "    \n",
    "    console.print(f\"[green]OK[/green] Results exported to {output_path}\")\n",
    "    console.print(f\"  Rows: {len(pdf)}\")\n",
    "    \n",
    "    # Show sample\n",
    "    console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n",
    "    display(pdf.head(10))\n",
    "else:\n",
    "    console.print(\"[red]ERROR[/red] No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Updated Delta Table Schema (Connection-Level)\n\nThe Delta table now captures **ALL available fields** from NiFi Status API at the **connection level** (not processor level). This provides maximum granularity for analysis.\n\n### 23 Total Fields\n\n| Column | Type | Description |\n|--------|------|-------------|\n| **Metadata (3 fields)** | | |\n| `snapshot_timestamp` | Timestamp | When the snapshot was captured |\n| `server` | String | Server identifier (hostname, environment name) |\n| `process_group_id` | String | NiFi process group ID |\n| **Connection Identity (3 fields)** | | |\n| `connection_id` | String | Connection UUID |\n| `connection_name` | String | Connection name (often empty or \"success\") |\n| `connection_group_id` | String | Parent process group ID |\n| **Source Processor (2 fields)** | | |\n| `source_id` | String | Source processor UUID |\n| `source_name` | String | Source processor name |\n| **Destination Processor (2 fields)** | | |\n| `destination_id` | String | Destination processor UUID |\n| `destination_name` | String | Destination processor name |\n| **Flow Metrics - 5-minute window (6 fields)** | | |\n| `flow_files_in` | Long | FlowFiles entering connection |\n| `flow_files_out` | Long | FlowFiles leaving connection |\n| `bytes_in` | Long | Bytes entering connection |\n| `bytes_out` | Long | Bytes leaving connection |\n| `input` | String | Formatted input stats (e.g., \"1,250 (50.8 KB)\") |\n| `output` | String | Formatted output stats |\n| **Queue Metrics - current state (4 fields)** | | |\n| `queued_count` | Long | FlowFiles currently queued |\n| `queued_bytes` | Long | Bytes currently queued |\n| `queued` | String | Formatted queue stats |\n| `queued_size` | String | Formatted queue size |\n| **Status Indicators (2 fields)** | | |\n| `percent_use_count` | Long | % of queue count threshold used |\n| `percent_use_bytes` | Long | % of queue bytes threshold used |\n| **Timestamps (1 field)** | | |\n| `stats_last_refreshed` | String | When stats were last updated |\n\n## Key Differences from Previous Version\n\n**Before:** Limited fields, processor-level aggregation\n- Only captured basic metrics\n- Aggregated connections by processor\n- Could not identify specific bottleneck connections\n- No queue monitoring capability\n\n**Now:** 23 fields, connection-level granularity\n- Captures ALL 15+ fields from NiFi Status API\n- Stores each connection separately\n- Can identify exact bottleneck points\n- Enables queue monitoring, backpressure detection, flow lineage\n\n**Impact:** ~2x more rows (typical NiFi flow has 1-2 connections per processor), but unlocks powerful new analysis capabilities.\n\n## New Analysis Capabilities\n\n### 1. Backpressure Detection\nIdentify connections with high queue depth:\n```sql\nSELECT source_name, destination_name, MAX(queued_count) as max_queued\nFROM main.default.nifi_processor_snapshots\nWHERE queued_count > 100\nGROUP BY source_name, destination_name\nORDER BY max_queued DESC;\n```\n\n### 2. Queue Limit Monitoring\nFind connections approaching capacity:\n```sql\nSELECT source_name, destination_name, MAX(percent_use_count) as max_percent_full\nFROM main.default.nifi_processor_snapshots\nWHERE percent_use_count > 80\nGROUP BY source_name, destination_name;\n```\n\n### 3. Bidirectional Flow Tracking\nCompare input vs output to find imbalances:\n```sql\nSELECT source_name,\n       SUM(flow_files_in) as total_in,\n       SUM(flow_files_out) as total_out,\n       SUM(flow_files_in) - SUM(flow_files_out) as net_change\nFROM main.default.nifi_processor_snapshots\nGROUP BY source_name\nHAVING ABS(net_change) > 100;\n```\n\n### 4. Processor-Level Analysis (Still Possible!)\nAggregate connections to processor-level when needed:\n```sql\nWITH processor_activity AS (\n    SELECT source_name,\n           MAX(flow_files_out) - MIN(flow_files_out) as delta\n    FROM main.default.nifi_processor_snapshots\n    WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n    GROUP BY source_name\n)\nSELECT * FROM processor_activity WHERE delta = 0;\n```\n\n### 5. Flow Path Lineage\nTrack data movement through the flow:\n```sql\nSELECT source_name, destination_name, \n       SUM(flow_files_out) as total_flowfiles\nFROM main.default.nifi_processor_snapshots\nWHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\nGROUP BY source_name, destination_name\nORDER BY total_flowfiles DESC;\n```\n\n## Unity Catalog Configuration\n\nThe notebook uses Unity Catalog with 3-level naming:\n- **Catalog**: `main` (default)\n- **Schema**: `default` (default)\n- **Table**: `nifi_processor_snapshots`\n- **Full path**: `main.default.nifi_processor_snapshots`\n\nYou can customize this in Cell 3 by editing `delta_table_path`.\n\n## To Schedule This Notebook:\n\n1. **Create a Databricks Job:**\n   - Go to **Workflows** → **Jobs** → **Create Job**\n   - Type: **Notebook**\n   - Notebook path: Select this notebook\n   - Compute: **Serverless** (recommended)\n   - Schedule: `0 */5 * * *` (every 5 minutes for continuous monitoring)\n\n2. **Create Alerts:**\n   - Set up Databricks alerts on the Delta table\n   - Get notified when connections approach queue limits\n   - Alert on inactive processors\n\n## Key Concepts\n\n**Connection-Level Storage:**\n- Each connection is stored as a separate row\n- Preserves source → destination relationships\n- Enables fine-grained debugging and analysis\n- Can still aggregate to processor-level in queries\n\n**Snapshot-based Analysis:**\n- Each run captures a snapshot of flowfile counts at that moment\n- Take snapshots every 5 minutes for real-time monitoring\n- Calculate deltas (MAX - MIN) to identify inactive connections\n- Connection with delta = 0 means no flowfiles processed in that time period\n\n**Why Connection-Level Instead of Processor-Level?**\n- Identify which specific connection is bottlenecked\n- Monitor queue depth per connection\n- Track flow paths (source → destination lineage)\n- More debugging capability with minimal storage overhead\n\n**5-Minute Window:**\n- NiFi Status API returns metrics aggregated over the last 5 minutes\n- Running snapshots every 5 minutes captures distinct time windows\n- Historical data retained for 24 hours (configurable in NiFi)\n\n**Queue Metrics:**\n- `queued_count`: Current number of FlowFiles waiting in connection\n- `percent_use_count`: How full the queue is (approaching backpressure threshold)\n- Helps identify bottlenecks before they cause performance issues\n\n## Migration Notes\n\n**IMPORTANT:** Running Cell 7 will DROP the existing table to start fresh with the new 23-field schema. This is necessary because:\n1. Schema changed to connection-level with 23 fields\n2. Data model changed from processor aggregation to connection-level\n3. Cannot merge old and new data structures\n\n**Before running:** If you want to preserve old data, create a backup:\n```python\nspark.sql(\"CREATE TABLE main.default.nifi_processor_snapshots_backup AS SELECT * FROM main.default.nifi_processor_snapshots\")\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}