{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NiFi Processor Usage Analyzer - Multi-Flow Edition\n",
    "\n",
    "This notebook analyzes NiFi processor execution counts across **multiple process groups** to identify unused or underutilized processors.\n",
    "\n",
    "**Features:**\n",
    "- Analyzes multiple flows from CSV input\n",
    "- Fast execution count analysis (~5-10 seconds per flow)\n",
    "- Snapshot mode with flow_name tracking\n",
    "- Delta Lake integration with timestamp\n",
    "- Standalone - no external files needed\n",
    "\n",
    "**Setup:**\n",
    "1. Upload CSV with flow definitions (id, flow_name)\n",
    "2. Edit the configuration in Cell 3\n",
    "3. Run all cells\n",
    "4. View results in Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "%pip install requests rich --quiet\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\n",
    "\n",
    "# Databricks-specific imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "# Disable SSL warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('nifi_analyzer')\n",
    "\n",
    "# Initialize Rich console\n",
    "console = Console()\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Configuration\n# EDIT THESE VALUES FOR YOUR NIFI INSTANCE\n\nCONFIG = {\n    # NiFi Connection\n    'nifi_url': 'https://thbnk01hdpnp002.th-bnk01.nxp.com:8443/nifi',\n    'username': 'nxg16670',\n    'password': 'your-password-here',  # ← EDIT THIS\n    'verify_ssl': False,\n    \n    # Server Identifier (for tracking multiple NiFi servers)\n    'server': 'prod-nifi-01',  # ← EDIT THIS (e.g., 'prod', 'dev', hostname)\n    \n    # Flow Definitions CSV\n    # CSV Format: id,flow_name\n    # Example:\n    #   8c8677c4-29d6-36...,Production_Flow_1\n    #   abc-123-def...,Development_Flow_2\n    'flows_csv_path': '/dbfs/nifi_analysis/flows.csv',  # ← Path to your CSV\n    \n    # Snapshot Storage (Unity Catalog - 3-level naming)\n    'enable_snapshots': True,\n    'delta_table_path': 'main.default.nifi_processor_snapshots',  # catalog.schema.table\n}\n\nconsole.print(\"[green]✓ Configuration loaded![/green]\")\nconsole.print(f\"  NiFi URL: {CONFIG['nifi_url']}\")\nconsole.print(f\"  Username: {CONFIG['username']}\")\nconsole.print(f\"  Server: {CONFIG['server']}\")\nconsole.print(f\"  Flows CSV: {CONFIG['flows_csv_path']}\")\nconsole.print(f\"  Delta table: {CONFIG['delta_table_path']}\")\nconsole.print(f\"  Snapshots enabled: {CONFIG['enable_snapshots']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: NiFi Client Class - RECURSIVE PROCESSOR EXTRACTION WITH PARENT GROUP INFO\n\nclass NiFiClient:\n    \"\"\"Client for interacting with Apache NiFi REST API.\"\"\"\n    \n    def __init__(self, base_url: str, username: str, password: str, verify_ssl: bool = True):\n        self.base_url = base_url.rstrip('/')\n        if not self.base_url.endswith('/nifi'):\n            self.base_url += '/nifi'\n        self.api_url = f\"{self.base_url}-api\"\n        self.verify_ssl = verify_ssl\n        self.session = requests.Session()\n        self.token = None\n        self.username = username\n        self.password = password\n        self._authenticate(username, password)\n        \n    def _authenticate(self, username: str, password: str) -> None:\n        \"\"\"Authenticate with NiFi.\"\"\"\n        try:\n            response = requests.post(\n                f\"{self.api_url}/access/token\",\n                data={'username': username, 'password': password},\n                verify=self.verify_ssl\n            )\n            \n            if response.status_code == 201:\n                self.token = response.text\n                self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n                logger.info(\"Successfully authenticated with token\")\n            else:\n                logger.warning(f\"Token auth failed with status {response.status_code}\")\n                logger.warning(\"Falling back to basic auth\")\n                from requests.auth import HTTPBasicAuth\n                self.session.auth = HTTPBasicAuth(username, password)\n        except Exception as e:\n            logger.warning(f\"Token auth error: {e}, falling back to basic auth\")\n            from requests.auth import HTTPBasicAuth\n            self.session.auth = HTTPBasicAuth(username, password)\n    \n    def _request(self, method: str, endpoint: str, **kwargs) -> requests.Response:\n        \"\"\"Make authenticated request with 401 retry.\"\"\"\n        url = f\"{self.api_url}/{endpoint.lstrip('/')}\"\n        kwargs.setdefault('verify', self.verify_ssl)\n        \n        response = self.session.request(method, url, **kwargs)\n        \n        # Handle 401 by re-authenticating once\n        if response.status_code == 401:\n            logger.warning(\"Received 401, attempting re-authentication\")\n            self._authenticate(self.username, self.password)\n            response = self.session.request(method, url, **kwargs)\n            if response.status_code == 401:\n                raise Exception(\"Authentication failed: Unauthorized\")\n        \n        response.raise_for_status()\n        return response\n    \n    def get_process_group(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get process group details including all processors.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}\")\n        return response.json()\n    \n    def get_process_group_status(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get execution statistics for process group.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}/status?recursive=true\")\n        return response.json()\n    \n    def get_process_group_name(self, group_id: str) -> str:\n        \"\"\"\n        Get the name of a process group by its ID.\n\n        Args:\n            group_id: Process group UUID\n\n        Returns:\n            Process group name, or group_id if fetch fails\n        \"\"\"\n        try:\n            response = self._request(\"GET\", f\"/flow/process-groups/{group_id}\")\n            data = response.json()\n            group_name = data.get('processGroupFlow', {}).get('component', {}).get('name', group_id)\n            logger.info(f\"Fetched group name for {group_id}: {group_name}\")\n            return group_name\n        except Exception as e:\n            logger.warning(f\"Failed to fetch group name for {group_id}: {str(e)}\")\n            return group_id  # Fallback to ID if name fetch fails\n    \n    def get_processor_statistics(self, group_id: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get processor-level statistics with IDs and parent groups using Status API.\n\n        Uses Status API with recursive=true which returns nested processGroupStatusSnapshots.\n        We recursively walk through all levels to extract ALL processors from deeply nested groups,\n        along with their parent group IDs and names.\n\n        Returns:\n            List of processor dictionaries with IDs, metrics, and parent group info\n        \"\"\"\n        # Get Status API with recursive=true ONCE\n        status_data = self.get_process_group_status(group_id)\n\n        # First pass: Recursively extract processors WITH groupId\n        def extract_processors_recursive(pg_snapshot_data, depth=0):\n            \"\"\"Recursively extract processors from nested process group status snapshot.\"\"\"\n            processors = []\n\n            # Extract processors at THIS level from processorStatusSnapshots array\n            processor_snapshots = pg_snapshot_data.get('processorStatusSnapshots', [])\n            \n            logger.info(f\"{'  ' * depth}Level {depth}: Found {len(processor_snapshots)} processors\")\n\n            for snapshot in processor_snapshots:\n                # Each snapshot wraps a processorStatusSnapshot with the actual data\n                proc_snap = snapshot.get('processorStatusSnapshot', {})\n\n                processors.append({\n                    'processor_id': proc_snap.get('id'),\n                    'processor_name': proc_snap.get('name'),\n                    'processor_type': proc_snap.get('type', '').split('.')[-1],\n                    'parent_group_id': proc_snap.get('groupId'),  # NEW: Capture parent group ID\n                    'flow_files_in': int(proc_snap.get('flowFilesIn', 0)),\n                    'bytes_in': int(proc_snap.get('bytesIn', 0)),\n                    'flow_files_out': int(proc_snap.get('flowFilesOut', 0)),\n                    'bytes_out': int(proc_snap.get('bytesOut', 0)),\n                    'tasks': int(proc_snap.get('taskCount', 0)),\n                    'run_status': proc_snap.get('runStatus', 'Unknown')\n                })\n\n            # Recursively process nested child process groups\n            # Note: processGroupStatusSnapshots (plural) contains wrapped processGroupStatusSnapshot (singular) objects\n            child_group_snapshots = pg_snapshot_data.get('processGroupStatusSnapshots', [])\n            if child_group_snapshots:\n                logger.info(f\"{'  ' * depth}Level {depth}: Found {len(child_group_snapshots)} child groups, recursing...\")\n\n            for child_group_snapshot in child_group_snapshots:\n                # Unwrap the processGroupStatusSnapshot object\n                child_pg_snapshot = child_group_snapshot.get('processGroupStatusSnapshot', {})\n                child_processors = extract_processors_recursive(child_pg_snapshot, depth + 1)\n                processors.extend(child_processors)\n\n            return processors\n\n        # Start recursion from root - need to navigate to aggregateSnapshot first\n        pg_status = status_data.get('processGroupStatus', {})\n        aggregate_snapshot = pg_status.get('aggregateSnapshot', {})\n        all_processors = extract_processors_recursive(aggregate_snapshot)\n\n        logger.info(f\"Total processors extracted from all levels: {len(all_processors)}\")\n\n        # Second pass: Collect unique parent group IDs and batch fetch names\n        parent_group_ids = set()\n        for proc in all_processors:\n            if proc.get('parent_group_id'):\n                parent_group_ids.add(proc['parent_group_id'])\n\n        logger.info(f\"Fetching names for {len(parent_group_ids)} unique parent groups...\")\n        group_name_cache = {}\n        for pg_id in parent_group_ids:\n            group_name_cache[pg_id] = self.get_process_group_name(pg_id)\n\n        # Third pass: Add parent group names to all processors\n        for proc in all_processors:\n            parent_id = proc.get('parent_group_id')\n            proc['parent_group_name'] = group_name_cache.get(parent_id, parent_id) if parent_id else None\n\n        logger.info(f\"Returning {len(all_processors)} processor records with parent group info\")\n        return all_processors\n    \n    def close(self):\n        \"\"\"Close session.\"\"\"\n        self.session.close()\n\nconsole.print(\"[green]✓ NiFiClient class defined (with parent group info)![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Multi-Flow Analyzer Class - PROCESSOR-LEVEL WITH PARENT GROUP INFO\n\nclass MultiFlowAnalyzer:\n    \"\"\"Analyzes multiple NiFi flows and stores PROCESSOR-LEVEL results in Delta Lake.\"\"\"\n    \n    def __init__(self, client: NiFiClient, server: str = 'unknown'):\n        self.client = client\n        self.console = Console()\n        self.server = server\n        self.all_results = []\n        self.snapshot_timestamp = datetime.now()\n    \n    def analyze_flow(self, flow_id: str, flow_name: str) -> Dict:\n        \"\"\"Analyze a single flow - processor level.\"\"\"\n        flow_results = {\n            'flow_name': flow_name,\n            'flow_id': flow_id,\n            'processor_count': 0,\n            'processors': []\n        }\n        \n        try:\n            # Get processor-level statistics with parent group info\n            processors = self.client.get_processor_statistics(flow_id)\n            flow_results['processor_count'] = len(processors)\n            \n            # Add metadata to each processor\n            for proc in processors:\n                flow_results['processors'].append({\n                    # Metadata (4 fields)\n                    'snapshot_timestamp': self.snapshot_timestamp,\n                    'server': self.server,\n                    'flow_name': flow_name,\n                    'process_group_id': flow_id,\n                    \n                    # Processor identity (5 fields - added parent group)\n                    'processor_id': proc.get('processor_id'),\n                    'processor_name': proc.get('processor_name'),\n                    'processor_type': proc.get('processor_type'),\n                    'parent_group_id': proc.get('parent_group_id'),       # NEW\n                    'parent_group_name': proc.get('parent_group_name'),   # NEW\n                    \n                    # Activity metrics (6 fields)\n                    'flow_files_in': proc.get('flow_files_in', 0),\n                    'bytes_in': proc.get('bytes_in', 0),\n                    'flow_files_out': proc.get('flow_files_out', 0),\n                    'bytes_out': proc.get('bytes_out', 0),\n                    'tasks': proc.get('tasks', 0),\n                    'run_status': proc.get('run_status', 'Unknown')\n                })\n            \n            return flow_results\n            \n        except Exception as e:\n            self.console.print(f\"[red]ERROR[/red] Failed to analyze {flow_name}: {e}\")\n            flow_results['error'] = str(e)\n            return flow_results\n    \n    def analyze_all_flows(self, flows_csv_path: str):\n        \"\"\"Analyze all flows from CSV.\"\"\"\n        self.console.print(f\"\\n[cyan]Multi-Flow Analysis Starting (PROCESSOR-LEVEL)...[/cyan]\")\n        self.console.print(f\"  Server: {self.server}\")\n        self.console.print(f\"  Timestamp: {self.snapshot_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        \n        # Read flows CSV\n        try:\n            flows_df = spark.read.csv(flows_csv_path, header=True)\n            flows = flows_df.collect()\n            \n            self.console.print(f\"[green]Found {len(flows)} flows to analyze[/green]\\n\")\n            \n        except Exception as e:\n            self.console.print(f\"[red]ERROR[/red] Failed to read CSV: {e}\")\n            raise\n        \n        # Analyze each flow\n        with Progress(\n            SpinnerColumn(),\n            TextColumn(\"[progress.description]{task.description}\"),\n            BarColumn(),\n            console=self.console\n        ) as progress:\n            task = progress.add_task(\"Analyzing flows...\", total=len(flows))\n            \n            for flow in flows:\n                flow_id = flow['id']\n                flow_name = flow['flow_name']\n                \n                progress.update(task, description=f\"Analyzing: {flow_name}\")\n                \n                flow_results = self.analyze_flow(flow_id, flow_name)\n                self.all_results.append(flow_results)\n                \n                # Display flow summary\n                if 'error' not in flow_results:\n                    self.console.print(\n                        f\"  [green]✓[/green] {flow_name}: {flow_results['processor_count']} processors\"\n                    )\n                else:\n                    self.console.print(\n                        f\"  [red]✗[/red] {flow_name}: {flow_results['error']}\"\n                    )\n                \n                progress.advance(task)\n        \n        # Display overall summary\n        self.display_summary()\n    \n    def display_summary(self):\n        \"\"\"Display analysis summary.\"\"\"\n        total_processors = sum(r['processor_count'] for r in self.all_results if 'error' not in r)\n        successful_flows = sum(1 for r in self.all_results if 'error' not in r)\n        failed_flows = sum(1 for r in self.all_results if 'error' in r)\n        \n        self.console.print(f\"\\n[cyan]Overall Summary:[/cyan]\")\n        self.console.print(f\"  Server: {self.server}\")\n        self.console.print(f\"  Total flows: {len(self.all_results)}\")\n        self.console.print(f\"  Successful: {successful_flows}\")\n        self.console.print(f\"  Failed: {failed_flows}\")\n        self.console.print(f\"  Total processors: {total_processors}\")\n        \n        # Create summary table\n        table = Table(title=\"\\nFlow Analysis Summary\")\n        table.add_column(\"Flow Name\", style=\"cyan\")\n        table.add_column(\"Processors\", justify=\"right\", style=\"yellow\")\n        table.add_column(\"Status\", style=\"green\")\n        \n        for result in self.all_results:\n            status = \"[red]Error[/red]\" if 'error' in result else \"[green]Success[/green]\"\n            table.add_row(\n                result['flow_name'],\n                str(result['processor_count']),\n                status\n            )\n        \n        self.console.print(table)\n    \n    def get_results_dataframe(self):\n        \"\"\"Convert all results to Spark DataFrame with 15-field processor schema (includes parent group).\"\"\"\n        all_rows = []\n        \n        for flow_result in self.all_results:\n            if 'error' not in flow_result:\n                all_rows.extend(flow_result['processors'])\n        \n        if not all_rows:\n            return None\n        \n        # Convert to list of tuples (15 fields)\n        rows = [\n            (\n                row['snapshot_timestamp'],\n                row['server'],\n                row['flow_name'],\n                row['process_group_id'],\n                row['processor_id'],\n                row['processor_name'],\n                row['processor_type'],\n                row.get('parent_group_id'),         # NEW\n                row.get('parent_group_name'),       # NEW\n                row.get('flow_files_in', 0),\n                row.get('bytes_in', 0),\n                row.get('flow_files_out', 0),\n                row.get('bytes_out', 0),\n                row.get('tasks', 0),\n                row.get('run_status', 'Unknown')\n            )\n            for row in all_rows\n        ]\n        \n        # Define schema (15 fields - processor level with parent group info)\n        schema = StructType([\n            # Metadata (4 fields)\n            StructField(\"snapshot_timestamp\", TimestampType(), False),\n            StructField(\"server\", StringType(), False),\n            StructField(\"flow_name\", StringType(), False),\n            StructField(\"process_group_id\", StringType(), False),\n            \n            # Processor identity (5 fields)\n            StructField(\"processor_id\", StringType(), True),\n            StructField(\"processor_name\", StringType(), False),\n            StructField(\"processor_type\", StringType(), True),\n            StructField(\"parent_group_id\", StringType(), True),      # NEW FIELD\n            StructField(\"parent_group_name\", StringType(), True),    # NEW FIELD\n            \n            # Activity metrics (6 fields)\n            StructField(\"flow_files_in\", LongType(), False),\n            StructField(\"bytes_in\", LongType(), False),\n            StructField(\"flow_files_out\", LongType(), False),\n            StructField(\"bytes_out\", LongType(), False),\n            StructField(\"tasks\", LongType(), False),\n            StructField(\"run_status\", StringType(), False)\n        ])\n        \n        spark = SparkSession.builder.getOrCreate()\n        return spark.createDataFrame(rows, schema)\n\nconsole.print(\"[green]✓ MultiFlowAnalyzer class defined (with parent group info)![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Run Multi-Flow Analysis\n\nconsole.print(\"\\n[cyan]Starting Multi-Flow NiFi Analysis...[/cyan]\\n\")\n\n# Connect to NiFi\nconsole.print(\"[yellow]Connecting to NiFi...[/yellow]\")\nclient = NiFiClient(\n    base_url=CONFIG['nifi_url'],\n    username=CONFIG['username'],\n    password=CONFIG['password'],\n    verify_ssl=CONFIG['verify_ssl']\n)\nconsole.print(\"[green]OK[/green] Connected successfully\\n\")\n\n# Create analyzer and run analysis\nanalyzer = MultiFlowAnalyzer(client=client, server=CONFIG['server'])\nanalyzer.analyze_all_flows(CONFIG['flows_csv_path'])\n\n# Cleanup\nclient.close()\n\nconsole.print(\"\\n[green]✓ Multi-flow analysis complete![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Save Snapshots to Delta Lake\n\nif CONFIG['enable_snapshots']:\n    console.print(\"\\n[yellow]Saving snapshots to Delta Lake...[/yellow]\")\n    \n    df = analyzer.get_results_dataframe()\n    \n    if df is not None:\n        table_name = CONFIG['delta_table_path']\n        \n        # Check if table exists\n        table_exists = spark.catalog._jcatalog.tableExists(table_name)\n        \n        if not table_exists:\n            # First run: Create table\n            console.print(f\"[yellow]Table doesn't exist, creating: {table_name}[/yellow]\")\n            df.write \\\n                .format(\"delta\") \\\n                .mode(\"overwrite\") \\\n                .option(\"overwriteSchema\", \"true\") \\\n                .saveAsTable(table_name)\n            console.print(f\"[green]OK[/green] Table created successfully with 24-field connection-level schema\")\n        else:\n            # Subsequent runs: Append data\n            console.print(f\"[yellow]Table exists, appending data to: {table_name}[/yellow]\")\n            df.write \\\n                .format(\"delta\") \\\n                .mode(\"append\") \\\n                .option(\"mergeSchema\", \"true\") \\\n                .saveAsTable(table_name)\n            console.print(f\"[green]OK[/green] Data appended successfully\")\n        \n        console.print(f\"  Timestamp: {analyzer.snapshot_timestamp}\")\n        console.print(f\"  Total rows written: {df.count()}\")\n        \n        # Show sample\n        console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n        display(df.limit(10))\n    else:\n        console.print(\"[red]ERROR[/red] No data to save\")\nelse:\n    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")\n\n# NOTE: If you need to manually drop the table to start fresh, run this in a separate cell:\n# spark.sql(f\"DROP TABLE IF EXISTS {CONFIG['delta_table_path']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Query Historical Snapshots (Connection-Level Analysis)\n\nif CONFIG['enable_snapshots']:\n    console.print(\"\\n[cyan]Querying connection-level snapshots...[/cyan]\\n\")\n    \n    table_name = CONFIG['delta_table_path']\n    \n    try:\n        # Show snapshots per flow and server\n        console.print(\"[yellow]Snapshot count by server and flow:[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                flow_name,\n                COUNT(DISTINCT snapshot_timestamp) as snapshots,\n                COUNT(*) as total_connections,\n                MAX(snapshot_timestamp) as last_snapshot\n            FROM {table_name}\n            GROUP BY server, flow_name\n            ORDER BY server, flow_name\n        \"\"\").show(truncate=False)\n        \n        # NEW: Find connections with high queue depth (backpressure detection)\n        console.print(\"\\n[yellow]Connections with queued flowfiles (backpressure):[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                flow_name,\n                source_name,\n                destination_name,\n                MAX(queued_count) as max_queued_flowfiles,\n                MAX(queued_bytes) as max_queued_bytes,\n                MAX(percent_use_count) as max_percent_full\n            FROM {table_name}\n            WHERE queued_count > 0\n            GROUP BY server, flow_name, source_name, destination_name\n            ORDER BY max_queued_flowfiles DESC\n            LIMIT 20\n        \"\"\").show(truncate=False)\n        \n        # NEW: Identify connections approaching queue limits\n        console.print(\"\\n[yellow]Connections approaching queue limits (>50% full):[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                flow_name,\n                source_name,\n                destination_name,\n                MAX(percent_use_count) as max_percent_full,\n                MAX(queued_count) as max_queued_count\n            FROM {table_name}\n            WHERE percent_use_count > 50\n            GROUP BY server, flow_name, source_name, destination_name\n            ORDER BY max_percent_full DESC\n            LIMIT 20\n        \"\"\").show(truncate=False)\n        \n        # Find inactive connections (no flow for 7 days)\n        console.print(\"\\n[yellow]Inactive connections (no flowfiles for 7 days):[/yellow]\")\n        spark.sql(f\"\"\"\n            WITH connection_activity AS (\n                SELECT \n                    server,\n                    flow_name,\n                    source_name,\n                    destination_name,\n                    MAX(flow_files_out) - MIN(flow_files_out) as delta_flowfiles,\n                    MIN(snapshot_timestamp) as first_snapshot,\n                    MAX(snapshot_timestamp) as last_snapshot,\n                    COUNT(DISTINCT snapshot_timestamp) as num_snapshots\n                FROM {table_name}\n                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n                GROUP BY server, flow_name, source_name, destination_name\n            )\n            SELECT \n                server,\n                flow_name,\n                source_name,\n                destination_name,\n                delta_flowfiles,\n                num_snapshots\n            FROM connection_activity\n            WHERE delta_flowfiles = 0\n            ORDER BY server, flow_name, source_name\n            LIMIT 50\n        \"\"\").show(truncate=False)\n        \n        # Aggregate to processor level (still possible!)\n        console.print(\"\\n[yellow]Inactive processors by flow (aggregated from connections):[/yellow]\")\n        spark.sql(f\"\"\"\n            WITH processor_activity AS (\n                SELECT \n                    server,\n                    flow_name,\n                    source_name as processor_name,\n                    MAX(flow_files_out) - MIN(flow_files_out) as delta_flowfiles\n                FROM {table_name}\n                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n                GROUP BY server, flow_name, source_name\n            )\n            SELECT \n                server,\n                flow_name,\n                COUNT(*) as inactive_processor_count\n            FROM processor_activity\n            WHERE delta_flowfiles = 0\n            GROUP BY server, flow_name\n            ORDER BY server, inactive_processor_count DESC\n        \"\"\").show(truncate=False)\n        \n        # NEW: Track queue growth over time\n        console.print(\"\\n[yellow]Queue depth trends (hourly averages):[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                DATE_TRUNC('hour', snapshot_timestamp) as hour,\n                server,\n                flow_name,\n                source_name,\n                destination_name,\n                AVG(queued_count) as avg_queued_flowfiles,\n                MAX(queued_count) as max_queued_flowfiles\n            FROM {table_name}\n            WHERE snapshot_timestamp >= current_date() - INTERVAL 1 DAYS\n              AND queued_count > 0\n            GROUP BY hour, server, flow_name, source_name, destination_name\n            ORDER BY hour DESC, avg_queued_flowfiles DESC\n            LIMIT 20\n        \"\"\").show(truncate=False)\n        \n        # NEW: Bidirectional flow analysis\n        console.print(\"\\n[yellow]Flow balance (input vs output by connection):[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                flow_name,\n                source_name,\n                destination_name,\n                SUM(flow_files_in) as total_flowfiles_in,\n                SUM(flow_files_out) as total_flowfiles_out,\n                SUM(flow_files_in) - SUM(flow_files_out) as net_change\n            FROM {table_name}\n            WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n            GROUP BY server, flow_name, source_name, destination_name\n            HAVING ABS(SUM(flow_files_in) - SUM(flow_files_out)) > 100\n            ORDER BY ABS(net_change) DESC\n            LIMIT 20\n        \"\"\").show(truncate=False)\n        \n    except Exception as e:\n        console.print(f\"[red]ERROR[/red] Failed to query: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Export Results to CSV by Flow\n",
    "\n",
    "console.print(\"\\n[yellow]Exporting results to CSV...[/yellow]\")\n",
    "\n",
    "timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "df = analyzer.get_results_dataframe()\n",
    "if df is not None:\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    # Export overall summary\n",
    "    output_path = f\"/dbfs/nifi_analysis/all_flows_{timestamp_str}.csv\"\n",
    "    pdf.to_csv(output_path, index=False)\n",
    "    console.print(f\"[green]OK[/green] All flows exported to {output_path}\")\n",
    "    \n",
    "    # Export per flow\n",
    "    for flow_name in pdf['flow_name'].unique():\n",
    "        flow_df = pdf[pdf['flow_name'] == flow_name]\n",
    "        flow_path = f\"/dbfs/nifi_analysis/{flow_name}_{timestamp_str}.csv\"\n",
    "        flow_df.to_csv(flow_path, index=False)\n",
    "        console.print(f\"  [green]✓[/green] {flow_name}: {len(flow_df)} processors\")\n",
    "    \n",
    "    console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n",
    "    display(pdf.head(10))\n",
    "else:\n",
    "    console.print(\"[red]ERROR[/red] No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Updated Delta Table Schema (Connection-Level)\n\nThe Delta table now captures **ALL available fields** from NiFi Status API at the **connection level** (not processor level). This provides maximum granularity for analysis.\n\n### 24 Total Fields\n\n| Column | Type | Description |\n|--------|------|-------------|\n| **Metadata (4 fields)** | | |\n| `snapshot_timestamp` | Timestamp | When the snapshot was captured |\n| `server` | String | Server identifier (hostname, environment name) |\n| `flow_name` | String | Flow name from CSV |\n| `process_group_id` | String | NiFi process group ID |\n| **Connection Identity (3 fields)** | | |\n| `connection_id` | String | Connection UUID |\n| `connection_name` | String | Connection name (often empty or \"success\") |\n| `connection_group_id` | String | Parent process group ID |\n| **Source Processor (2 fields)** | | |\n| `source_id` | String | Source processor UUID |\n| `source_name` | String | Source processor name |\n| **Destination Processor (2 fields)** | | |\n| `destination_id` | String | Destination processor UUID |\n| `destination_name` | String | Destination processor name |\n| **Flow Metrics - 5-minute window (6 fields)** | | |\n| `flow_files_in` | Long | FlowFiles entering connection |\n| `flow_files_out` | Long | FlowFiles leaving connection |\n| `bytes_in` | Long | Bytes entering connection |\n| `bytes_out` | Long | Bytes leaving connection |\n| `input` | String | Formatted input stats (e.g., \"1,250 (50.8 KB)\") |\n| `output` | String | Formatted output stats |\n| **Queue Metrics - current state (4 fields)** | | |\n| `queued_count` | Long | FlowFiles currently queued |\n| `queued_bytes` | Long | Bytes currently queued |\n| `queued` | String | Formatted queue stats |\n| `queued_size` | String | Formatted queue size |\n| **Status Indicators (2 fields)** | | |\n| `percent_use_count` | Long | % of queue count threshold used |\n| `percent_use_bytes` | Long | % of queue bytes threshold used |\n| **Timestamps (1 field)** | | |\n| `stats_last_refreshed` | String | When stats were last updated |\n\n## Key Differences from Previous Version\n\n**Before:** 9 fields, processor-level aggregation\n- Only captured: flowFilesOut, bytesOut\n- Aggregated connections by source processor\n- Could not identify specific bottleneck connections\n- No queue monitoring capability\n\n**Now:** 24 fields, connection-level granularity\n- Captures ALL 15+ fields from NiFi Status API\n- Stores each connection separately\n- Can identify exact bottleneck points\n- Enables queue monitoring, backpressure detection, flow lineage\n\n**Impact:** ~2x more rows (typical NiFi flow has 1-2 connections per processor), but unlocks powerful new analysis capabilities.\n\n## New Analysis Capabilities\n\n### 1. Backpressure Detection\nIdentify connections with high queue depth:\n```sql\nSELECT source_name, destination_name, MAX(queued_count) as max_queued\nFROM main.default.nifi_processor_snapshots\nWHERE queued_count > 100\nGROUP BY source_name, destination_name\nORDER BY max_queued DESC;\n```\n\n### 2. Queue Limit Monitoring\nFind connections approaching capacity:\n```sql\nSELECT source_name, destination_name, MAX(percent_use_count) as max_percent_full\nFROM main.default.nifi_processor_snapshots\nWHERE percent_use_count > 80\nGROUP BY source_name, destination_name;\n```\n\n### 3. Bidirectional Flow Tracking\nCompare input vs output to find imbalances:\n```sql\nSELECT source_name,\n       SUM(flow_files_in) as total_in,\n       SUM(flow_files_out) as total_out,\n       SUM(flow_files_in) - SUM(flow_files_out) as net_change\nFROM main.default.nifi_processor_snapshots\nGROUP BY source_name\nHAVING ABS(net_change) > 100;\n```\n\n### 4. Queue Growth Trends\nMonitor queue depth over time:\n```sql\nSELECT DATE_TRUNC('hour', snapshot_timestamp) as hour,\n       source_name, destination_name,\n       AVG(queued_count) as avg_queued_flowfiles\nFROM main.default.nifi_processor_snapshots\nWHERE snapshot_timestamp >= current_date() - INTERVAL 1 DAYS\nGROUP BY hour, source_name, destination_name\nORDER BY hour, avg_queued_flowfiles DESC;\n```\n\n### 5. Processor-Level Analysis (Still Possible!)\nAggregate connections to processor-level when needed:\n```sql\nWITH processor_activity AS (\n    SELECT source_name,\n           MAX(flow_files_out) - MIN(flow_files_out) as delta\n    FROM main.default.nifi_processor_snapshots\n    WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n    GROUP BY source_name\n)\nSELECT * FROM processor_activity WHERE delta = 0;\n```\n\n### 6. Flow Path Lineage\nTrack data movement through the flow:\n```sql\nSELECT source_name, destination_name, \n       SUM(flow_files_out) as total_flowfiles\nFROM main.default.nifi_processor_snapshots\nWHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\nGROUP BY source_name, destination_name\nORDER BY total_flowfiles DESC;\n```\n\n## Unity Catalog Configuration\n\nThe notebook uses Unity Catalog with 3-level naming:\n- **Catalog**: `main` (default)\n- **Schema**: `default` (default)\n- **Table**: `nifi_processor_snapshots`\n- **Full path**: `main.default.nifi_processor_snapshots`\n\nYou can customize this in Cell 3 by editing `delta_table_path`.\n\n## CSV Format\n\nYour `flows.csv` should look like:\n```\nid,flow_name\n8c8677c4-29d6-3607-a32e-1234567890ab,Production_Data_Pipeline\nabc-123-def-456-7890-abcdef123456,Development_Testing_Flow\nxyz-789-ghi-012-3456-7890abcdef12,QA_Validation_Flow\n```\n\nUpload it to: `/dbfs/nifi_analysis/flows.csv`\n\n## Server Identifier\n\nThe `server` field helps track data from multiple NiFi instances:\n- Use hostname: `prod-nifi-01`, `dev-nifi-02`\n- Use environment: `prod`, `dev`, `qa`, `staging`\n- Use datacenter: `dc1-nifi`, `dc2-nifi`\n\nThis allows you to:\n- Compare processor usage across environments\n- Track migration from one server to another\n- Aggregate metrics across multiple NiFi clusters\n\n## Key Concepts\n\n**Connection-Level Storage:**\n- Each connection is stored as a separate row\n- Preserves source → destination relationships\n- Enables fine-grained debugging and analysis\n- Can still aggregate to processor-level in queries\n\n**Snapshot-based Analysis:**\n- Each run captures a snapshot of flowfile counts at that moment\n- Take snapshots every 5 minutes over a week\n- Calculate deltas (MAX - MIN) to identify inactive connections\n- Connection with delta = 0 means no flowfiles processed in that time period\n\n**Why Connection-Level Instead of Processor-Level?**\n- Identify which specific connection is bottlenecked\n- Monitor queue depth per connection\n- Track flow paths (source → destination lineage)\n- More debugging capability with minimal storage overhead\n\n**5-Minute Window:**\n- NiFi Status API returns metrics aggregated over the last 5 minutes\n- Running snapshots every 5 minutes captures distinct time windows\n- Historical data retained for 24 hours (configurable in NiFi)\n\n**Queue Metrics:**\n- `queued_count`: Current number of FlowFiles waiting in connection\n- `percent_use_count`: How full the queue is (approaching backpressure threshold)\n- Helps identify bottlenecks before they cause performance issues\n\n## Migration Notes\n\n**IMPORTANT:** Running Cell 7 will DROP the existing table to start fresh with the new 24-field schema. This is necessary because:\n1. Schema changed from 9 fields to 24 fields\n2. Data model changed from processor-level to connection-level\n3. Cannot merge old and new data structures\n\n**Before running:** If you want to preserve old data, create a backup:\n```python\nspark.sql(\"CREATE TABLE main.default.nifi_processor_snapshots_backup AS SELECT * FROM main.default.nifi_processor_snapshots\")\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}