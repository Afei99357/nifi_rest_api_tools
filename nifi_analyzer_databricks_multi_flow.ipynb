{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NiFi Processor Usage Analyzer - Multi-Flow Edition\n",
    "\n",
    "This notebook analyzes NiFi processor execution counts across **multiple process groups** to identify unused or underutilized processors.\n",
    "\n",
    "**Features:**\n",
    "- Analyzes multiple flows from CSV input\n",
    "- Fast execution count analysis (~5-10 seconds per flow)\n",
    "- Snapshot mode with flow_name tracking\n",
    "- Delta Lake integration with timestamp\n",
    "- Standalone - no external files needed\n",
    "\n",
    "**Setup:**\n",
    "1. Upload CSV with flow definitions (id, flow_name)\n",
    "2. Edit the configuration in Cell 3\n",
    "3. Run all cells\n",
    "4. View results in Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "%pip install requests rich --quiet\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\n",
    "\n",
    "# Databricks-specific imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "# Disable SSL warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('nifi_analyzer')\n",
    "\n",
    "# Initialize Rich console\n",
    "console = Console()\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "# EDIT THESE VALUES FOR YOUR NIFI INSTANCE\n",
    "\n",
    "CONFIG = {\n",
    "    # NiFi Connection\n",
    "    'nifi_url': 'https://thbnk01hdpnp002.th-bnk01.nxp.com:8443/nifi',\n",
    "    'username': 'nxg16670',\n",
    "    'password': 'your-password-here',  # ← EDIT THIS\n",
    "    'verify_ssl': False,\n",
    "    \n",
    "    # Flow Definitions CSV\n",
    "    # CSV Format: id,flow_name\n",
    "    # Example:\n",
    "    #   8c8677c4-29d6-36...,Production_Flow_1\n",
    "    #   abc-123-def...,Development_Flow_2\n",
    "    'flows_csv_path': '/dbfs/nifi_analysis/flows.csv',  # ← Path to your CSV\n",
    "    \n",
    "    # Snapshot Storage\n",
    "    'enable_snapshots': True,\n",
    "    'delta_table_name': 'nifi_processor_snapshots',\n",
    "    'delta_database': 'default',\n",
    "}\n",
    "\n",
    "console.print(\"[green]✓ Configuration loaded![/green]\")\n",
    "console.print(f\"  NiFi URL: {CONFIG['nifi_url']}\")\n",
    "console.print(f\"  Username: {CONFIG['username']}\")\n",
    "console.print(f\"  Flows CSV: {CONFIG['flows_csv_path']}\")\n",
    "console.print(f\"  Snapshots enabled: {CONFIG['enable_snapshots']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3b: Create Example CSV (Run once to create template)\n",
    "# This cell creates an example CSV file - edit it with your actual flow IDs\n",
    "\n",
    "example_csv_content = \"\"\"id,flow_name\n",
    "8c8677c4-29d6-3607-a32e-example1,Production_Data_Pipeline\n",
    "abc-123-def-456-example2,Development_Testing_Flow\n",
    "xyz-789-ghi-012-example3,QA_Validation_Flow\n",
    "\"\"\"\n",
    "\n",
    "# Write example CSV\n",
    "dbutils.fs.mkdirs(\"/dbfs/nifi_analysis\")\n",
    "dbutils.fs.put(CONFIG['flows_csv_path'], example_csv_content, overwrite=True)\n",
    "\n",
    "console.print(f\"[green]✓ Example CSV created at {CONFIG['flows_csv_path']}[/green]\")\n",
    "console.print(\"[yellow]Edit this file with your actual flow IDs and names![/yellow]\")\n",
    "\n",
    "# Display the CSV\n",
    "console.print(\"\\n[cyan]Current CSV content:[/cyan]\")\n",
    "display(spark.read.csv(CONFIG['flows_csv_path'], header=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: NiFi Client Class\n",
    "\n",
    "class NiFiClient:\n",
    "    \"\"\"Client for interacting with Apache NiFi REST API.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, username: str, password: str, verify_ssl: bool = True):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.api_url = f\"{self.base_url}-api\"\n",
    "        self.verify_ssl = verify_ssl\n",
    "        self.session = requests.Session()\n",
    "        self.token = None\n",
    "        self._authenticate(username, password)\n",
    "        \n",
    "    def _authenticate(self, username: str, password: str) -> None:\n",
    "        \"\"\"Authenticate with NiFi.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/access/token\",\n",
    "            data={'username': username, 'password': password},\n",
    "            verify=self.verify_ssl\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 201:\n",
    "            self.token = response.text\n",
    "            self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n",
    "            logger.info(\"Successfully authenticated\")\n",
    "        else:\n",
    "            raise Exception(f\"Authentication failed: {response.status_code}\")\n",
    "    \n",
    "    def _request(self, method: str, endpoint: str, **kwargs) -> requests.Response:\n",
    "        \"\"\"Make authenticated request.\"\"\"\n",
    "        url = f\"{self.api_url}/{endpoint.lstrip('/')}\"\n",
    "        kwargs.setdefault('verify', self.verify_ssl)\n",
    "        response = self.session.request(method, url, **kwargs)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    \n",
    "    def list_processors(self, process_group_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get all processors in a process group.\"\"\"\n",
    "        response = self._request(\"GET\", f\"/flow/process-groups/{process_group_id}/processors\")\n",
    "        return response.json().get('processors', [])\n",
    "    \n",
    "    def get_process_group_status(self, group_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get execution statistics for process group.\"\"\"\n",
    "        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}/status\")\n",
    "        return response.json()\n",
    "    \n",
    "    def get_processor_invocation_counts(self, group_id: str) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Extract invocation counts (recursive).\"\"\"\n",
    "        status_data = self.get_process_group_status(group_id)\n",
    "        processor_stats = {}\n",
    "        \n",
    "        pg_status = status_data.get(\"processGroupStatus\", {})\n",
    "        if not pg_status:\n",
    "            return processor_stats\n",
    "        \n",
    "        # Extract from current group\n",
    "        for proc_status in pg_status.get(\"processorStatus\", []):\n",
    "            try:\n",
    "                proc_id = proc_status[\"id\"]\n",
    "                processor_stats[proc_id] = {\n",
    "                    \"name\": proc_status[\"name\"],\n",
    "                    \"type\": proc_status[\"type\"].split('.')[-1],\n",
    "                    \"invocations\": proc_status.get(\"aggregateSnapshot\", {}).get(\"invocations\", 0)\n",
    "                }\n",
    "            except KeyError as e:\n",
    "                logger.error(f\"Missing key in processor status: {e}\")\n",
    "        \n",
    "        # Recurse into child groups\n",
    "        for child_pg_status in pg_status.get(\"processGroupStatus\", []):\n",
    "            try:\n",
    "                child_id = child_pg_status[\"id\"]\n",
    "                child_stats = self.get_processor_invocation_counts(child_id)\n",
    "                processor_stats.update(child_stats)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing child group: {e}\")\n",
    "        \n",
    "        return processor_stats\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close session.\"\"\"\n",
    "        self.session.close()\n",
    "\n",
    "console.print(\"[green]✓ NiFiClient class defined![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Multi-Flow Analyzer Class\n",
    "\n",
    "class MultiFlowAnalyzer:\n",
    "    \"\"\"Analyzes multiple NiFi flows and stores results in Delta Lake.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: NiFiClient):\n",
    "        self.client = client\n",
    "        self.console = Console()\n",
    "        self.all_results = []  # Store all flow results\n",
    "        self.snapshot_timestamp = datetime.now()\n",
    "    \n",
    "    def analyze_flow(self, flow_id: str, flow_name: str) -> Dict:\n",
    "        \"\"\"Analyze a single flow.\"\"\"\n",
    "        flow_results = {\n",
    "            'flow_name': flow_name,\n",
    "            'flow_id': flow_id,\n",
    "            'processor_count': 0,\n",
    "            'processors': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Get processors\n",
    "            processors = self.client.list_processors(flow_id)\n",
    "            flow_results['processor_count'] = len(processors)\n",
    "            \n",
    "            # Get execution counts\n",
    "            exec_stats = self.client.get_processor_invocation_counts(flow_id)\n",
    "            \n",
    "            # Build processor data\n",
    "            for proc in processors:\n",
    "                proc_id = proc['id']\n",
    "                proc_name = proc['component']['name']\n",
    "                proc_type = proc['component']['type'].split('.')[-1]\n",
    "                invocations = exec_stats.get(proc_id, {}).get('invocations', 0)\n",
    "                \n",
    "                flow_results['processors'].append({\n",
    "                    'snapshot_timestamp': self.snapshot_timestamp,\n",
    "                    'flow_name': flow_name,\n",
    "                    'process_group_id': flow_id,\n",
    "                    'processor_id': proc_id,\n",
    "                    'processor_name': proc_name,\n",
    "                    'processor_type': proc_type,\n",
    "                    'invocations': invocations\n",
    "                })\n",
    "            \n",
    "            return flow_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[red]ERROR[/red] Failed to analyze {flow_name}: {e}\")\n",
    "            flow_results['error'] = str(e)\n",
    "            return flow_results\n",
    "    \n",
    "    def analyze_all_flows(self, flows_csv_path: str):\n",
    "        \"\"\"Analyze all flows from CSV.\"\"\"\n",
    "        self.console.print(f\"\\n[cyan]Multi-Flow Analysis Starting...[/cyan]\")\n",
    "        self.console.print(f\"  Timestamp: {self.snapshot_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Read flows CSV\n",
    "        try:\n",
    "            flows_df = spark.read.csv(flows_csv_path, header=True)\n",
    "            flows = flows_df.collect()\n",
    "            \n",
    "            self.console.print(f\"[green]Found {len(flows)} flows to analyze[/green]\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[red]ERROR[/red] Failed to read CSV: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Analyze each flow\n",
    "        with Progress(\n",
    "            SpinnerColumn(),\n",
    "            TextColumn(\"[progress.description]{task.description}\"),\n",
    "            BarColumn(),\n",
    "            console=self.console\n",
    "        ) as progress:\n",
    "            task = progress.add_task(\"Analyzing flows...\", total=len(flows))\n",
    "            \n",
    "            for flow in flows:\n",
    "                flow_id = flow['id']\n",
    "                flow_name = flow['flow_name']\n",
    "                \n",
    "                progress.update(task, description=f\"Analyzing: {flow_name}\")\n",
    "                \n",
    "                flow_results = self.analyze_flow(flow_id, flow_name)\n",
    "                self.all_results.append(flow_results)\n",
    "                \n",
    "                # Display flow summary\n",
    "                if 'error' not in flow_results:\n",
    "                    self.console.print(\n",
    "                        f\"  [green]✓[/green] {flow_name}: {flow_results['processor_count']} processors\"\n",
    "                    )\n",
    "                else:\n",
    "                    self.console.print(\n",
    "                        f\"  [red]✗[/red] {flow_name}: {flow_results['error']}\"\n",
    "                    )\n",
    "                \n",
    "                progress.advance(task)\n",
    "        \n",
    "        # Display overall summary\n",
    "        self.display_summary()\n",
    "    \n",
    "    def display_summary(self):\n",
    "        \"\"\"Display analysis summary.\"\"\"\n",
    "        total_processors = sum(r['processor_count'] for r in self.all_results if 'error' not in r)\n",
    "        successful_flows = sum(1 for r in self.all_results if 'error' not in r)\n",
    "        failed_flows = sum(1 for r in self.all_results if 'error' in r)\n",
    "        \n",
    "        self.console.print(f\"\\n[cyan]Overall Summary:[/cyan]\")\n",
    "        self.console.print(f\"  Total flows: {len(self.all_results)}\")\n",
    "        self.console.print(f\"  Successful: {successful_flows}\")\n",
    "        self.console.print(f\"  Failed: {failed_flows}\")\n",
    "        self.console.print(f\"  Total processors: {total_processors}\")\n",
    "        \n",
    "        # Create summary table\n",
    "        table = Table(title=\"\\nFlow Analysis Summary\")\n",
    "        table.add_column(\"Flow Name\", style=\"cyan\")\n",
    "        table.add_column(\"Processors\", justify=\"right\", style=\"yellow\")\n",
    "        table.add_column(\"Status\", style=\"green\")\n",
    "        \n",
    "        for result in self.all_results:\n",
    "            status = \"[red]Error[/red]\" if 'error' in result else \"[green]Success[/green]\"\n",
    "            table.add_row(\n",
    "                result['flow_name'],\n",
    "                str(result['processor_count']),\n",
    "                status\n",
    "            )\n",
    "        \n",
    "        self.console.print(table)\n",
    "    \n",
    "    def get_results_dataframe(self):\n",
    "        \"\"\"Convert all results to Spark DataFrame.\"\"\"\n",
    "        all_rows = []\n",
    "        \n",
    "        for flow_result in self.all_results:\n",
    "            if 'error' not in flow_result:\n",
    "                all_rows.extend(flow_result['processors'])\n",
    "        \n",
    "        if not all_rows:\n",
    "            return None\n",
    "        \n",
    "        # Convert to list of tuples\n",
    "        rows = [\n",
    "            (\n",
    "                row['snapshot_timestamp'],\n",
    "                row['flow_name'],\n",
    "                row['process_group_id'],\n",
    "                row['processor_id'],\n",
    "                row['processor_name'],\n",
    "                row['processor_type'],\n",
    "                row['invocations']\n",
    "            )\n",
    "            for row in all_rows\n",
    "        ]\n",
    "        \n",
    "        # Define schema with flow_name\n",
    "        schema = StructType([\n",
    "            StructField(\"snapshot_timestamp\", TimestampType(), False),\n",
    "            StructField(\"flow_name\", StringType(), False),\n",
    "            StructField(\"process_group_id\", StringType(), False),\n",
    "            StructField(\"processor_id\", StringType(), False),\n",
    "            StructField(\"processor_name\", StringType(), False),\n",
    "            StructField(\"processor_type\", StringType(), False),\n",
    "            StructField(\"invocations\", LongType(), False)\n",
    "        ])\n",
    "        \n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        return spark.createDataFrame(rows, schema)\n",
    "\n",
    "console.print(\"[green]✓ MultiFlowAnalyzer class defined![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run Multi-Flow Analysis\n",
    "\n",
    "console.print(\"\\n[cyan]Starting Multi-Flow NiFi Analysis...[/cyan]\\n\")\n",
    "\n",
    "# Connect to NiFi\n",
    "console.print(\"[yellow]Connecting to NiFi...[/yellow]\")\n",
    "client = NiFiClient(\n",
    "    base_url=CONFIG['nifi_url'],\n",
    "    username=CONFIG['username'],\n",
    "    password=CONFIG['password'],\n",
    "    verify_ssl=CONFIG['verify_ssl']\n",
    ")\n",
    "console.print(\"[green]OK[/green] Connected successfully\\n\")\n",
    "\n",
    "# Create analyzer and run analysis\n",
    "analyzer = MultiFlowAnalyzer(client)\n",
    "analyzer.analyze_all_flows(CONFIG['flows_csv_path'])\n",
    "\n",
    "# Cleanup\n",
    "client.close()\n",
    "\n",
    "console.print(\"\\n[green]✓ Multi-flow analysis complete![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save Snapshots to Delta Lake\n",
    "\n",
    "if CONFIG['enable_snapshots']:\n",
    "    console.print(\"\\n[yellow]Saving snapshots to Delta Lake...[/yellow]\")\n",
    "    \n",
    "    df = analyzer.get_results_dataframe()\n",
    "    \n",
    "    if df is not None:\n",
    "        table_name = f\"{CONFIG['delta_database']}.{CONFIG['delta_table_name']}\"\n",
    "        \n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(table_name)\n",
    "        \n",
    "        console.print(f\"[green]OK[/green] Snapshots saved to {table_name}\")\n",
    "        console.print(f\"  Timestamp: {analyzer.snapshot_timestamp}\")\n",
    "        console.print(f\"  Total rows: {df.count()}\")\n",
    "        \n",
    "        # Show sample\n",
    "        console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n",
    "        display(df.limit(10))\n",
    "    else:\n",
    "        console.print(\"[red]ERROR[/red] No data to save\")\n",
    "else:\n",
    "    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Query Historical Snapshots by Flow\n",
    "\n",
    "if CONFIG['enable_snapshots']:\n",
    "    console.print(\"\\n[cyan]Querying historical snapshots by flow...[/cyan]\\n\")\n",
    "    \n",
    "    table_name = f\"{CONFIG['delta_database']}.{CONFIG['delta_table_name']}\"\n",
    "    \n",
    "    try:\n",
    "        # Show snapshots per flow\n",
    "        console.print(\"[yellow]Snapshot count by flow:[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                flow_name,\n",
    "                COUNT(DISTINCT snapshot_timestamp) as snapshots,\n",
    "                COUNT(*) as total_processors,\n",
    "                MAX(snapshot_timestamp) as last_snapshot\n",
    "            FROM {table_name}\n",
    "            GROUP BY flow_name\n",
    "            ORDER BY flow_name\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # Find inactive processors by flow (last 7 days)\n",
    "        console.print(\"\\n[yellow]Inactive processors by flow (last 7 days):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            WITH recent_activity AS (\n",
    "                SELECT \n",
    "                    flow_name,\n",
    "                    processor_name,\n",
    "                    processor_type,\n",
    "                    MAX(invocations) - MIN(invocations) as delta_invocations,\n",
    "                    MIN(snapshot_timestamp) as first_snapshot,\n",
    "                    MAX(snapshot_timestamp) as last_snapshot\n",
    "                FROM {table_name}\n",
    "                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "                GROUP BY flow_name, processor_name, processor_type\n",
    "            )\n",
    "            SELECT \n",
    "                flow_name,\n",
    "                COUNT(*) as inactive_processor_count\n",
    "            FROM recent_activity\n",
    "            WHERE delta_invocations = 0\n",
    "            GROUP BY flow_name\n",
    "            ORDER BY inactive_processor_count DESC\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # Detailed view of inactive processors\n",
    "        console.print(\"\\n[yellow]Detailed inactive processor list:[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            WITH recent_activity AS (\n",
    "                SELECT \n",
    "                    flow_name,\n",
    "                    processor_name,\n",
    "                    processor_type,\n",
    "                    MAX(invocations) - MIN(invocations) as delta_invocations\n",
    "                FROM {table_name}\n",
    "                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "                GROUP BY flow_name, processor_name, processor_type\n",
    "            )\n",
    "            SELECT \n",
    "                flow_name,\n",
    "                processor_name,\n",
    "                processor_type,\n",
    "                delta_invocations\n",
    "            FROM recent_activity\n",
    "            WHERE delta_invocations = 0\n",
    "            ORDER BY flow_name, processor_name\n",
    "            LIMIT 50\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]ERROR[/red] Failed to query: {e}\")\n",
    "else:\n",
    "    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Export Results to CSV by Flow\n",
    "\n",
    "console.print(\"\\n[yellow]Exporting results to CSV...[/yellow]\")\n",
    "\n",
    "timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "df = analyzer.get_results_dataframe()\n",
    "if df is not None:\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    # Export overall summary\n",
    "    output_path = f\"/dbfs/nifi_analysis/all_flows_{timestamp_str}.csv\"\n",
    "    pdf.to_csv(output_path, index=False)\n",
    "    console.print(f\"[green]OK[/green] All flows exported to {output_path}\")\n",
    "    \n",
    "    # Export per flow\n",
    "    for flow_name in pdf['flow_name'].unique():\n",
    "        flow_df = pdf[pdf['flow_name'] == flow_name]\n",
    "        flow_path = f\"/dbfs/nifi_analysis/{flow_name}_{timestamp_str}.csv\"\n",
    "        flow_df.to_csv(flow_path, index=False)\n",
    "        console.print(f\"  [green]✓[/green] {flow_name}: {len(flow_df)} processors\")\n",
    "    \n",
    "    console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n",
    "    display(pdf.head(10))\n",
    "else:\n",
    "    console.print(\"[red]ERROR[/red] No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Updated Delta Table Schema\n",
    "\n",
    "The Delta table now includes:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `snapshot_timestamp` | Timestamp | **When the snapshot was captured** |\n",
    "| `flow_name` | String | **Flow name from CSV** |\n",
    "| `process_group_id` | String | NiFi process group ID |\n",
    "| `processor_id` | String | Processor unique ID |\n",
    "| `processor_name` | String | Processor name |\n",
    "| `processor_type` | String | Processor type |\n",
    "| `invocations` | Long | Execution count |\n",
    "\n",
    "## Example Queries\n",
    "\n",
    "```sql\n",
    "-- Find all inactive processors across all flows (last 7 days)\n",
    "WITH activity AS (\n",
    "    SELECT \n",
    "        flow_name,\n",
    "        processor_name,\n",
    "        MAX(invocations) - MIN(invocations) as delta\n",
    "    FROM default.nifi_processor_snapshots\n",
    "    WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "    GROUP BY flow_name, processor_name\n",
    ")\n",
    "SELECT * FROM activity WHERE delta = 0;\n",
    "\n",
    "-- Compare flows to see which has most inactive processors\n",
    "SELECT \n",
    "    flow_name,\n",
    "    COUNT(*) as total_processors,\n",
    "    SUM(CASE WHEN invocations = 0 THEN 1 ELSE 0 END) as unused_processors\n",
    "FROM (\n",
    "    SELECT flow_name, processor_name, MAX(invocations) as invocations\n",
    "    FROM default.nifi_processor_snapshots\n",
    "    WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "    GROUP BY flow_name, processor_name\n",
    ")\n",
    "GROUP BY flow_name\n",
    "ORDER BY unused_processors DESC;\n",
    "```\n",
    "\n",
    "## CSV Format\n",
    "\n",
    "Your `flows.csv` should look like:\n",
    "```\n",
    "id,flow_name\n",
    "8c8677c4-29d6-3607-a32e-1234567890ab,Production_Data_Pipeline\n",
    "abc-123-def-456-7890-abcdef123456,Development_Testing_Flow\n",
    "xyz-789-ghi-012-3456-7890abcdef12,QA_Validation_Flow\n",
    "```\n",
    "\n",
    "Upload it to: `/dbfs/nifi_analysis/flows.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
