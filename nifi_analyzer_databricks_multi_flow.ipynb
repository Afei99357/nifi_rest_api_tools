{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NiFi Processor Usage Analyzer - Multi-Flow Edition\n",
    "\n",
    "This notebook analyzes NiFi processor execution counts across **multiple process groups** to identify unused or underutilized processors.\n",
    "\n",
    "**Features:**\n",
    "- Analyzes multiple flows from CSV input\n",
    "- Fast execution count analysis (~5-10 seconds per flow)\n",
    "- Snapshot mode with flow_name tracking\n",
    "- Delta Lake integration with timestamp\n",
    "- Standalone - no external files needed\n",
    "\n",
    "**Setup:**\n",
    "1. Upload CSV with flow definitions (id, flow_name)\n",
    "2. Edit the configuration in Cell 3\n",
    "3. Run all cells\n",
    "4. View results in Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "%pip install requests rich --quiet\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\n",
    "\n",
    "# Databricks-specific imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "# Disable SSL warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('nifi_analyzer')\n",
    "\n",
    "# Initialize Rich console\n",
    "console = Console()\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Configuration\n# EDIT THESE VALUES FOR YOUR NIFI INSTANCE\n\nCONFIG = {\n    # NiFi Connection\n    'nifi_url': 'https://thbnk01hdpnp002.th-bnk01.nxp.com:8443/nifi',\n    'username': 'nxg16670',\n    'password': 'your-password-here',  # ← EDIT THIS\n    'verify_ssl': False,\n    \n    # Server Identifier (for tracking multiple NiFi servers)\n    'server': 'prod-nifi-01',  # ← EDIT THIS (e.g., 'prod', 'dev', hostname)\n    \n    # Flow Definitions CSV\n    # CSV Format: id,flow_name\n    # Example:\n    #   8c8677c4-29d6-36...,Production_Flow_1\n    #   abc-123-def...,Development_Flow_2\n    'flows_csv_path': '/dbfs/nifi_analysis/flows.csv',  # ← Path to your CSV\n    \n    # Snapshot Storage (Unity Catalog - 3-level naming)\n    'enable_snapshots': True,\n    'delta_table_path': 'main.default.nifi_processor_snapshots',  # catalog.schema.table\n}\n\nconsole.print(\"[green]✓ Configuration loaded![/green]\")\nconsole.print(f\"  NiFi URL: {CONFIG['nifi_url']}\")\nconsole.print(f\"  Username: {CONFIG['username']}\")\nconsole.print(f\"  Server: {CONFIG['server']}\")\nconsole.print(f\"  Flows CSV: {CONFIG['flows_csv_path']}\")\nconsole.print(f\"  Delta table: {CONFIG['delta_table_path']}\")\nconsole.print(f\"  Snapshots enabled: {CONFIG['enable_snapshots']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: NiFi Client Class\n\nclass NiFiClient:\n    \"\"\"Client for interacting with Apache NiFi REST API.\"\"\"\n    \n    def __init__(self, base_url: str, username: str, password: str, verify_ssl: bool = True):\n        self.base_url = base_url.rstrip('/')\n        if not self.base_url.endswith('/nifi'):\n            self.base_url += '/nifi'\n        self.api_url = f\"{self.base_url}-api\"\n        self.verify_ssl = verify_ssl\n        self.session = requests.Session()\n        self.token = None\n        self.username = username\n        self.password = password\n        self._authenticate(username, password)\n        \n    def _authenticate(self, username: str, password: str) -> None:\n        \"\"\"Authenticate with NiFi.\"\"\"\n        try:\n            response = requests.post(\n                f\"{self.api_url}/access/token\",\n                data={'username': username, 'password': password},\n                verify=self.verify_ssl\n            )\n            \n            if response.status_code == 201:\n                self.token = response.text\n                self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n                logger.info(\"Successfully authenticated with token\")\n            else:\n                logger.warning(f\"Token auth failed with status {response.status_code}\")\n                logger.warning(\"Falling back to basic auth\")\n                from requests.auth import HTTPBasicAuth\n                self.session.auth = HTTPBasicAuth(username, password)\n        except Exception as e:\n            logger.warning(f\"Token auth error: {e}, falling back to basic auth\")\n            from requests.auth import HTTPBasicAuth\n            self.session.auth = HTTPBasicAuth(username, password)\n    \n    def _request(self, method: str, endpoint: str, **kwargs) -> requests.Response:\n        \"\"\"Make authenticated request with 401 retry.\"\"\"\n        url = f\"{self.api_url}/{endpoint.lstrip('/')}\"\n        kwargs.setdefault('verify', self.verify_ssl)\n        \n        response = self.session.request(method, url, **kwargs)\n        \n        # Handle 401 by re-authenticating once\n        if response.status_code == 401:\n            logger.warning(\"Received 401, attempting re-authentication\")\n            self._authenticate(self.username, self.password)\n            response = self.session.request(method, url, **kwargs)\n            if response.status_code == 401:\n                raise Exception(\"Authentication failed: Unauthorized\")\n        \n        response.raise_for_status()\n        return response\n    \n    def get_process_group(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get process group details including all processors.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}\")\n        return response.json()\n    \n    def list_processors(self, process_group_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Get all processors in a process group (recursive).\"\"\"\n        pg_data = self.get_process_group(process_group_id)\n        processors = pg_data[\"processGroupFlow\"][\"flow\"][\"processors\"]\n        \n        # Recursively get processors from child groups\n        child_groups = pg_data[\"processGroupFlow\"][\"flow\"][\"processGroups\"]\n        for child in child_groups:\n            processors.extend(self.list_processors(child[\"id\"]))\n        \n        return processors\n    \n    def get_process_group_status(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get execution statistics for process group.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}/status\")\n        return response.json()\n    \n    def get_connection_statistics(self, group_id: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extract ALL connection statistics from Status API with processor IDs enriched from Flow API.\n\n        This method:\n        1. Gets connection details from Flow API (has source/destinationId)\n        2. Gets connection statistics from Status API (has metrics)\n        3. Merges them by connection ID to get both IDs and metrics\n\n        Args:\n            group_id: Process group ID\n\n        Returns:\n            List of connection dictionaries with all available fields including processor IDs\n\n        Example:\n            [\n                {\n                    'id': 'conn-abc-123',\n                    'sourceId': 'proc-123',  # From Flow API\n                    'sourceName': 'FetchSFTP',\n                    'destinationId': 'proc-456',  # From Flow API\n                    'destinationName': 'PutHDFS',\n                    'flowFilesOut': 1250,\n                    ...\n                },\n                ...\n            ]\n        \"\"\"\n        # Step 1: Get connection details from Flow API (has processor IDs)\n        pg_data = self.get_process_group(group_id)\n        connections_with_ids = {}\n\n        # Build lookup: connection_id -> {sourceId, destinationId, sourceName, destinationName}\n        flow_connections = pg_data.get(\"processGroupFlow\", {}).get(\"flow\", {}).get(\"connections\", [])\n        for conn in flow_connections:\n            conn_id = conn.get(\"id\")\n            source_info = conn.get(\"source\", {})\n            dest_info = conn.get(\"destination\", {})\n\n            connections_with_ids[conn_id] = {\n                'sourceId': source_info.get('id'),\n                'sourceName': source_info.get('name'),\n                'destinationId': dest_info.get('id'),\n                'destinationName': dest_info.get('name'),\n                'groupId': conn.get('parentGroupId')\n            }\n\n        # Step 2: Get connection statistics from Status API (has metrics)\n        status_data = self.get_process_group_status(group_id)\n        all_connections = []\n\n        pg_status = status_data.get(\"processGroupStatus\", {})\n        if not pg_status:\n            logger.warning(f\"No 'processGroupStatus' key in response for group {group_id}\")\n            return all_connections\n\n        connection_statuses = pg_status.get(\"aggregateSnapshot\", {}).get(\"connectionStatusSnapshots\", [])\n        logger.debug(f\"Found {len(connection_statuses)} connection statuses in group {group_id[:8]}\")\n\n        # Step 3: Merge connection IDs from Flow API with metrics from Status API\n        for conn_status in connection_statuses:\n            conn_snap = conn_status.get(\"connectionStatusSnapshot\", {})\n            conn_id = conn_snap.get('id')\n\n            # Get processor IDs from Flow API data\n            conn_ids = connections_with_ids.get(conn_id, {})\n\n            # Merge: processor IDs from Flow API + metrics from Status API\n            connection_data = {\n                # Connection identity with REAL processor IDs\n                'id': conn_id,\n                'groupId': conn_ids.get('groupId') or conn_snap.get('groupId'),\n                'name': conn_snap.get('name', ''),\n                'sourceId': conn_ids.get('sourceId'),  # From Flow API\n                'sourceName': conn_ids.get('sourceName') or conn_snap.get('sourceName'),\n                'destinationId': conn_ids.get('destinationId'),  # From Flow API\n                'destinationName': conn_ids.get('destinationName') or conn_snap.get('destinationName'),\n\n                # Flow metrics from Status API (6 fields)\n                'flowFilesIn': conn_snap.get('flowFilesIn', 0),\n                'flowFilesOut': conn_snap.get('flowFilesOut', 0),\n                'bytesIn': conn_snap.get('bytesIn', 0),\n                'bytesOut': conn_snap.get('bytesOut', 0),\n                'input': conn_snap.get('input', ''),\n                'output': conn_snap.get('output', ''),\n\n                # Queue metrics from Status API (4 fields)\n                'queuedCount': conn_snap.get('queuedCount', 0),\n                'queuedBytes': conn_snap.get('queuedBytes', 0),\n                'queued': conn_snap.get('queued', ''),\n                'queuedSize': conn_snap.get('queuedSize', ''),\n\n                # Status indicators from Status API (2 fields)\n                'percentUseCount': conn_snap.get('percentUseCount', 0),\n                'percentUseBytes': conn_snap.get('percentUseBytes', 0),\n\n                # Timestamps (1 field)\n                'statsLastRefreshed': conn_snap.get('statsLastRefreshed', ''),\n            }\n\n            all_connections.append(connection_data)\n\n        logger.debug(f\"Extracted {len(all_connections)} connection records with processor IDs\")\n\n        # Recursively process child groups\n        child_groups = pg_data.get(\"processGroupFlow\", {}).get(\"flow\", {}).get(\"processGroups\", [])\n        logger.debug(f\"Found {len(child_groups)} child groups in {group_id[:8]}\")\n\n        for child_pg in child_groups:\n            try:\n                child_id = child_pg[\"id\"]\n                child_name = child_pg.get(\"component\", {}).get(\"name\", \"unknown\")\n                logger.debug(f\"Recursing into child group: {child_name} ({child_id[:8]})\")\n                child_connections = self.get_connection_statistics(child_id)\n                all_connections.extend(child_connections)\n                logger.debug(f\"Added {len(child_connections)} connections from child group {child_name}\")\n            except Exception as e:\n                logger.error(f\"Error processing child group: {e}\")\n\n        logger.info(f\"Group {group_id[:8]}: collected {len(all_connections)} total connections with processor IDs\")\n        return all_connections\n    \n    def close(self):\n        \"\"\"Close session.\"\"\"\n        self.session.close()\n\nconsole.print(\"[green]✓ NiFiClient class defined![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Multi-Flow Analyzer Class\n\nclass MultiFlowAnalyzer:\n    \"\"\"Analyzes multiple NiFi flows and stores results in Delta Lake.\"\"\"\n    \n    def __init__(self, client: NiFiClient, server: str = 'unknown'):\n        self.client = client\n        self.console = Console()\n        self.server = server  # Server identifier (e.g., hostname or environment)\n        self.all_results = []  # Store all flow results\n        self.snapshot_timestamp = datetime.now()\n    \n    def analyze_flow(self, flow_id: str, flow_name: str) -> Dict:\n        \"\"\"Analyze a single flow.\"\"\"\n        flow_results = {\n            'flow_name': flow_name,\n            'flow_id': flow_id,\n            'connection_count': 0,\n            'connections': []\n        }\n        \n        try:\n            # Get connection statistics (ALL 15+ fields)\n            connections = self.client.get_connection_statistics(flow_id)\n            flow_results['connection_count'] = len(connections)\n            \n            # Build connection data with all 24 fields\n            for conn in connections:\n                flow_results['connections'].append({\n                    # Metadata (4 fields)\n                    'snapshot_timestamp': self.snapshot_timestamp,\n                    'server': self.server,\n                    'flow_name': flow_name,\n                    'process_group_id': flow_id,\n                    \n                    # Connection identity (3 fields)\n                    'connection_id': conn.get('id'),\n                    'connection_name': conn.get('name', ''),\n                    'connection_group_id': conn.get('groupId'),\n                    \n                    # Source processor (2 fields)\n                    'source_id': conn.get('sourceId'),\n                    'source_name': conn.get('sourceName'),\n                    \n                    # Destination processor (2 fields)\n                    'destination_id': conn.get('destinationId'),\n                    'destination_name': conn.get('destinationName'),\n                    \n                    # Flow metrics - 5-minute window (6 fields)\n                    'flow_files_in': conn.get('flowFilesIn', 0),\n                    'flow_files_out': conn.get('flowFilesOut', 0),\n                    'bytes_in': conn.get('bytesIn', 0),\n                    'bytes_out': conn.get('bytesOut', 0),\n                    'input': conn.get('input', ''),\n                    'output': conn.get('output', ''),\n                    \n                    # Queue metrics - current state (4 fields)\n                    'queued_count': conn.get('queuedCount', 0),\n                    'queued_bytes': conn.get('queuedBytes', 0),\n                    'queued': conn.get('queued', ''),\n                    'queued_size': conn.get('queuedSize', ''),\n                    \n                    # Status indicators (2 fields)\n                    'percent_use_count': conn.get('percentUseCount', 0),\n                    'percent_use_bytes': conn.get('percentUseBytes', 0),\n                    \n                    # Timestamps (1 field)\n                    'stats_last_refreshed': conn.get('statsLastRefreshed', '')\n                })\n            \n            return flow_results\n            \n        except Exception as e:\n            self.console.print(f\"[red]ERROR[/red] Failed to analyze {flow_name}: {e}\")\n            flow_results['error'] = str(e)\n            return flow_results\n    \n    def analyze_all_flows(self, flows_csv_path: str):\n        \"\"\"Analyze all flows from CSV.\"\"\"\n        self.console.print(f\"\\n[cyan]Multi-Flow Analysis Starting...[/cyan]\")\n        self.console.print(f\"  Server: {self.server}\")\n        self.console.print(f\"  Timestamp: {self.snapshot_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        \n        # Read flows CSV\n        try:\n            flows_df = spark.read.csv(flows_csv_path, header=True)\n            flows = flows_df.collect()\n            \n            self.console.print(f\"[green]Found {len(flows)} flows to analyze[/green]\\n\")\n            \n        except Exception as e:\n            self.console.print(f\"[red]ERROR[/red] Failed to read CSV: {e}\")\n            raise\n        \n        # Analyze each flow\n        with Progress(\n            SpinnerColumn(),\n            TextColumn(\"[progress.description]{task.description}\"),\n            BarColumn(),\n            console=self.console\n        ) as progress:\n            task = progress.add_task(\"Analyzing flows...\", total=len(flows))\n            \n            for flow in flows:\n                flow_id = flow['id']\n                flow_name = flow['flow_name']\n                \n                progress.update(task, description=f\"Analyzing: {flow_name}\")\n                \n                flow_results = self.analyze_flow(flow_id, flow_name)\n                self.all_results.append(flow_results)\n                \n                # Display flow summary\n                if 'error' not in flow_results:\n                    self.console.print(\n                        f\"  [green]✓[/green] {flow_name}: {flow_results['connection_count']} connections\"\n                    )\n                else:\n                    self.console.print(\n                        f\"  [red]✗[/red] {flow_name}: {flow_results['error']}\"\n                    )\n                \n                progress.advance(task)\n        \n        # Display overall summary\n        self.display_summary()\n    \n    def display_summary(self):\n        \"\"\"Display analysis summary.\"\"\"\n        total_connections = sum(r['connection_count'] for r in self.all_results if 'error' not in r)\n        successful_flows = sum(1 for r in self.all_results if 'error' not in r)\n        failed_flows = sum(1 for r in self.all_results if 'error' in r)\n        \n        self.console.print(f\"\\n[cyan]Overall Summary:[/cyan]\")\n        self.console.print(f\"  Server: {self.server}\")\n        self.console.print(f\"  Total flows: {len(self.all_results)}\")\n        self.console.print(f\"  Successful: {successful_flows}\")\n        self.console.print(f\"  Failed: {failed_flows}\")\n        self.console.print(f\"  Total connections: {total_connections}\")\n        \n        # Create summary table\n        table = Table(title=\"\\nFlow Analysis Summary\")\n        table.add_column(\"Flow Name\", style=\"cyan\")\n        table.add_column(\"Connections\", justify=\"right\", style=\"yellow\")\n        table.add_column(\"Status\", style=\"green\")\n        \n        for result in self.all_results:\n            status = \"[red]Error[/red]\" if 'error' in result else \"[green]Success[/green]\"\n            table.add_row(\n                result['flow_name'],\n                str(result['connection_count']),\n                status\n            )\n        \n        self.console.print(table)\n    \n    def get_results_dataframe(self):\n        \"\"\"Convert all results to Spark DataFrame with 24-field connection schema.\"\"\"\n        all_rows = []\n        \n        for flow_result in self.all_results:\n            if 'error' not in flow_result:\n                all_rows.extend(flow_result['connections'])\n        \n        if not all_rows:\n            return None\n        \n        # Helper function to safely convert to int\n        def safe_int(value, default=0):\n            \"\"\"Convert value to int, handling strings and None.\"\"\"\n            if value is None:\n                return default\n            try:\n                return int(value)\n            except (ValueError, TypeError):\n                return default\n        \n        # Convert to list of tuples (24 fields) - with safe_int conversion\n        rows = [\n            (\n                row['snapshot_timestamp'],\n                row['server'],\n                row['flow_name'],\n                row['process_group_id'],\n                row['connection_id'],\n                row['connection_name'],\n                row['connection_group_id'],\n                row['source_id'],\n                row['source_name'],\n                row['destination_id'],\n                row['destination_name'],\n                safe_int(row['flow_files_in']),\n                safe_int(row['flow_files_out']),\n                safe_int(row['bytes_in']),\n                safe_int(row['bytes_out']),\n                row['input'],\n                row['output'],\n                safe_int(row['queued_count']),\n                safe_int(row['queued_bytes']),\n                row['queued'],\n                row['queued_size'],\n                safe_int(row['percent_use_count']),\n                safe_int(row['percent_use_bytes']),\n                row['stats_last_refreshed']\n            )\n            for row in all_rows\n        ]\n        \n        # Define schema with ALL 24 fields (connection-level)\n        schema = StructType([\n            # Metadata (4 fields)\n            StructField(\"snapshot_timestamp\", TimestampType(), False),\n            StructField(\"server\", StringType(), False),\n            StructField(\"flow_name\", StringType(), False),\n            StructField(\"process_group_id\", StringType(), False),\n            \n            # Connection identity (3 fields)\n            StructField(\"connection_id\", StringType(), True),\n            StructField(\"connection_name\", StringType(), True),\n            StructField(\"connection_group_id\", StringType(), True),\n            \n            # Source processor (2 fields)\n            StructField(\"source_id\", StringType(), True),\n            StructField(\"source_name\", StringType(), True),\n            \n            # Destination processor (2 fields)\n            StructField(\"destination_id\", StringType(), True),\n            StructField(\"destination_name\", StringType(), True),\n            \n            # Flow metrics (6 fields)\n            StructField(\"flow_files_in\", LongType(), False),\n            StructField(\"flow_files_out\", LongType(), False),\n            StructField(\"bytes_in\", LongType(), False),\n            StructField(\"bytes_out\", LongType(), False),\n            StructField(\"input\", StringType(), True),\n            StructField(\"output\", StringType(), True),\n            \n            # Queue metrics (4 fields)\n            StructField(\"queued_count\", LongType(), False),\n            StructField(\"queued_bytes\", LongType(), False),\n            StructField(\"queued\", StringType(), True),\n            StructField(\"queued_size\", StringType(), True),\n            \n            # Status indicators (2 fields)\n            StructField(\"percent_use_count\", LongType(), False),\n            StructField(\"percent_use_bytes\", LongType(), False),\n            \n            # Timestamps (1 field)\n            StructField(\"stats_last_refreshed\", StringType(), True)\n        ])\n        \n        spark = SparkSession.builder.getOrCreate()\n        return spark.createDataFrame(rows, schema)\n\nconsole.print(\"[green]✓ MultiFlowAnalyzer class defined![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Run Multi-Flow Analysis\n\nconsole.print(\"\\n[cyan]Starting Multi-Flow NiFi Analysis...[/cyan]\\n\")\n\n# Connect to NiFi\nconsole.print(\"[yellow]Connecting to NiFi...[/yellow]\")\nclient = NiFiClient(\n    base_url=CONFIG['nifi_url'],\n    username=CONFIG['username'],\n    password=CONFIG['password'],\n    verify_ssl=CONFIG['verify_ssl']\n)\nconsole.print(\"[green]OK[/green] Connected successfully\\n\")\n\n# Create analyzer and run analysis\nanalyzer = MultiFlowAnalyzer(client=client, server=CONFIG['server'])\nanalyzer.analyze_all_flows(CONFIG['flows_csv_path'])\n\n# Cleanup\nclient.close()\n\nconsole.print(\"\\n[green]✓ Multi-flow analysis complete![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Save Snapshots to Delta Lake\n\nif CONFIG['enable_snapshots']:\n    console.print(\"\\n[yellow]Saving snapshots to Delta Lake...[/yellow]\")\n    \n    df = analyzer.get_results_dataframe()\n    \n    if df is not None:\n        table_name = CONFIG['delta_table_path']\n        \n        # Check if table exists\n        table_exists = spark.catalog._jcatalog.tableExists(table_name)\n        \n        if not table_exists:\n            # First run: Create table\n            console.print(f\"[yellow]Table doesn't exist, creating: {table_name}[/yellow]\")\n            df.write \\\n                .format(\"delta\") \\\n                .mode(\"overwrite\") \\\n                .option(\"overwriteSchema\", \"true\") \\\n                .saveAsTable(table_name)\n            console.print(f\"[green]OK[/green] Table created successfully with 24-field connection-level schema\")\n        else:\n            # Subsequent runs: Append data\n            console.print(f\"[yellow]Table exists, appending data to: {table_name}[/yellow]\")\n            df.write \\\n                .format(\"delta\") \\\n                .mode(\"append\") \\\n                .option(\"mergeSchema\", \"true\") \\\n                .saveAsTable(table_name)\n            console.print(f\"[green]OK[/green] Data appended successfully\")\n        \n        console.print(f\"  Timestamp: {analyzer.snapshot_timestamp}\")\n        console.print(f\"  Total rows written: {df.count()}\")\n        \n        # Show sample\n        console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n        display(df.limit(10))\n    else:\n        console.print(\"[red]ERROR[/red] No data to save\")\nelse:\n    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")\n\n# NOTE: If you need to manually drop the table to start fresh, run this in a separate cell:\n# spark.sql(f\"DROP TABLE IF EXISTS {CONFIG['delta_table_path']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Query Historical Snapshots (Connection-Level Analysis)\n\nif CONFIG['enable_snapshots']:\n    console.print(\"\\n[cyan]Querying connection-level snapshots...[/cyan]\\n\")\n    \n    table_name = CONFIG['delta_table_path']\n    \n    try:\n        # Show snapshots per flow and server\n        console.print(\"[yellow]Snapshot count by server and flow:[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                flow_name,\n                COUNT(DISTINCT snapshot_timestamp) as snapshots,\n                COUNT(*) as total_connections,\n                MAX(snapshot_timestamp) as last_snapshot\n            FROM {table_name}\n            GROUP BY server, flow_name\n            ORDER BY server, flow_name\n        \"\"\").show(truncate=False)\n        \n        # NEW: Find connections with high queue depth (backpressure detection)\n        console.print(\"\\n[yellow]Connections with queued flowfiles (backpressure):[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                flow_name,\n                source_name,\n                destination_name,\n                MAX(queued_count) as max_queued_flowfiles,\n                MAX(queued_bytes) as max_queued_bytes,\n                MAX(percent_use_count) as max_percent_full\n            FROM {table_name}\n            WHERE queued_count > 0\n            GROUP BY server, flow_name, source_name, destination_name\n            ORDER BY max_queued_flowfiles DESC\n            LIMIT 20\n        \"\"\").show(truncate=False)\n        \n        # NEW: Identify connections approaching queue limits\n        console.print(\"\\n[yellow]Connections approaching queue limits (>50% full):[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                flow_name,\n                source_name,\n                destination_name,\n                MAX(percent_use_count) as max_percent_full,\n                MAX(queued_count) as max_queued_count\n            FROM {table_name}\n            WHERE percent_use_count > 50\n            GROUP BY server, flow_name, source_name, destination_name\n            ORDER BY max_percent_full DESC\n            LIMIT 20\n        \"\"\").show(truncate=False)\n        \n        # Find inactive connections (no flow for 7 days)\n        console.print(\"\\n[yellow]Inactive connections (no flowfiles for 7 days):[/yellow]\")\n        spark.sql(f\"\"\"\n            WITH connection_activity AS (\n                SELECT \n                    server,\n                    flow_name,\n                    source_name,\n                    destination_name,\n                    MAX(flow_files_out) - MIN(flow_files_out) as delta_flowfiles,\n                    MIN(snapshot_timestamp) as first_snapshot,\n                    MAX(snapshot_timestamp) as last_snapshot,\n                    COUNT(DISTINCT snapshot_timestamp) as num_snapshots\n                FROM {table_name}\n                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n                GROUP BY server, flow_name, source_name, destination_name\n            )\n            SELECT \n                server,\n                flow_name,\n                source_name,\n                destination_name,\n                delta_flowfiles,\n                num_snapshots\n            FROM connection_activity\n            WHERE delta_flowfiles = 0\n            ORDER BY server, flow_name, source_name\n            LIMIT 50\n        \"\"\").show(truncate=False)\n        \n        # Aggregate to processor level (still possible!)\n        console.print(\"\\n[yellow]Inactive processors by flow (aggregated from connections):[/yellow]\")\n        spark.sql(f\"\"\"\n            WITH processor_activity AS (\n                SELECT \n                    server,\n                    flow_name,\n                    source_name as processor_name,\n                    MAX(flow_files_out) - MIN(flow_files_out) as delta_flowfiles\n                FROM {table_name}\n                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n                GROUP BY server, flow_name, source_name\n            )\n            SELECT \n                server,\n                flow_name,\n                COUNT(*) as inactive_processor_count\n            FROM processor_activity\n            WHERE delta_flowfiles = 0\n            GROUP BY server, flow_name\n            ORDER BY server, inactive_processor_count DESC\n        \"\"\").show(truncate=False)\n        \n        # NEW: Track queue growth over time\n        console.print(\"\\n[yellow]Queue depth trends (hourly averages):[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                DATE_TRUNC('hour', snapshot_timestamp) as hour,\n                server,\n                flow_name,\n                source_name,\n                destination_name,\n                AVG(queued_count) as avg_queued_flowfiles,\n                MAX(queued_count) as max_queued_flowfiles\n            FROM {table_name}\n            WHERE snapshot_timestamp >= current_date() - INTERVAL 1 DAYS\n              AND queued_count > 0\n            GROUP BY hour, server, flow_name, source_name, destination_name\n            ORDER BY hour DESC, avg_queued_flowfiles DESC\n            LIMIT 20\n        \"\"\").show(truncate=False)\n        \n        # NEW: Bidirectional flow analysis\n        console.print(\"\\n[yellow]Flow balance (input vs output by connection):[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                server,\n                flow_name,\n                source_name,\n                destination_name,\n                SUM(flow_files_in) as total_flowfiles_in,\n                SUM(flow_files_out) as total_flowfiles_out,\n                SUM(flow_files_in) - SUM(flow_files_out) as net_change\n            FROM {table_name}\n            WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n            GROUP BY server, flow_name, source_name, destination_name\n            HAVING ABS(SUM(flow_files_in) - SUM(flow_files_out)) > 100\n            ORDER BY ABS(net_change) DESC\n            LIMIT 20\n        \"\"\").show(truncate=False)\n        \n    except Exception as e:\n        console.print(f\"[red]ERROR[/red] Failed to query: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Export Results to CSV by Flow\n",
    "\n",
    "console.print(\"\\n[yellow]Exporting results to CSV...[/yellow]\")\n",
    "\n",
    "timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "df = analyzer.get_results_dataframe()\n",
    "if df is not None:\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    # Export overall summary\n",
    "    output_path = f\"/dbfs/nifi_analysis/all_flows_{timestamp_str}.csv\"\n",
    "    pdf.to_csv(output_path, index=False)\n",
    "    console.print(f\"[green]OK[/green] All flows exported to {output_path}\")\n",
    "    \n",
    "    # Export per flow\n",
    "    for flow_name in pdf['flow_name'].unique():\n",
    "        flow_df = pdf[pdf['flow_name'] == flow_name]\n",
    "        flow_path = f\"/dbfs/nifi_analysis/{flow_name}_{timestamp_str}.csv\"\n",
    "        flow_df.to_csv(flow_path, index=False)\n",
    "        console.print(f\"  [green]✓[/green] {flow_name}: {len(flow_df)} processors\")\n",
    "    \n",
    "    console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n",
    "    display(pdf.head(10))\n",
    "else:\n",
    "    console.print(\"[red]ERROR[/red] No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Updated Delta Table Schema (Connection-Level)\n\nThe Delta table now captures **ALL available fields** from NiFi Status API at the **connection level** (not processor level). This provides maximum granularity for analysis.\n\n### 24 Total Fields\n\n| Column | Type | Description |\n|--------|------|-------------|\n| **Metadata (4 fields)** | | |\n| `snapshot_timestamp` | Timestamp | When the snapshot was captured |\n| `server` | String | Server identifier (hostname, environment name) |\n| `flow_name` | String | Flow name from CSV |\n| `process_group_id` | String | NiFi process group ID |\n| **Connection Identity (3 fields)** | | |\n| `connection_id` | String | Connection UUID |\n| `connection_name` | String | Connection name (often empty or \"success\") |\n| `connection_group_id` | String | Parent process group ID |\n| **Source Processor (2 fields)** | | |\n| `source_id` | String | Source processor UUID |\n| `source_name` | String | Source processor name |\n| **Destination Processor (2 fields)** | | |\n| `destination_id` | String | Destination processor UUID |\n| `destination_name` | String | Destination processor name |\n| **Flow Metrics - 5-minute window (6 fields)** | | |\n| `flow_files_in` | Long | FlowFiles entering connection |\n| `flow_files_out` | Long | FlowFiles leaving connection |\n| `bytes_in` | Long | Bytes entering connection |\n| `bytes_out` | Long | Bytes leaving connection |\n| `input` | String | Formatted input stats (e.g., \"1,250 (50.8 KB)\") |\n| `output` | String | Formatted output stats |\n| **Queue Metrics - current state (4 fields)** | | |\n| `queued_count` | Long | FlowFiles currently queued |\n| `queued_bytes` | Long | Bytes currently queued |\n| `queued` | String | Formatted queue stats |\n| `queued_size` | String | Formatted queue size |\n| **Status Indicators (2 fields)** | | |\n| `percent_use_count` | Long | % of queue count threshold used |\n| `percent_use_bytes` | Long | % of queue bytes threshold used |\n| **Timestamps (1 field)** | | |\n| `stats_last_refreshed` | String | When stats were last updated |\n\n## Key Differences from Previous Version\n\n**Before:** 9 fields, processor-level aggregation\n- Only captured: flowFilesOut, bytesOut\n- Aggregated connections by source processor\n- Could not identify specific bottleneck connections\n- No queue monitoring capability\n\n**Now:** 24 fields, connection-level granularity\n- Captures ALL 15+ fields from NiFi Status API\n- Stores each connection separately\n- Can identify exact bottleneck points\n- Enables queue monitoring, backpressure detection, flow lineage\n\n**Impact:** ~2x more rows (typical NiFi flow has 1-2 connections per processor), but unlocks powerful new analysis capabilities.\n\n## New Analysis Capabilities\n\n### 1. Backpressure Detection\nIdentify connections with high queue depth:\n```sql\nSELECT source_name, destination_name, MAX(queued_count) as max_queued\nFROM main.default.nifi_processor_snapshots\nWHERE queued_count > 100\nGROUP BY source_name, destination_name\nORDER BY max_queued DESC;\n```\n\n### 2. Queue Limit Monitoring\nFind connections approaching capacity:\n```sql\nSELECT source_name, destination_name, MAX(percent_use_count) as max_percent_full\nFROM main.default.nifi_processor_snapshots\nWHERE percent_use_count > 80\nGROUP BY source_name, destination_name;\n```\n\n### 3. Bidirectional Flow Tracking\nCompare input vs output to find imbalances:\n```sql\nSELECT source_name,\n       SUM(flow_files_in) as total_in,\n       SUM(flow_files_out) as total_out,\n       SUM(flow_files_in) - SUM(flow_files_out) as net_change\nFROM main.default.nifi_processor_snapshots\nGROUP BY source_name\nHAVING ABS(net_change) > 100;\n```\n\n### 4. Queue Growth Trends\nMonitor queue depth over time:\n```sql\nSELECT DATE_TRUNC('hour', snapshot_timestamp) as hour,\n       source_name, destination_name,\n       AVG(queued_count) as avg_queued_flowfiles\nFROM main.default.nifi_processor_snapshots\nWHERE snapshot_timestamp >= current_date() - INTERVAL 1 DAYS\nGROUP BY hour, source_name, destination_name\nORDER BY hour, avg_queued_flowfiles DESC;\n```\n\n### 5. Processor-Level Analysis (Still Possible!)\nAggregate connections to processor-level when needed:\n```sql\nWITH processor_activity AS (\n    SELECT source_name,\n           MAX(flow_files_out) - MIN(flow_files_out) as delta\n    FROM main.default.nifi_processor_snapshots\n    WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n    GROUP BY source_name\n)\nSELECT * FROM processor_activity WHERE delta = 0;\n```\n\n### 6. Flow Path Lineage\nTrack data movement through the flow:\n```sql\nSELECT source_name, destination_name, \n       SUM(flow_files_out) as total_flowfiles\nFROM main.default.nifi_processor_snapshots\nWHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\nGROUP BY source_name, destination_name\nORDER BY total_flowfiles DESC;\n```\n\n## Unity Catalog Configuration\n\nThe notebook uses Unity Catalog with 3-level naming:\n- **Catalog**: `main` (default)\n- **Schema**: `default` (default)\n- **Table**: `nifi_processor_snapshots`\n- **Full path**: `main.default.nifi_processor_snapshots`\n\nYou can customize this in Cell 3 by editing `delta_table_path`.\n\n## CSV Format\n\nYour `flows.csv` should look like:\n```\nid,flow_name\n8c8677c4-29d6-3607-a32e-1234567890ab,Production_Data_Pipeline\nabc-123-def-456-7890-abcdef123456,Development_Testing_Flow\nxyz-789-ghi-012-3456-7890abcdef12,QA_Validation_Flow\n```\n\nUpload it to: `/dbfs/nifi_analysis/flows.csv`\n\n## Server Identifier\n\nThe `server` field helps track data from multiple NiFi instances:\n- Use hostname: `prod-nifi-01`, `dev-nifi-02`\n- Use environment: `prod`, `dev`, `qa`, `staging`\n- Use datacenter: `dc1-nifi`, `dc2-nifi`\n\nThis allows you to:\n- Compare processor usage across environments\n- Track migration from one server to another\n- Aggregate metrics across multiple NiFi clusters\n\n## Key Concepts\n\n**Connection-Level Storage:**\n- Each connection is stored as a separate row\n- Preserves source → destination relationships\n- Enables fine-grained debugging and analysis\n- Can still aggregate to processor-level in queries\n\n**Snapshot-based Analysis:**\n- Each run captures a snapshot of flowfile counts at that moment\n- Take snapshots every 5 minutes over a week\n- Calculate deltas (MAX - MIN) to identify inactive connections\n- Connection with delta = 0 means no flowfiles processed in that time period\n\n**Why Connection-Level Instead of Processor-Level?**\n- Identify which specific connection is bottlenecked\n- Monitor queue depth per connection\n- Track flow paths (source → destination lineage)\n- More debugging capability with minimal storage overhead\n\n**5-Minute Window:**\n- NiFi Status API returns metrics aggregated over the last 5 minutes\n- Running snapshots every 5 minutes captures distinct time windows\n- Historical data retained for 24 hours (configurable in NiFi)\n\n**Queue Metrics:**\n- `queued_count`: Current number of FlowFiles waiting in connection\n- `percent_use_count`: How full the queue is (approaching backpressure threshold)\n- Helps identify bottlenecks before they cause performance issues\n\n## Migration Notes\n\n**IMPORTANT:** Running Cell 7 will DROP the existing table to start fresh with the new 24-field schema. This is necessary because:\n1. Schema changed from 9 fields to 24 fields\n2. Data model changed from processor-level to connection-level\n3. Cannot merge old and new data structures\n\n**Before running:** If you want to preserve old data, create a backup:\n```python\nspark.sql(\"CREATE TABLE main.default.nifi_processor_snapshots_backup AS SELECT * FROM main.default.nifi_processor_snapshots\")\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}