{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NiFi Processor Usage Analyzer - Multi-Flow Edition\n",
    "\n",
    "This notebook analyzes NiFi processor execution counts across **multiple process groups** to identify unused or underutilized processors.\n",
    "\n",
    "**Features:**\n",
    "- Analyzes multiple flows from CSV input\n",
    "- Fast execution count analysis (~5-10 seconds per flow)\n",
    "- Snapshot mode with flow_name tracking\n",
    "- Delta Lake integration with timestamp\n",
    "- Standalone - no external files needed\n",
    "\n",
    "**Setup:**\n",
    "1. Upload CSV with flow definitions (id, flow_name)\n",
    "2. Edit the configuration in Cell 3\n",
    "3. Run all cells\n",
    "4. View results in Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "%pip install requests rich --quiet\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\n",
    "\n",
    "# Databricks-specific imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "# Disable SSL warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('nifi_analyzer')\n",
    "\n",
    "# Initialize Rich console\n",
    "console = Console()\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Configuration\n# EDIT THESE VALUES FOR YOUR NIFI INSTANCE\n\nCONFIG = {\n    # NiFi Connection\n    'nifi_url': 'https://thbnk01hdpnp002.th-bnk01.nxp.com:8443/nifi',\n    'username': 'nxg16670',\n    'password': 'your-password-here',  # ← EDIT THIS\n    'verify_ssl': False,\n    \n    # Flow Definitions CSV\n    # CSV Format: id,flow_name\n    # Example:\n    #   8c8677c4-29d6-36...,Production_Flow_1\n    #   abc-123-def...,Development_Flow_2\n    'flows_csv_path': '/dbfs/nifi_analysis/flows.csv',  # ← Path to your CSV\n    \n    # Snapshot Storage (Unity Catalog - 3-level naming)\n    'enable_snapshots': True,\n    'delta_table_path': 'main.default.nifi_processor_snapshots',  # catalog.schema.table\n}\n\nconsole.print(\"[green]✓ Configuration loaded![/green]\")\nconsole.print(f\"  NiFi URL: {CONFIG['nifi_url']}\")\nconsole.print(f\"  Username: {CONFIG['username']}\")\nconsole.print(f\"  Flows CSV: {CONFIG['flows_csv_path']}\")\nconsole.print(f\"  Delta table: {CONFIG['delta_table_path']}\")\nconsole.print(f\"  Snapshots enabled: {CONFIG['enable_snapshots']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: NiFi Client Class\n\nclass NiFiClient:\n    \"\"\"Client for interacting with Apache NiFi REST API.\"\"\"\n    \n    def __init__(self, base_url: str, username: str, password: str, verify_ssl: bool = True):\n        self.base_url = base_url.rstrip('/')\n        if not self.base_url.endswith('/nifi'):\n            self.base_url += '/nifi'\n        self.api_url = f\"{self.base_url}-api\"\n        self.verify_ssl = verify_ssl\n        self.session = requests.Session()\n        self.token = None\n        self.username = username\n        self.password = password\n        self._authenticate(username, password)\n        \n    def _authenticate(self, username: str, password: str) -> None:\n        \"\"\"Authenticate with NiFi.\"\"\"\n        try:\n            response = requests.post(\n                f\"{self.api_url}/access/token\",\n                data={'username': username, 'password': password},\n                verify=self.verify_ssl\n            )\n            \n            if response.status_code == 201:\n                self.token = response.text\n                self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n                logger.info(\"Successfully authenticated with token\")\n            else:\n                logger.warning(f\"Token auth failed with status {response.status_code}\")\n                logger.warning(\"Falling back to basic auth\")\n                from requests.auth import HTTPBasicAuth\n                self.session.auth = HTTPBasicAuth(username, password)\n        except Exception as e:\n            logger.warning(f\"Token auth error: {e}, falling back to basic auth\")\n            from requests.auth import HTTPBasicAuth\n            self.session.auth = HTTPBasicAuth(username, password)\n    \n    def _request(self, method: str, endpoint: str, **kwargs) -> requests.Response:\n        \"\"\"Make authenticated request with 401 retry.\"\"\"\n        url = f\"{self.api_url}/{endpoint.lstrip('/')}\"\n        kwargs.setdefault('verify', self.verify_ssl)\n        \n        response = self.session.request(method, url, **kwargs)\n        \n        # Handle 401 by re-authenticating once\n        if response.status_code == 401:\n            logger.warning(\"Received 401, attempting re-authentication\")\n            self._authenticate(self.username, self.password)\n            response = self.session.request(method, url, **kwargs)\n            if response.status_code == 401:\n                raise Exception(\"Authentication failed: Unauthorized\")\n        \n        response.raise_for_status()\n        return response\n    \n    def get_process_group(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get process group details including all processors.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}\")\n        return response.json()\n    \n    def list_processors(self, process_group_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Get all processors in a process group (recursive).\"\"\"\n        pg_data = self.get_process_group(process_group_id)\n        processors = pg_data[\"processGroupFlow\"][\"flow\"][\"processors\"]\n        \n        # Recursively get processors from child groups\n        child_groups = pg_data[\"processGroupFlow\"][\"flow\"][\"processGroups\"]\n        for child in child_groups:\n            processors.extend(self.list_processors(child[\"id\"]))\n        \n        return processors\n    \n    def get_process_group_status(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get execution statistics for process group.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}/status\")\n        return response.json()\n    \n    def get_processor_invocation_counts(self, group_id: str) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Extract invocation counts (recursive).\"\"\"\n        status_data = self.get_process_group_status(group_id)\n        processor_stats = {}\n        \n        pg_status = status_data.get(\"processGroupStatus\", {})\n        if not pg_status:\n            return processor_stats\n        \n        # Extract from current group\n        for proc_status in pg_status.get(\"processorStatus\", []):\n            try:\n                proc_id = proc_status[\"id\"]\n                processor_stats[proc_id] = {\n                    \"name\": proc_status[\"name\"],\n                    \"type\": proc_status[\"type\"].split('.')[-1],\n                    \"invocations\": proc_status.get(\"aggregateSnapshot\", {}).get(\"invocations\", 0)\n                }\n            except KeyError as e:\n                logger.error(f\"Missing key in processor status: {e}\")\n        \n        # Recurse into child groups\n        for child_pg_status in pg_status.get(\"processGroupStatus\", []):\n            try:\n                child_id = child_pg_status[\"id\"]\n                child_stats = self.get_processor_invocation_counts(child_id)\n                processor_stats.update(child_stats)\n            except Exception as e:\n                logger.error(f\"Error processing child group: {e}\")\n        \n        return processor_stats\n    \n    def close(self):\n        \"\"\"Close session.\"\"\"\n        self.session.close()\n\nconsole.print(\"[green]✓ NiFiClient class defined![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Multi-Flow Analyzer Class\n",
    "\n",
    "class MultiFlowAnalyzer:\n",
    "    \"\"\"Analyzes multiple NiFi flows and stores results in Delta Lake.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: NiFiClient):\n",
    "        self.client = client\n",
    "        self.console = Console()\n",
    "        self.all_results = []  # Store all flow results\n",
    "        self.snapshot_timestamp = datetime.now()\n",
    "    \n",
    "    def analyze_flow(self, flow_id: str, flow_name: str) -> Dict:\n",
    "        \"\"\"Analyze a single flow.\"\"\"\n",
    "        flow_results = {\n",
    "            'flow_name': flow_name,\n",
    "            'flow_id': flow_id,\n",
    "            'processor_count': 0,\n",
    "            'processors': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Get processors\n",
    "            processors = self.client.list_processors(flow_id)\n",
    "            flow_results['processor_count'] = len(processors)\n",
    "            \n",
    "            # Get execution counts\n",
    "            exec_stats = self.client.get_processor_invocation_counts(flow_id)\n",
    "            \n",
    "            # Build processor data\n",
    "            for proc in processors:\n",
    "                proc_id = proc['id']\n",
    "                proc_name = proc['component']['name']\n",
    "                proc_type = proc['component']['type'].split('.')[-1]\n",
    "                invocations = exec_stats.get(proc_id, {}).get('invocations', 0)\n",
    "                \n",
    "                flow_results['processors'].append({\n",
    "                    'snapshot_timestamp': self.snapshot_timestamp,\n",
    "                    'flow_name': flow_name,\n",
    "                    'process_group_id': flow_id,\n",
    "                    'processor_id': proc_id,\n",
    "                    'processor_name': proc_name,\n",
    "                    'processor_type': proc_type,\n",
    "                    'invocations': invocations\n",
    "                })\n",
    "            \n",
    "            return flow_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[red]ERROR[/red] Failed to analyze {flow_name}: {e}\")\n",
    "            flow_results['error'] = str(e)\n",
    "            return flow_results\n",
    "    \n",
    "    def analyze_all_flows(self, flows_csv_path: str):\n",
    "        \"\"\"Analyze all flows from CSV.\"\"\"\n",
    "        self.console.print(f\"\\n[cyan]Multi-Flow Analysis Starting...[/cyan]\")\n",
    "        self.console.print(f\"  Timestamp: {self.snapshot_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Read flows CSV\n",
    "        try:\n",
    "            flows_df = spark.read.csv(flows_csv_path, header=True)\n",
    "            flows = flows_df.collect()\n",
    "            \n",
    "            self.console.print(f\"[green]Found {len(flows)} flows to analyze[/green]\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[red]ERROR[/red] Failed to read CSV: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Analyze each flow\n",
    "        with Progress(\n",
    "            SpinnerColumn(),\n",
    "            TextColumn(\"[progress.description]{task.description}\"),\n",
    "            BarColumn(),\n",
    "            console=self.console\n",
    "        ) as progress:\n",
    "            task = progress.add_task(\"Analyzing flows...\", total=len(flows))\n",
    "            \n",
    "            for flow in flows:\n",
    "                flow_id = flow['id']\n",
    "                flow_name = flow['flow_name']\n",
    "                \n",
    "                progress.update(task, description=f\"Analyzing: {flow_name}\")\n",
    "                \n",
    "                flow_results = self.analyze_flow(flow_id, flow_name)\n",
    "                self.all_results.append(flow_results)\n",
    "                \n",
    "                # Display flow summary\n",
    "                if 'error' not in flow_results:\n",
    "                    self.console.print(\n",
    "                        f\"  [green]✓[/green] {flow_name}: {flow_results['processor_count']} processors\"\n",
    "                    )\n",
    "                else:\n",
    "                    self.console.print(\n",
    "                        f\"  [red]✗[/red] {flow_name}: {flow_results['error']}\"\n",
    "                    )\n",
    "                \n",
    "                progress.advance(task)\n",
    "        \n",
    "        # Display overall summary\n",
    "        self.display_summary()\n",
    "    \n",
    "    def display_summary(self):\n",
    "        \"\"\"Display analysis summary.\"\"\"\n",
    "        total_processors = sum(r['processor_count'] for r in self.all_results if 'error' not in r)\n",
    "        successful_flows = sum(1 for r in self.all_results if 'error' not in r)\n",
    "        failed_flows = sum(1 for r in self.all_results if 'error' in r)\n",
    "        \n",
    "        self.console.print(f\"\\n[cyan]Overall Summary:[/cyan]\")\n",
    "        self.console.print(f\"  Total flows: {len(self.all_results)}\")\n",
    "        self.console.print(f\"  Successful: {successful_flows}\")\n",
    "        self.console.print(f\"  Failed: {failed_flows}\")\n",
    "        self.console.print(f\"  Total processors: {total_processors}\")\n",
    "        \n",
    "        # Create summary table\n",
    "        table = Table(title=\"\\nFlow Analysis Summary\")\n",
    "        table.add_column(\"Flow Name\", style=\"cyan\")\n",
    "        table.add_column(\"Processors\", justify=\"right\", style=\"yellow\")\n",
    "        table.add_column(\"Status\", style=\"green\")\n",
    "        \n",
    "        for result in self.all_results:\n",
    "            status = \"[red]Error[/red]\" if 'error' in result else \"[green]Success[/green]\"\n",
    "            table.add_row(\n",
    "                result['flow_name'],\n",
    "                str(result['processor_count']),\n",
    "                status\n",
    "            )\n",
    "        \n",
    "        self.console.print(table)\n",
    "    \n",
    "    def get_results_dataframe(self):\n",
    "        \"\"\"Convert all results to Spark DataFrame.\"\"\"\n",
    "        all_rows = []\n",
    "        \n",
    "        for flow_result in self.all_results:\n",
    "            if 'error' not in flow_result:\n",
    "                all_rows.extend(flow_result['processors'])\n",
    "        \n",
    "        if not all_rows:\n",
    "            return None\n",
    "        \n",
    "        # Convert to list of tuples\n",
    "        rows = [\n",
    "            (\n",
    "                row['snapshot_timestamp'],\n",
    "                row['flow_name'],\n",
    "                row['process_group_id'],\n",
    "                row['processor_id'],\n",
    "                row['processor_name'],\n",
    "                row['processor_type'],\n",
    "                row['invocations']\n",
    "            )\n",
    "            for row in all_rows\n",
    "        ]\n",
    "        \n",
    "        # Define schema with flow_name\n",
    "        schema = StructType([\n",
    "            StructField(\"snapshot_timestamp\", TimestampType(), False),\n",
    "            StructField(\"flow_name\", StringType(), False),\n",
    "            StructField(\"process_group_id\", StringType(), False),\n",
    "            StructField(\"processor_id\", StringType(), False),\n",
    "            StructField(\"processor_name\", StringType(), False),\n",
    "            StructField(\"processor_type\", StringType(), False),\n",
    "            StructField(\"invocations\", LongType(), False)\n",
    "        ])\n",
    "        \n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        return spark.createDataFrame(rows, schema)\n",
    "\n",
    "console.print(\"[green]✓ MultiFlowAnalyzer class defined![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run Multi-Flow Analysis\n",
    "\n",
    "console.print(\"\\n[cyan]Starting Multi-Flow NiFi Analysis...[/cyan]\\n\")\n",
    "\n",
    "# Connect to NiFi\n",
    "console.print(\"[yellow]Connecting to NiFi...[/yellow]\")\n",
    "client = NiFiClient(\n",
    "    base_url=CONFIG['nifi_url'],\n",
    "    username=CONFIG['username'],\n",
    "    password=CONFIG['password'],\n",
    "    verify_ssl=CONFIG['verify_ssl']\n",
    ")\n",
    "console.print(\"[green]OK[/green] Connected successfully\\n\")\n",
    "\n",
    "# Create analyzer and run analysis\n",
    "analyzer = MultiFlowAnalyzer(client)\n",
    "analyzer.analyze_all_flows(CONFIG['flows_csv_path'])\n",
    "\n",
    "# Cleanup\n",
    "client.close()\n",
    "\n",
    "console.print(\"\\n[green]✓ Multi-flow analysis complete![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Save Snapshots to Delta Lake\n\nif CONFIG['enable_snapshots']:\n    console.print(\"\\n[yellow]Saving snapshots to Delta Lake...[/yellow]\")\n    \n    df = analyzer.get_results_dataframe()\n    \n    if df is not None:\n        table_name = CONFIG['delta_table_path']\n        \n        # Check if table exists\n        table_exists = spark.catalog._jcatalog.tableExists(table_name)\n        \n        if not table_exists:\n            console.print(f\"[yellow]Table doesn't exist, creating: {table_name}[/yellow]\")\n            # Create table with explicit schema\n            df.write \\\n                .format(\"delta\") \\\n                .mode(\"overwrite\") \\\n                .option(\"overwriteSchema\", \"true\") \\\n                .saveAsTable(table_name)\n            console.print(f\"[green]OK[/green] Table created successfully\")\n        else:\n            console.print(f\"[yellow]Table exists, appending data to: {table_name}[/yellow]\")\n            # Append to existing table\n            df.write \\\n                .format(\"delta\") \\\n                .mode(\"append\") \\\n                .option(\"mergeSchema\", \"true\") \\\n                .saveAsTable(table_name)\n            console.print(f\"[green]OK[/green] Data appended successfully\")\n        \n        console.print(f\"  Timestamp: {analyzer.snapshot_timestamp}\")\n        console.print(f\"  Total rows written: {df.count()}\")\n        \n        # Show sample\n        console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n        display(df.limit(10))\n    else:\n        console.print(\"[red]ERROR[/red] No data to save\")\nelse:\n    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Query Historical Snapshots by Flow\n\nif CONFIG['enable_snapshots']:\n    console.print(\"\\n[cyan]Querying historical snapshots by flow...[/cyan]\\n\")\n    \n    table_name = CONFIG['delta_table_path']\n    \n    try:\n        # Show snapshots per flow\n        console.print(\"[yellow]Snapshot count by flow:[/yellow]\")\n        spark.sql(f\"\"\"\n            SELECT \n                flow_name,\n                COUNT(DISTINCT snapshot_timestamp) as snapshots,\n                COUNT(*) as total_processors,\n                MAX(snapshot_timestamp) as last_snapshot\n            FROM {table_name}\n            GROUP BY flow_name\n            ORDER BY flow_name\n        \"\"\").show(truncate=False)\n        \n        # Find inactive processors by flow (last 7 days)\n        console.print(\"\\n[yellow]Inactive processors by flow (last 7 days):[/yellow]\")\n        spark.sql(f\"\"\"\n            WITH recent_activity AS (\n                SELECT \n                    flow_name,\n                    processor_name,\n                    processor_type,\n                    MAX(invocations) - MIN(invocations) as delta_invocations,\n                    MIN(snapshot_timestamp) as first_snapshot,\n                    MAX(snapshot_timestamp) as last_snapshot\n                FROM {table_name}\n                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n                GROUP BY flow_name, processor_name, processor_type\n            )\n            SELECT \n                flow_name,\n                COUNT(*) as inactive_processor_count\n            FROM recent_activity\n            WHERE delta_invocations = 0\n            GROUP BY flow_name\n            ORDER BY inactive_processor_count DESC\n        \"\"\").show(truncate=False)\n        \n        # Detailed view of inactive processors\n        console.print(\"\\n[yellow]Detailed inactive processor list:[/yellow]\")\n        spark.sql(f\"\"\"\n            WITH recent_activity AS (\n                SELECT \n                    flow_name,\n                    processor_name,\n                    processor_type,\n                    MAX(invocations) - MIN(invocations) as delta_invocations\n                FROM {table_name}\n                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n                GROUP BY flow_name, processor_name, processor_type\n            )\n            SELECT \n                flow_name,\n                processor_name,\n                processor_type,\n                delta_invocations\n            FROM recent_activity\n            WHERE delta_invocations = 0\n            ORDER BY flow_name, processor_name\n            LIMIT 50\n        \"\"\").show(truncate=False)\n        \n    except Exception as e:\n        console.print(f\"[red]ERROR[/red] Failed to query: {e}\")\nelse:\n    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Export Results to CSV by Flow\n",
    "\n",
    "console.print(\"\\n[yellow]Exporting results to CSV...[/yellow]\")\n",
    "\n",
    "timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "df = analyzer.get_results_dataframe()\n",
    "if df is not None:\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    # Export overall summary\n",
    "    output_path = f\"/dbfs/nifi_analysis/all_flows_{timestamp_str}.csv\"\n",
    "    pdf.to_csv(output_path, index=False)\n",
    "    console.print(f\"[green]OK[/green] All flows exported to {output_path}\")\n",
    "    \n",
    "    # Export per flow\n",
    "    for flow_name in pdf['flow_name'].unique():\n",
    "        flow_df = pdf[pdf['flow_name'] == flow_name]\n",
    "        flow_path = f\"/dbfs/nifi_analysis/{flow_name}_{timestamp_str}.csv\"\n",
    "        flow_df.to_csv(flow_path, index=False)\n",
    "        console.print(f\"  [green]✓[/green] {flow_name}: {len(flow_df)} processors\")\n",
    "    \n",
    "    console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n",
    "    display(pdf.head(10))\n",
    "else:\n",
    "    console.print(\"[red]ERROR[/red] No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n\n## Updated Delta Table Schema\n\nThe Delta table now includes:\n\n| Column | Type | Description |\n|--------|------|-------------|\n| `snapshot_timestamp` | Timestamp | **When the snapshot was captured** |\n| `flow_name` | String | **Flow name from CSV** |\n| `process_group_id` | String | NiFi process group ID |\n| `processor_id` | String | Processor unique ID |\n| `processor_name` | String | Processor name |\n| `processor_type` | String | Processor type |\n| `invocations` | Long | Execution count |\n\n## Unity Catalog Configuration\n\nThe notebook uses Unity Catalog with 3-level naming:\n- **Catalog**: `main` (default)\n- **Schema**: `default` (default)\n- **Table**: `nifi_processor_snapshots`\n- **Full path**: `main.default.nifi_processor_snapshots`\n\nYou can customize this in Cell 3 by editing `delta_table_path`.\n\n## Example Queries\n\n```sql\n-- Find all inactive processors across all flows (last 7 days)\nWITH activity AS (\n    SELECT \n        flow_name,\n        processor_name,\n        MAX(invocations) - MIN(invocations) as delta\n    FROM main.default.nifi_processor_snapshots\n    WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n    GROUP BY flow_name, processor_name\n)\nSELECT * FROM activity WHERE delta = 0;\n\n-- Compare flows to see which has most inactive processors\nSELECT \n    flow_name,\n    COUNT(*) as total_processors,\n    SUM(CASE WHEN invocations = 0 THEN 1 ELSE 0 END) as unused_processors\nFROM (\n    SELECT flow_name, processor_name, MAX(invocations) as invocations\n    FROM main.default.nifi_processor_snapshots\n    WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n    GROUP BY flow_name, processor_name\n)\nGROUP BY flow_name\nORDER BY unused_processors DESC;\n```\n\n## CSV Format\n\nYour `flows.csv` should look like:\n```\nid,flow_name\n8c8677c4-29d6-3607-a32e-1234567890ab,Production_Data_Pipeline\nabc-123-def-456-7890-abcdef123456,Development_Testing_Flow\nxyz-789-ghi-012-3456-7890abcdef12,QA_Validation_Flow\n```\n\nUpload it to: `/dbfs/nifi_analysis/flows.csv`"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}