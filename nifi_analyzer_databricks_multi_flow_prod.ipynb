{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6791bf8-1576-4719-aaac-8ffd4cbd4d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# NiFi Processor Usage Analyzer - Multi-Flow Edition\n",
    "\n",
    "This notebook analyzes NiFi processor execution counts across **multiple process groups** to identify unused or underutilized processors.\n",
    "\n",
    "**Features:**\n",
    "- Analyzes multiple flows from CSV input\n",
    "- Fast execution count analysis (~5-10 seconds per flow)\n",
    "- Snapshot mode with flow_name tracking\n",
    "- Delta Lake integration with timestamp\n",
    "- Standalone - no external files needed\n",
    "\n",
    "**Setup:**\n",
    "1. Upload CSV with flow definitions (id, flow_name)\n",
    "2. Edit the configuration in Cell 3\n",
    "3. Run all cells\n",
    "4. View results in Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adef03ec-294c-45f1-b52b-9b4bc98c7479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:36,530 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:37,530 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "%pip install requests rich --quiet\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea522c1a-91a6-46cf-b8c5-9958a4d56be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Libraries imported successfully!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:38,549 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Libraries\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\n",
    "\n",
    "# Databricks-specific imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "# Disable SSL warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('nifi_analyzer')\n",
    "\n",
    "# Initialize Rich console\n",
    "console = Console()\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a83ba3-6c06-4fcc-9509-d19304cd414e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Configuration loaded!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Configuration loaded!\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  NiFi URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://us-chd01-prod-nifi.us-chd01.nxp.com:8443/nifi/</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  NiFi URL: \u001b[4;94mhttps://us-chd01-prod-nifi.us-chd01.nxp.com:8443/nifi/\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Username: nxg16670\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Username: nxg16670\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Server: prod\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Server: prod\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Flows CSV: <span style=\"color: #800080; text-decoration-color: #800080\">/Volumes/1dp_mfg_sbx/validation_test_eric/files/nifi_flow_status/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">nifi_group_ids_prod.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Flows CSV: \u001b[35m/Volumes/1dp_mfg_sbx/validation_test_eric/files/nifi_flow_status/\u001b[0m\u001b[95mnifi_group_ids_prod.csv\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Delta table: 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Delta table: 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Snapshots enabled: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Snapshots enabled: \u001b[3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Cell 3: Configuration\n",
    "# EDIT THESE VALUES FOR YOUR NIFI INSTANCE\n",
    "\n",
    "CONFIG = {\n",
    "    # NiFi Connection\n",
    "    'nifi_url': 'https://us-chd01-prod-nifi.us-chd01.nxp.com:8443/nifi/',\n",
    "    'username': 'nxg16670',\n",
    "    'password': '6be!x!_Ex855cXJ',  # ← EDIT THIS\n",
    "    'verify_ssl': False,\n",
    "    \n",
    "    # Server Identifier (for tracking multiple NiFi servers)\n",
    "    'server': 'prod',  # ← EDIT THIS (e.g., 'prod', 'dev', hostname)\n",
    "    \n",
    "    # Flow Definitions CSV\n",
    "    # CSV Format: id,flow_name\n",
    "    # Example:\n",
    "    #   8c8677c4-29d6-36...,Production_Flow_1\n",
    "    #   abc-123-def...,Development_Flow_2\n",
    "    'flows_csv_path': '/Volumes/1dp_mfg_sbx/validation_test_eric/files/nifi_flow_status/nifi_group_ids_prod.csv',  # ← Path to your CSV\n",
    "    \n",
    "    # Snapshot Storage (Unity Catalog - 3-level naming)\n",
    "    'enable_snapshots': True,\n",
    "    'delta_table_path': '1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes',  # catalog.schema.table\n",
    "}\n",
    "\n",
    "console.print(\"[green]✓ Configuration loaded![/green]\")\n",
    "console.print(f\"  NiFi URL: {CONFIG['nifi_url']}\")\n",
    "console.print(f\"  Username: {CONFIG['username']}\")\n",
    "console.print(f\"  Server: {CONFIG['server']}\")\n",
    "console.print(f\"  Flows CSV: {CONFIG['flows_csv_path']}\")\n",
    "console.print(f\"  Delta table: {CONFIG['delta_table_path']}\")\n",
    "console.print(f\"  Snapshots enabled: {CONFIG['enable_snapshots']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7358fe55-8364-4397-b48d-b6f9ef4327bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Cell 4: NiFi Client Class - RECURSIVE PROCESSOR EXTRACTION (CORRECTED)\n\nclass NiFiClient:\n    \"\"\"Client for interacting with Apache NiFi REST API.\"\"\"\n    \n    def __init__(self, base_url: str, username: str, password: str, verify_ssl: bool = True):\n        self.base_url = base_url.rstrip('/')\n        if not self.base_url.endswith('/nifi'):\n            self.base_url += '/nifi'\n        self.api_url = f\"{self.base_url}-api\"\n        self.verify_ssl = verify_ssl\n        self.session = requests.Session()\n        self.token = None\n        self.username = username\n        self.password = password\n        self._authenticate(username, password)\n        \n    def _authenticate(self, username: str, password: str) -> None:\n        \"\"\"Authenticate with NiFi.\"\"\"\n        try:\n            response = requests.post(\n                f\"{self.api_url}/access/token\",\n                data={'username': username, 'password': password},\n                verify=self.verify_ssl\n            )\n            \n            if response.status_code == 201:\n                self.token = response.text\n                self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n                logger.info(\"Successfully authenticated with token\")\n            else:\n                logger.warning(f\"Token auth failed with status {response.status_code}\")\n                logger.warning(\"Falling back to basic auth\")\n                from requests.auth import HTTPBasicAuth\n                self.session.auth = HTTPBasicAuth(username, password)\n        except Exception as e:\n            logger.warning(f\"Token auth error: {e}, falling back to basic auth\")\n            from requests.auth import HTTPBasicAuth\n            self.session.auth = HTTPBasicAuth(username, password)\n    \n    def _request(self, method: str, endpoint: str, **kwargs) -> requests.Response:\n        \"\"\"Make authenticated request with 401 retry.\"\"\"\n        url = f\"{self.api_url}/{endpoint.lstrip('/')}\"\n        kwargs.setdefault('verify', self.verify_ssl)\n        \n        response = self.session.request(method, url, **kwargs)\n        \n        # Handle 401 by re-authenticating once\n        if response.status_code == 401:\n            logger.warning(\"Received 401, attempting re-authentication\")\n            self._authenticate(self.username, self.password)\n            response = self.session.request(method, url, **kwargs)\n            if response.status_code == 401:\n                raise Exception(\"Authentication failed: Unauthorized\")\n        \n        response.raise_for_status()\n        return response\n    \n    def get_process_group(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get process group details including all processors.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}\")\n        return response.json()\n    \n    def get_process_group_status(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get execution statistics for process group.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}/status?recursive=true\")\n        return response.json()\n    \n    def get_processor_statistics(self, group_id: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get processor-level statistics with IDs using Status API.\n\n        Uses Status API with recursive=true which returns nested processGroupStatusSnapshots.\n        We recursively walk through all levels to extract ALL processors from deeply nested groups.\n\n        Returns:\n            List of processor dictionaries with IDs and metrics\n        \"\"\"\n        # Get Status API with recursive=true ONCE\n        status_data = self.get_process_group_status(group_id)\n\n        # Recursively extract processors from all nested levels\n        def extract_processors_recursive(pg_snapshot_data, depth=0):\n            \"\"\"Recursively extract processors from nested process group status snapshot.\"\"\"\n            processors = []\n\n            # Extract processors at THIS level from processorStatusSnapshots array\n            processor_snapshots = pg_snapshot_data.get('processorStatusSnapshots', [])\n            \n            logger.info(f\"{'  ' * depth}Level {depth}: Found {len(processor_snapshots)} processors\")\n\n            for snapshot in processor_snapshots:\n                # Each snapshot wraps a processorStatusSnapshot with the actual data\n                proc_snap = snapshot.get('processorStatusSnapshot', {})\n\n                processors.append({\n                    'processor_id': proc_snap.get('id'),\n                    'processor_name': proc_snap.get('name'),\n                    'processor_type': proc_snap.get('type', '').split('.')[-1],\n                    'flow_files_in': int(proc_snap.get('flowFilesIn', 0)),\n                    'bytes_in': int(proc_snap.get('bytesIn', 0)),\n                    'flow_files_out': int(proc_snap.get('flowFilesOut', 0)),\n                    'bytes_out': int(proc_snap.get('bytesOut', 0)),\n                    'tasks': int(proc_snap.get('taskCount', 0)),\n                    'run_status': proc_snap.get('runStatus', 'Unknown')\n                })\n\n            # Recursively process nested child process groups\n            # Note: processGroupStatusSnapshots (plural) contains wrapped processGroupStatusSnapshot (singular) objects\n            child_group_snapshots = pg_snapshot_data.get('processGroupStatusSnapshots', [])\n            if child_group_snapshots:\n                logger.info(f\"{'  ' * depth}Level {depth}: Found {len(child_group_snapshots)} child groups, recursing...\")\n\n            for child_group_snapshot in child_group_snapshots:\n                # Unwrap the processGroupStatusSnapshot object\n                child_pg_snapshot = child_group_snapshot.get('processGroupStatusSnapshot', {})\n                child_processors = extract_processors_recursive(child_pg_snapshot, depth + 1)\n                processors.extend(child_processors)\n\n            return processors\n\n        # Start recursion from root - need to navigate to aggregateSnapshot first\n        pg_status = status_data.get('processGroupStatus', {})\n        aggregate_snapshot = pg_status.get('aggregateSnapshot', {})\n        all_processors = extract_processors_recursive(aggregate_snapshot)\n\n        logger.info(f\"Total processors extracted from all levels: {len(all_processors)}\")\n        return all_processors\n    \n    def close(self):\n        \"\"\"Close session.\"\"\"\n        self.session.close()\n\nconsole.print(\"[green]✓ NiFiClient class defined (recursive processor extraction CORRECTED)![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f3c653-5945-470c-a95c-0dbfb7b60e29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Cell 5: Multi-Flow Analyzer Class - SIMPLIFIED PROCESSOR-LEVEL\n\nclass MultiFlowAnalyzer:\n    \"\"\"Analyzes multiple NiFi flows and stores PROCESSOR-LEVEL results in Delta Lake.\"\"\"\n    \n    def __init__(self, client: NiFiClient, server: str = 'unknown'):\n        self.client = client\n        self.console = Console()\n        self.server = server\n        self.all_results = []\n        self.snapshot_timestamp = datetime.now()\n    \n    def analyze_flow(self, flow_id: str, flow_name: str) -> Dict:\n        \"\"\"Analyze a single flow - processor level.\"\"\"\n        flow_results = {\n            'flow_name': flow_name,\n            'flow_id': flow_id,\n            'processor_count': 0,\n            'processors': []\n        }\n        \n        try:\n            # Get processor-level statistics (SIMPLIFIED!)\n            processors = self.client.get_processor_statistics(flow_id)\n            flow_results['processor_count'] = len(processors)\n            \n            # Add metadata to each processor\n            for proc in processors:\n                flow_results['processors'].append({\n                    # Metadata (4 fields)\n                    'snapshot_timestamp': self.snapshot_timestamp,\n                    'server': self.server,\n                    'flow_name': flow_name,\n                    'process_group_id': flow_id,\n                    \n                    # Processor identity (3 fields)\n                    'processor_id': proc.get('processor_id'),\n                    'processor_name': proc.get('processor_name'),\n                    'processor_type': proc.get('processor_type'),\n                    \n                    # Activity metrics (6 fields)\n                    'flow_files_in': proc.get('flow_files_in', 0),\n                    'bytes_in': proc.get('bytes_in', 0),\n                    'flow_files_out': proc.get('flow_files_out', 0),\n                    'bytes_out': proc.get('bytes_out', 0),\n                    'tasks': proc.get('tasks', 0),\n                    'run_status': proc.get('run_status', 'Unknown')\n                })\n            \n            return flow_results\n            \n        except Exception as e:\n            self.console.print(f\"[red]ERROR[/red] Failed to analyze {flow_name}: {e}\")\n            flow_results['error'] = str(e)\n            return flow_results\n    \n    def analyze_all_flows(self, flows_csv_path: str):\n        \"\"\"Analyze all flows from CSV.\"\"\"\n        self.console.print(f\"\\n[cyan]Multi-Flow Analysis Starting (PROCESSOR-LEVEL)...[/cyan]\")\n        self.console.print(f\"  Server: {self.server}\")\n        self.console.print(f\"  Timestamp: {self.snapshot_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        \n        # Read flows CSV\n        try:\n            flows_df = spark.read.csv(flows_csv_path, header=True)\n            flows = flows_df.collect()\n            \n            self.console.print(f\"[green]Found {len(flows)} flows to analyze[/green]\\n\")\n            \n        except Exception as e:\n            self.console.print(f\"[red]ERROR[/red] Failed to read CSV: {e}\")\n            raise\n        \n        # Analyze each flow\n        with Progress(\n            SpinnerColumn(),\n            TextColumn(\"[progress.description]{task.description}\"),\n            BarColumn(),\n            console=self.console\n        ) as progress:\n            task = progress.add_task(\"Analyzing flows...\", total=len(flows))\n            \n            for flow in flows:\n                flow_id = flow['id']\n                flow_name = flow['flow_name']\n                \n                progress.update(task, description=f\"Analyzing: {flow_name}\")\n                \n                flow_results = self.analyze_flow(flow_id, flow_name)\n                self.all_results.append(flow_results)\n                \n                # Display flow summary\n                if 'error' not in flow_results:\n                    self.console.print(\n                        f\"  [green]✓[/green] {flow_name}: {flow_results['processor_count']} processors\"\n                    )\n                else:\n                    self.console.print(\n                        f\"  [red]✗[/red] {flow_name}: {flow_results['error']}\"\n                    )\n                \n                progress.advance(task)\n        \n        # Display overall summary\n        self.display_summary()\n    \n    def display_summary(self):\n        \"\"\"Display analysis summary.\"\"\"\n        total_processors = sum(r['processor_count'] for r in self.all_results if 'error' not in r)\n        successful_flows = sum(1 for r in self.all_results if 'error' not in r)\n        failed_flows = sum(1 for r in self.all_results if 'error' in r)\n        \n        self.console.print(f\"\\n[cyan]Overall Summary:[/cyan]\")\n        self.console.print(f\"  Server: {self.server}\")\n        self.console.print(f\"  Total flows: {len(self.all_results)}\")\n        self.console.print(f\"  Successful: {successful_flows}\")\n        self.console.print(f\"  Failed: {failed_flows}\")\n        self.console.print(f\"  Total processors: {total_processors}\")\n        \n        # Create summary table\n        table = Table(title=\"\\nFlow Analysis Summary\")\n        table.add_column(\"Flow Name\", style=\"cyan\")\n        table.add_column(\"Processors\", justify=\"right\", style=\"yellow\")\n        table.add_column(\"Status\", style=\"green\")\n        \n        for result in self.all_results:\n            status = \"[red]Error[/red]\" if 'error' in result else \"[green]Success[/green]\"\n            table.add_row(\n                result['flow_name'],\n                str(result['processor_count']),\n                status\n            )\n        \n        self.console.print(table)\n    \n    def get_results_dataframe(self):\n        \"\"\"Convert all results to Spark DataFrame with SIMPLIFIED 11-field processor schema.\"\"\"\n        all_rows = []\n        \n        for flow_result in self.all_results:\n            if 'error' not in flow_result:\n                all_rows.extend(flow_result['processors'])\n        \n        if not all_rows:\n            return None\n        \n        # Helper function to safely convert to int\n        def safe_int(value, default=0):\n            \"\"\"Convert value to int, handling strings and None.\"\"\"\n            if value is None:\n                return default\n            try:\n                return int(value)\n            except (ValueError, TypeError):\n                return default\n        \n        # Convert to list of tuples (13 fields)\n        rows = [\n            (\n                row['snapshot_timestamp'],\n                row['server'],\n                row['flow_name'],\n                row['process_group_id'],\n                row['processor_id'],\n                row['processor_name'],\n                row['processor_type'],\n                row.get('flow_files_in', 0),\n                row.get('bytes_in', 0),\n                row.get('flow_files_out', 0),\n                row.get('bytes_out', 0),\n                row.get('tasks', 0),\n                row.get('run_status', 'Unknown')\n            )\n            for row in all_rows\n        ]\n        \n        # Define SIMPLIFIED schema (13 fields - processor level with IDs)\n        schema = StructType([\n            # Metadata (4 fields)\n            StructField(\"snapshot_timestamp\", TimestampType(), False),\n            StructField(\"server\", StringType(), False),\n            StructField(\"flow_name\", StringType(), False),\n            StructField(\"process_group_id\", StringType(), False),\n            \n            # Processor identity (3 fields)\n            StructField(\"processor_id\", StringType(), True),\n            StructField(\"processor_name\", StringType(), False),\n            StructField(\"processor_type\", StringType(), True),\n            \n            # Activity metrics (6 fields)\n            StructField(\"flow_files_in\", LongType(), False),\n            StructField(\"bytes_in\", LongType(), False),\n            StructField(\"flow_files_out\", LongType(), False),\n            StructField(\"bytes_out\", LongType(), False),\n            StructField(\"tasks\", LongType(), False),\n            StructField(\"run_status\", StringType(), False)\n        ])\n        \n        spark = SparkSession.builder.getOrCreate()\n        return spark.createDataFrame(rows, schema)\n\nconsole.print(\"[green]✓ MultiFlowAnalyzer class defined (simplified processor-level)![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b0410c-171e-47c4-af2b-23e37969c8bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Starting Multi-Flow NiFi Analysis...</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mStarting Multi-Flow NiFi Analysis\u001b[0m\u001b[36m...\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Connecting to NiFi...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mConnecting to NiFi\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:39,532 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:40,166 - nifi_analyzer - INFO - Successfully authenticated with token\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">OK</span> Connected successfully\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mOK\u001b[0m Connected successfully\n",
       "\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Multi-Flow Analysis Starting...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mMulti-Flow Analysis Starting\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Server: prod\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Server: prod\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Timestamp: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:40</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Timestamp: \u001b[1;36m2026\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m09\u001b[0m \u001b[1;92m18:59:40\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:40,530 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:40,854 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Found </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">14</span><span style=\"color: #008000; text-decoration-color: #008000\"> flows to analyze</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mFound \u001b[0m\u001b[1;32m14\u001b[0m\u001b[32m flows to analyze\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6ed65758ce4e52b5413d3b3b1fed5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> STDF Unit Probe Ingest: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m STDF Unit Probe Ingest: \u001b[1;36m3\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> File Ingest: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m File Ingest: \u001b[1;36m4\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Final Test TX SAF - Spark3: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Final Test TX SAF - Spark3: \u001b[1;36m1\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Final Test Bin SAF and STDF - Spark3: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Final Test Bin SAF and STDF - Spark3: \u001b[1;36m2\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> FDC Data Edge To Hadoop/Kafka: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m FDC Data Edge To Hadoop/Kafka: \u001b[1;36m10\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:41,529 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> STDF Burn-in: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m STDF Burn-in: \u001b[1;36m18\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Final Test TX STDF - Spark3: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Final Test TX STDF - Spark3: \u001b[1;36m46\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> File Availability Metrics: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m File Availability Metrics: \u001b[1;36m2\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> ICN8 Track-out time based loading: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m ICN8 Track-out time based loading: \u001b[1;36m46\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> ICN8 BRS Feedback: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m ICN8 BRS Feedback: \u001b[1;36m40\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Saf Unit Probe: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Saf Unit Probe: \u001b[1;36m3\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> FDC Prod Data Processing: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m FDC Prod Data Processing: \u001b[1;36m7\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Master Tables: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Master Tables: \u001b[1;36m4\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Database Ingest: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Database Ingest: \u001b[1;36m2\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Overall Summary:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mOverall Summary:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Server: prod\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Server: prod\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Total flows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Total flows: \u001b[1;36m14\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Successful: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Successful: \u001b[1;36m14\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Failed: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Failed: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Total connections: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">188</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Total connections: \u001b[1;36m188\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                                </span>\n",
       "<span style=\"font-style: italic\">                     Flow Analysis Summary                      </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Flow Name                            </span>┃<span style=\"font-weight: bold\"> Connections </span>┃<span style=\"font-weight: bold\"> Status  </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> STDF Unit Probe Ingest               </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           3 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> File Ingest                          </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           4 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Final Test TX SAF - Spark3           </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           1 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Final Test Bin SAF and STDF - Spark3 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           2 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> FDC Data Edge To Hadoop/Kafka        </span>│<span style=\"color: #808000; text-decoration-color: #808000\">          10 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> STDF Burn-in                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">          18 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Final Test TX STDF - Spark3          </span>│<span style=\"color: #808000; text-decoration-color: #808000\">          46 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> File Availability Metrics            </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           2 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ICN8 Track-out time based loading    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">          46 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ICN8 BRS Feedback                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">          40 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Saf Unit Probe                       </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           3 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> FDC Prod Data Processing             </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           7 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Master Tables                        </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           4 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Database Ingest                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           2 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "└──────────────────────────────────────┴─────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                                \u001b[0m\n",
       "\u001b[3m                     Flow Analysis Summary                      \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mFlow Name                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnections\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStatus \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mSTDF Unit Probe Ingest              \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          3\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFile Ingest                         \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          4\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFinal Test TX SAF - Spark3          \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          1\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFinal Test Bin SAF and STDF - Spark3\u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          2\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFDC Data Edge To Hadoop/Kafka       \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m         10\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mSTDF Burn-in                        \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m         18\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFinal Test TX STDF - Spark3         \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m         46\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFile Availability Metrics           \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          2\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mICN8 Track-out time based loading   \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m         46\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mICN8 BRS Feedback                   \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m         40\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mSaf Unit Probe                      \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          3\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFDC Prod Data Processing            \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          7\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mMaster Tables                       \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          4\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDatabase Ingest                     \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          2\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "└──────────────────────────────────────┴─────────────┴─────────┘\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ Multi-flow analysis complete!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[32m✓ Multi-flow analysis complete!\u001b[0m\n"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Cell 6: Run Multi-Flow Analysis\n",
    "\n",
    "console.print(\"\\n[cyan]Starting Multi-Flow NiFi Analysis...[/cyan]\\n\")\n",
    "\n",
    "# Connect to NiFi\n",
    "console.print(\"[yellow]Connecting to NiFi...[/yellow]\")\n",
    "client = NiFiClient(\n",
    "    base_url=CONFIG['nifi_url'],\n",
    "    username=CONFIG['username'],\n",
    "    password=CONFIG['password'],\n",
    "    verify_ssl=CONFIG['verify_ssl']\n",
    ")\n",
    "console.print(\"[green]OK[/green] Connected successfully\\n\")\n",
    "\n",
    "# Create analyzer and run analysis\n",
    "analyzer = MultiFlowAnalyzer(client=client, server=CONFIG['server'])\n",
    "analyzer.analyze_all_flows(CONFIG['flows_csv_path'])\n",
    "\n",
    "# Cleanup\n",
    "client.close()\n",
    "\n",
    "console.print(\"\\n[green]✓ Multi-flow analysis complete![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb83d48-0627-44f1-af76-bcdc0fcc0d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Saving snapshots to Delta Lake...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mSaving snapshots to Delta Lake\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:42,532 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Table exists, appending data to: 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mTable exists, appending data to: 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:43,530 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">OK</span> Data appended successfully\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mOK\u001b[0m Data appended successfully\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Timestamp: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:40</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">170613</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Timestamp: \u001b[1;36m2026\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m09\u001b[0m \u001b[1;92m18:59:40\u001b[0m.\u001b[1;36m170613\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:44,529 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Total rows written: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">188</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Total rows written: \u001b[1;36m188\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Sample data:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mSample data:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>snapshot_timestamp</th><th>server</th><th>flow_name</th><th>process_group_id</th><th>connection_id</th><th>connection_name</th><th>connection_group_id</th><th>source_id</th><th>source_name</th><th>destination_id</th><th>destination_name</th><th>flow_files_in</th><th>flow_files_out</th><th>bytes_in</th><th>bytes_out</th><th>input</th><th>output</th><th>queued_count</th><th>queued_bytes</th><th>queued</th><th>queued_size</th><th>percent_use_count</th><th>percent_use_bytes</th><th>stats_last_refreshed</th></tr></thead><tbody><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>STDF Unit Probe Ingest</td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>ea471957-dc64-3b05-a2ff-8e292e8bac9f</td><td></td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>null</td><td>From Ingest</td><td>null</td><td>input</td><td>4</td><td>4</td><td>83428</td><td>83428</td><td>4 (81.47 KB)</td><td>4 (81.47 KB)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>STDF Unit Probe Ingest</td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>ddde3800-a05a-1cb2-0000-00006a6159be</td><td></td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>null</td><td>to unit probe</td><td>null</td><td>from stdf ingest</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>STDF Unit Probe Ingest</td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>f198cfc4-f49a-3b94-98e2-071b04b9db2e</td><td></td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>null</td><td>to updates</td><td>null</td><td>To Unit Probe</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>File Ingest</td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>1c79317b-0193-1000-0000-000047ed80c8</td><td></td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>null</td><td>to cp_tx Kudu</td><td>null</td><td>input_kudu</td><td>7</td><td>7</td><td>3453369</td><td>3453369</td><td>7 (3.29 MB)</td><td>7 (3.29 MB)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>File Ingest</td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>0e352fbe-0191-1000-0000-000007369a75</td><td></td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>null</td><td>to bin results (mdlp)</td><td>null</td><td>to bin results (mdlp)</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>File Ingest</td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>0e2da00a-0191-1000-0000-000043641421</td><td></td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>null</td><td>to EDL Wat</td><td>null</td><td>to EDL WAT</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>File Ingest</td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>db9d36d9-a68e-16ac-0000-000008ffd444</td><td></td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>null</td><td>to CP TX</td><td>null</td><td>input</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>Final Test TX SAF - Spark3</td><td>7e7f1e93-c20b-3ab5-8791-9537827a965c</td><td>7a097d05-7f37-3022-87f6-92368ca58867</td><td></td><td>7e7f1e93-c20b-3ab5-8791-9537827a965c</td><td>null</td><td>routeToFTBinIngest</td><td>null</td><td>routeToFTBinIngest</td><td>2</td><td>2</td><td>48</td><td>48</td><td>2 (48 bytes)</td><td>2 (48 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>Final Test Bin SAF and STDF - Spark3</td><td>1b424d1e-3258-3283-b5bb-76a725bf1b11</td><td>da0b9bf4-d72d-3363-9a4f-9d57e1517262</td><td></td><td>1b424d1e-3258-3283-b5bb-76a725bf1b11</td><td>null</td><td>ftBinIngest</td><td>null</td><td>ftBinIngest</td><td>2</td><td>2</td><td>48</td><td>48</td><td>2 (48 bytes)</td><td>2 (48 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>Final Test Bin SAF and STDF - Spark3</td><td>1b424d1e-3258-3283-b5bb-76a725bf1b11</td><td>5cbf1863-0aa0-3a31-8ba2-15fdfc872e6b</td><td></td><td>1b424d1e-3258-3283-b5bb-76a725bf1b11</td><td>null</td><td>toFTDataProduct</td><td>null</td><td>toFTDataProduct</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "STDF Unit Probe Ingest",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         "ea471957-dc64-3b05-a2ff-8e292e8bac9f",
         "",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         null,
         "From Ingest",
         null,
         "input",
         4,
         4,
         83428,
         83428,
         "4 (81.47 KB)",
         "4 (81.47 KB)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "STDF Unit Probe Ingest",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         "ddde3800-a05a-1cb2-0000-00006a6159be",
         "",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         null,
         "to unit probe",
         null,
         "from stdf ingest",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "STDF Unit Probe Ingest",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         "f198cfc4-f49a-3b94-98e2-071b04b9db2e",
         "",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         null,
         "to updates",
         null,
         "To Unit Probe",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "File Ingest",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         "1c79317b-0193-1000-0000-000047ed80c8",
         "",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         null,
         "to cp_tx Kudu",
         null,
         "input_kudu",
         7,
         7,
         3453369,
         3453369,
         "7 (3.29 MB)",
         "7 (3.29 MB)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "File Ingest",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         "0e352fbe-0191-1000-0000-000007369a75",
         "",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         null,
         "to bin results (mdlp)",
         null,
         "to bin results (mdlp)",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "File Ingest",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         "0e2da00a-0191-1000-0000-000043641421",
         "",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         null,
         "to EDL Wat",
         null,
         "to EDL WAT",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "File Ingest",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         "db9d36d9-a68e-16ac-0000-000008ffd444",
         "",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         null,
         "to CP TX",
         null,
         "input",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "Final Test TX SAF - Spark3",
         "7e7f1e93-c20b-3ab5-8791-9537827a965c",
         "7a097d05-7f37-3022-87f6-92368ca58867",
         "",
         "7e7f1e93-c20b-3ab5-8791-9537827a965c",
         null,
         "routeToFTBinIngest",
         null,
         "routeToFTBinIngest",
         2,
         2,
         48,
         48,
         "2 (48 bytes)",
         "2 (48 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "Final Test Bin SAF and STDF - Spark3",
         "1b424d1e-3258-3283-b5bb-76a725bf1b11",
         "da0b9bf4-d72d-3363-9a4f-9d57e1517262",
         "",
         "1b424d1e-3258-3283-b5bb-76a725bf1b11",
         null,
         "ftBinIngest",
         null,
         "ftBinIngest",
         2,
         2,
         48,
         48,
         "2 (48 bytes)",
         "2 (48 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "Final Test Bin SAF and STDF - Spark3",
         "1b424d1e-3258-3283-b5bb-76a725bf1b11",
         "5cbf1863-0aa0-3a31-8ba2-15fdfc872e6b",
         "",
         "1b424d1e-3258-3283-b5bb-76a725bf1b11",
         null,
         "toFTDataProduct",
         null,
         "toFTDataProduct",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "snapshot_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "server",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "flow_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "process_group_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "connection_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "connection_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "connection_group_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "destination_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "destination_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "flow_files_in",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "flow_files_out",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bytes_in",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bytes_out",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "input",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "output",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "queued_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "queued_bytes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "queued",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "queued_size",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "percent_use_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "percent_use_bytes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "stats_last_refreshed",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     }
    }
   ],
   "source": [
    "# Cell 7: Save Snapshots to Delta Lake\n",
    "\n",
    "if CONFIG['enable_snapshots']:\n",
    "    console.print(\"\\n[yellow]Saving snapshots to Delta Lake...[/yellow]\")\n",
    "    \n",
    "    df = analyzer.get_results_dataframe()\n",
    "    \n",
    "    if df is not None:\n",
    "        table_name = CONFIG['delta_table_path']\n",
    "        \n",
    "        # Check if table exists\n",
    "        table_exists = spark.catalog._jcatalog.tableExists(table_name)\n",
    "        \n",
    "        if not table_exists:\n",
    "            # First run: Create table\n",
    "            console.print(f\"[yellow]Table doesn't exist, creating: {table_name}[/yellow]\")\n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(table_name)\n",
    "            console.print(f\"[green]OK[/green] Table created successfully with 24-field connection-level schema\")\n",
    "        else:\n",
    "            # Subsequent runs: Append data\n",
    "            console.print(f\"[yellow]Table exists, appending data to: {table_name}[/yellow]\")\n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .saveAsTable(table_name)\n",
    "            console.print(f\"[green]OK[/green] Data appended successfully\")\n",
    "        \n",
    "        console.print(f\"  Timestamp: {analyzer.snapshot_timestamp}\")\n",
    "        console.print(f\"  Total rows written: {df.count()}\")\n",
    "        \n",
    "        # Show sample\n",
    "        console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n",
    "        display(df.limit(10))\n",
    "    else:\n",
    "        console.print(\"[red]ERROR[/red] No data to save\")\n",
    "else:\n",
    "    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")\n",
    "\n",
    "# NOTE: If you need to manually drop the table to start fresh, run this in a separate cell:\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CONFIG['delta_table_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4708d0b-7e63-4d7c-94d2-ec64adb24c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Querying connection-level snapshots...</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mQuerying connection-level snapshots\u001b[0m\u001b[36m...\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Snapshot count by server and flow:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mSnapshot count by server and flow:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:45,529 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:46,119 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------------------------------------+---------+-----------------+--------------------------+\n|server  |flow_name                           |snapshots|total_connections|last_snapshot             |\n+--------+------------------------------------+---------+-----------------+--------------------------+\n|prod    |Database Ingest                     |5        |10               |2026-01-09 18:59:40.170613|\n|prod    |FDC Data Edge To Hadoop/Kafka       |5        |50               |2026-01-09 18:59:40.170613|\n|prod    |FDC Prod Data Processing            |5        |35               |2026-01-09 18:59:40.170613|\n|prod    |File Availability Metrics           |5        |10               |2026-01-09 18:59:40.170613|\n|prod    |File Ingest                         |5        |20               |2026-01-09 18:59:40.170613|\n|prod    |Final Test Bin SAF and STDF - Spark3|5        |10               |2026-01-09 18:59:40.170613|\n|prod    |Final Test TX SAF - Spark3          |5        |5                |2026-01-09 18:59:40.170613|\n|prod    |Final Test TX STDF - Spark3         |5        |230              |2026-01-09 18:59:40.170613|\n|prod    |ICN8 BRS Feedback                   |5        |200              |2026-01-09 18:59:40.170613|\n|prod    |ICN8 Track-out time based loading   |5        |230              |2026-01-09 18:59:40.170613|\n|prod    |Master Tables                       |5        |20               |2026-01-09 18:59:40.170613|\n|prod    |STDF Burn-in                        |5        |90               |2026-01-09 18:59:40.170613|\n|prod    |STDF Unit Probe Ingest              |5        |15               |2026-01-09 18:59:40.170613|\n|prod    |Saf Unit Probe                      |5        |15               |2026-01-09 18:59:40.170613|\n|thailand|ACCT Dicer                          |5        |300              |2026-01-09 18:59:09.14744 |\n|thailand|Archive GTP landing zone            |5        |170              |2026-01-09 18:59:09.14744 |\n|thailand|Chipsort                            |5        |120              |2026-01-09 18:59:09.14744 |\n|thailand|DIDT                                |5        |60               |2026-01-09 18:59:09.14744 |\n|thailand|Dicer DC                            |5        |135              |2026-01-09 18:59:09.14744 |\n|thailand|Dicer DC Curated                    |5        |70               |2026-01-09 18:59:09.14744 |\n+--------+------------------------------------+---------+-----------------+--------------------------+\nonly showing top 20 rows\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Connections with queued flowfiles </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">backpressure</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mConnections with queued flowfiles \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mbackpressure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:46,530 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:46,662 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+---------------------------------+---------------------------------+-------------------------------+--------------------+----------------+----------------+\n|server|flow_name                        |source_name                      |destination_name               |max_queued_flowfiles|max_queued_bytes|max_percent_full|\n+------+---------------------------------+---------------------------------+-------------------------------+--------------------+----------------+----------------+\n|prod  |Final Test TX STDF - Spark3      |Try with more partitions and time|Catch errors                   |200                 |0               |1               |\n|prod  |Final Test TX STDF - Spark3      |Add new partition AWS Redshift   |Catch errors                   |70                  |0               |0               |\n|prod  |Final Test TX STDF - Spark3      |Split even/odd days              |Ingest stdf.stdf_ft_dp - Spark3|67                  |0               |1               |\n|prod  |ICN8 Track-out time based loading|Move file to \"processing\" folder |Dummy                          |23                  |0               |0               |\n|prod  |ICN8 BRS Feedback                |Funnel                           |Dummy                          |7                   |0               |0               |\n+------+---------------------------------+---------------------------------+-------------------------------+--------------------+----------------+----------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Connections approaching queue limits </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">&gt;</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">50</span><span style=\"color: #808000; text-decoration-color: #808000\">% full</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mConnections approaching queue limits \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33m>\u001b[0m\u001b[1;33m50\u001b[0m\u001b[33m% full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+---------+-----------+----------------+----------------+----------------+\n|server|flow_name|source_name|destination_name|max_percent_full|max_queued_count|\n+------+---------+-----------+----------------+----------------+----------------+\n+------+---------+-----------+----------------+----------------+----------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Inactive connections </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">no flowfiles for </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">7</span><span style=\"color: #808000; text-decoration-color: #808000\"> days</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mInactive connections \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mno flowfiles for \u001b[0m\u001b[1;33m7\u001b[0m\u001b[33m days\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:47,529 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:47,728 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+------------------------------------+------------------------------+------------------------------+---------------+-------------+\n|server|flow_name                           |source_name                   |destination_name              |delta_flowfiles|num_snapshots|\n+------+------------------------------------+------------------------------+------------------------------+---------------+-------------+\n|prod  |Database Ingest                     |from related tables trigger   |from tx trigger               |0              |5            |\n|prod  |Database Ingest                     |sleuth_ingest out             |sleuth_ingest                 |0              |5            |\n|prod  |FDC Data Edge To Hadoop/Kafka       |ATKH Bridge To Kafka          |ATKH File Receiver            |0              |5            |\n|prod  |FDC Data Edge To Hadoop/Kafka       |ATKH Kafka Output             |ATKH Bridge To Kafka          |0              |5            |\n|prod  |FDC Prod Data Processing            |Email Xinyu                   |Done Email                    |0              |5            |\n|prod  |FDC Prod Data Processing            |Email Xinyu                   |Failed, Resend in 30 min      |0              |5            |\n|prod  |FDC Prod Data Processing            |Failed, Resend in 30 min      |Email Xinyu                   |0              |5            |\n|prod  |FDC Prod Data Processing            |Out For Email                 |Email Xinyu                   |0              |5            |\n|prod  |FDC Prod Data Processing            |Out For Email Notification    |Email Xinyu                   |0              |5            |\n|prod  |FDC Prod Data Processing            |Out For Error Notification    |Email Xinyu                   |0              |5            |\n|prod  |File Availability Metrics           |to_efars_load                 |to_efars_load                 |0              |5            |\n|prod  |File Availability Metrics           |to_hdfs_load                  |to_hdfs_load                  |0              |5            |\n|prod  |File Ingest                         |to CP TX                      |input                         |0              |5            |\n|prod  |File Ingest                         |to EDL Wat                    |to EDL WAT                    |0              |5            |\n|prod  |File Ingest                         |to bin results (mdlp)         |to bin results (mdlp)         |0              |5            |\n|prod  |Final Test Bin SAF and STDF - Spark3|toFTDataProduct               |toFTDataProduct               |0              |5            |\n|prod  |Final Test TX STDF - Spark3         |Add new partition AWS Redshift|Catch errors                  |0              |5            |\n|prod  |Final Test TX STDF - Spark3         |Add new partition and refresh |Try after 30 mins             |0              |5            |\n|prod  |Final Test TX STDF - Spark3         |Add new partition and refresh |Add new partition AWS Redshift|0              |5            |\n|prod  |Final Test TX STDF - Spark3         |Funnel                        |Set partition                 |0              |5            |\n+------+------------------------------------+------------------------------+------------------------------+---------------+-------------+\nonly showing top 20 rows\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Inactive processors by flow </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">aggregated from connections</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mInactive processors by flow \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33maggregated from connections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:48,373 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------------------------------------+------------------------+\n|server  |flow_name                           |inactive_processor_count|\n+--------+------------------------------------+------------------------+\n|prod    |ICN8 BRS Feedback                   |28                      |\n|prod    |STDF Burn-in                        |15                      |\n|prod    |Final Test TX STDF - Spark3         |12                      |\n|prod    |ICN8 Track-out time based loading   |10                      |\n|prod    |FDC Prod Data Processing            |5                       |\n|prod    |Master Tables                       |4                       |\n|prod    |File Ingest                         |3                       |\n|prod    |Saf Unit Probe                      |2                       |\n|prod    |Database Ingest                     |2                       |\n|prod    |File Availability Metrics           |2                       |\n|prod    |FDC Data Edge To Hadoop/Kafka       |2                       |\n|prod    |STDF Unit Probe Ingest              |2                       |\n|prod    |Final Test Bin SAF and STDF - Spark3|1                       |\n|thailand|ACCT Dicer                          |23                      |\n|thailand|RPM v 2.0                           |22                      |\n|thailand|RPM v 2.1                           |21                      |\n|thailand|Chipsort                            |18                      |\n|thailand|Dicer Log v3                        |18                      |\n|thailand|WBAOI - POC                         |18                      |\n|thailand|Molding - POC                       |17                      |\n+--------+------------------------------------+------------------------+\nonly showing top 20 rows\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Queue depth trends </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">hourly averages</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mQueue depth trends \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mhourly averages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:48,530 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:48,898 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+------+---------------------------------+---------------------------------+-------------------------------+--------------------+--------------------+\n|hour               |server|flow_name                        |source_name                      |destination_name               |avg_queued_flowfiles|max_queued_flowfiles|\n+-------------------+------+---------------------------------+---------------------------------+-------------------------------+--------------------+--------------------+\n|2026-01-09 18:00:00|prod  |Final Test TX STDF - Spark3      |Try with more partitions and time|Catch errors                   |192.5               |200                 |\n|2026-01-09 18:00:00|prod  |Final Test TX STDF - Spark3      |Add new partition AWS Redshift   |Catch errors                   |70.0                |70                  |\n|2026-01-09 18:00:00|prod  |Final Test TX STDF - Spark3      |Split even/odd days              |Ingest stdf.stdf_ft_dp - Spark3|51.1                |67                  |\n|2026-01-09 18:00:00|prod  |ICN8 Track-out time based loading|Move file to \"processing\" folder |Dummy                          |23.0                |23                  |\n|2026-01-09 18:00:00|prod  |ICN8 BRS Feedback                |Funnel                           |Dummy                          |7.0                 |7                   |\n+-------------------+------+---------------------------------+---------------------------------+-------------------------------+--------------------+--------------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Flow balance </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input vs output by connection</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mFlow balance \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33minput vs output by connection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:49,388 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+---------+-----------+----------------+------------------+-------------------+----------+\n|server|flow_name|source_name|destination_name|total_flowfiles_in|total_flowfiles_out|net_change|\n+------+---------+-----------+----------------+------------------+-------------------+----------+\n+------+---------+-----------+----------------+------------------+-------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Query Historical Snapshots (Connection-Level Analysis)\n",
    "\n",
    "if CONFIG['enable_snapshots']:\n",
    "    console.print(\"\\n[cyan]Querying connection-level snapshots...[/cyan]\\n\")\n",
    "    \n",
    "    table_name = CONFIG['delta_table_path']\n",
    "    \n",
    "    try:\n",
    "        # Show snapshots per flow and server\n",
    "        console.print(\"[yellow]Snapshot count by server and flow:[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                COUNT(DISTINCT snapshot_timestamp) as snapshots,\n",
    "                COUNT(*) as total_connections,\n",
    "                MAX(snapshot_timestamp) as last_snapshot\n",
    "            FROM {table_name}\n",
    "            GROUP BY server, flow_name\n",
    "            ORDER BY server, flow_name\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # NEW: Find connections with high queue depth (backpressure detection)\n",
    "        console.print(\"\\n[yellow]Connections with queued flowfiles (backpressure):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                source_name,\n",
    "                destination_name,\n",
    "                MAX(queued_count) as max_queued_flowfiles,\n",
    "                MAX(queued_bytes) as max_queued_bytes,\n",
    "                MAX(percent_use_count) as max_percent_full\n",
    "            FROM {table_name}\n",
    "            WHERE queued_count > 0\n",
    "            GROUP BY server, flow_name, source_name, destination_name\n",
    "            ORDER BY max_queued_flowfiles DESC\n",
    "            LIMIT 20\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # NEW: Identify connections approaching queue limits\n",
    "        console.print(\"\\n[yellow]Connections approaching queue limits (>50% full):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                source_name,\n",
    "                destination_name,\n",
    "                MAX(percent_use_count) as max_percent_full,\n",
    "                MAX(queued_count) as max_queued_count\n",
    "            FROM {table_name}\n",
    "            WHERE percent_use_count > 50\n",
    "            GROUP BY server, flow_name, source_name, destination_name\n",
    "            ORDER BY max_percent_full DESC\n",
    "            LIMIT 20\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # Find inactive connections (no flow for 7 days)\n",
    "        console.print(\"\\n[yellow]Inactive connections (no flowfiles for 7 days):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            WITH connection_activity AS (\n",
    "                SELECT \n",
    "                    server,\n",
    "                    flow_name,\n",
    "                    source_name,\n",
    "                    destination_name,\n",
    "                    MAX(flow_files_out) - MIN(flow_files_out) as delta_flowfiles,\n",
    "                    MIN(snapshot_timestamp) as first_snapshot,\n",
    "                    MAX(snapshot_timestamp) as last_snapshot,\n",
    "                    COUNT(DISTINCT snapshot_timestamp) as num_snapshots\n",
    "                FROM {table_name}\n",
    "                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "                GROUP BY server, flow_name, source_name, destination_name\n",
    "            )\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                source_name,\n",
    "                destination_name,\n",
    "                delta_flowfiles,\n",
    "                num_snapshots\n",
    "            FROM connection_activity\n",
    "            WHERE delta_flowfiles = 0\n",
    "            ORDER BY server, flow_name, source_name\n",
    "            LIMIT 50\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # Aggregate to processor level (still possible!)\n",
    "        console.print(\"\\n[yellow]Inactive processors by flow (aggregated from connections):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            WITH processor_activity AS (\n",
    "                SELECT \n",
    "                    server,\n",
    "                    flow_name,\n",
    "                    source_name as processor_name,\n",
    "                    MAX(flow_files_out) - MIN(flow_files_out) as delta_flowfiles\n",
    "                FROM {table_name}\n",
    "                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "                GROUP BY server, flow_name, source_name\n",
    "            )\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                COUNT(*) as inactive_processor_count\n",
    "            FROM processor_activity\n",
    "            WHERE delta_flowfiles = 0\n",
    "            GROUP BY server, flow_name\n",
    "            ORDER BY server, inactive_processor_count DESC\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # NEW: Track queue growth over time\n",
    "        console.print(\"\\n[yellow]Queue depth trends (hourly averages):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                DATE_TRUNC('hour', snapshot_timestamp) as hour,\n",
    "                server,\n",
    "                flow_name,\n",
    "                source_name,\n",
    "                destination_name,\n",
    "                AVG(queued_count) as avg_queued_flowfiles,\n",
    "                MAX(queued_count) as max_queued_flowfiles\n",
    "            FROM {table_name}\n",
    "            WHERE snapshot_timestamp >= current_date() - INTERVAL 1 DAYS\n",
    "              AND queued_count > 0\n",
    "            GROUP BY hour, server, flow_name, source_name, destination_name\n",
    "            ORDER BY hour DESC, avg_queued_flowfiles DESC\n",
    "            LIMIT 20\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # NEW: Bidirectional flow analysis\n",
    "        console.print(\"\\n[yellow]Flow balance (input vs output by connection):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                source_name,\n",
    "                destination_name,\n",
    "                SUM(flow_files_in) as total_flowfiles_in,\n",
    "                SUM(flow_files_out) as total_flowfiles_out,\n",
    "                SUM(flow_files_in) - SUM(flow_files_out) as net_change\n",
    "            FROM {table_name}\n",
    "            WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "            GROUP BY server, flow_name, source_name, destination_name\n",
    "            HAVING ABS(SUM(flow_files_in) - SUM(flow_files_out)) > 100\n",
    "            ORDER BY ABS(net_change) DESC\n",
    "            LIMIT 20\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]ERROR[/red] Failed to query: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baffe3ae-fe19-4192-b52d-ce7cc9760e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:49,558 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# # Cell 9: Export Results to CSV by Flow\n",
    "\n",
    "# console.print(\"\\n[yellow]Exporting results to CSV...[/yellow]\")\n",
    "\n",
    "# timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# df = analyzer.get_results_dataframe()\n",
    "# if df is not None:\n",
    "#     pdf = df.toPandas()\n",
    "    \n",
    "#     # Export overall summary\n",
    "#     output_path = f\"/dbfs/nifi_analysis/all_flows_{timestamp_str}.csv\"\n",
    "#     pdf.to_csv(output_path, index=False)\n",
    "#     console.print(f\"[green]OK[/green] All flows exported to {output_path}\")\n",
    "    \n",
    "#     # Export per flow\n",
    "#     for flow_name in pdf['flow_name'].unique():\n",
    "#         flow_df = pdf[pdf['flow_name'] == flow_name]\n",
    "#         flow_path = f\"/dbfs/nifi_analysis/{flow_name}_{timestamp_str}.csv\"\n",
    "#         flow_df.to_csv(flow_path, index=False)\n",
    "#         console.print(f\"  [green]✓[/green] {flow_name}: {len(flow_df)} processors\")\n",
    "    \n",
    "#     console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n",
    "#     display(pdf.head(10))\n",
    "# else:\n",
    "#     console.print(\"[red]ERROR[/red] No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4eb366b-247a-44ba-a9fa-fb29a19b9c73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "---\n\n## Delta Table Schema (Processor-Level)\n\nThe Delta table captures **processor-level statistics** from NiFi Status API's `processorStatusSnapshots` array. This provides processor IDs, activity metrics, and run status directly from NiFi.\n\n### 13 Total Fields\n\n| Column | Type | Description |\n|--------|------|-------------|\n| **Metadata (4 fields)** | | |\n| `snapshot_timestamp` | Timestamp | When the snapshot was captured |\n| `server` | String | Server identifier (prod, thailand, etc.) |\n| `flow_name` | String | Flow name from CSV |\n| `process_group_id` | String | NiFi process group ID |\n| **Processor Identity (3 fields)** | | |\n| `processor_id` | String | Processor UUID (from Status API) |\n| `processor_name` | String | Processor name |\n| `processor_type` | String | Processor type (e.g., UpdateAttribute, RouteOnAttribute) |\n| **Activity Metrics - 5-minute window (6 fields)** | | |\n| `flow_files_in` | Long | FlowFiles entering processor |\n| `bytes_in` | Long | Bytes entering processor |\n| `flow_files_out` | Long | FlowFiles leaving processor |\n| `bytes_out` | Long | Bytes leaving processor |\n| `tasks` | Long | Number of tasks executed |\n| `run_status` | String | Processor status (Running, Stopped, etc.) |\n\n## Data Source\n\n**API Used**: Status API with `recursive=true`\n```\nGET /nifi-api/flow/process-groups/{id}/status?recursive=true\n```\n\n**Data Path**:\n```\nprocessGroupStatus\n  └─ aggregateSnapshot\n      └─ processorStatusSnapshots[]\n          └─ processorStatusSnapshot\n              ├─ id (processor ID)\n              ├─ name (processor name)\n              ├─ type (processor type)\n              ├─ flowFilesIn, bytesIn\n              ├─ flowFilesOut, bytesOut\n              ├─ taskCount\n              └─ runStatus\n```\n\n## Key Features\n\n### Why Processor-Level?\n- **Direct from NiFi**: Processor IDs come directly from Status API (no name matching)\n- **Simple**: One API call instead of two (no Flow API needed)\n- **Complete**: Captures all processors including nested process groups (`recursive=true`)\n- **Accurate**: No recursion bugs or name-matching errors\n\n### What's Included\n- ✅ Processor IDs (UUIDs)\n- ✅ Processor names and types\n- ✅ Input/output metrics (flowfiles and bytes)\n- ✅ Task execution counts\n- ✅ Run status (Running, Stopped, etc.)\n\n### What's NOT Included\n- ❌ Connection-level data (use connection analysis for that)\n- ❌ Queue metrics (not in processorStatusSnapshots)\n- ❌ Connection bottleneck analysis\n\n## Analysis Queries\n\n### 1. Find Inactive Processors\nIdentify processors with no activity over time:\n```sql\nWITH processor_activity AS (\n    SELECT \n        server,\n        flow_name,\n        processor_name,\n        processor_id,\n        MAX(flow_files_out) - MIN(flow_files_out) as delta_flowfiles,\n        MAX(tasks) - MIN(tasks) as delta_tasks,\n        COUNT(DISTINCT snapshot_timestamp) as snapshots\n    FROM 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes\n    WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n    GROUP BY server, flow_name, processor_name, processor_id\n)\nSELECT *\nFROM processor_activity\nWHERE delta_flowfiles = 0 AND delta_tasks = 0\nORDER BY server, flow_name;\n```\n\n### 2. Find Most Active Processors\n```sql\nSELECT \n    server,\n    flow_name,\n    processor_name,\n    processor_type,\n    SUM(flow_files_out) as total_flowfiles,\n    SUM(bytes_out) as total_bytes,\n    SUM(tasks) as total_tasks\nFROM 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes\nWHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\nGROUP BY server, flow_name, processor_name, processor_type\nORDER BY total_flowfiles DESC\nLIMIT 20;\n```\n\n### 3. Check Processor Status\nFind stopped or disabled processors:\n```sql\nSELECT \n    server,\n    flow_name,\n    processor_name,\n    processor_type,\n    run_status,\n    MAX(snapshot_timestamp) as last_seen\nFROM 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes\nWHERE run_status != 'Running'\nGROUP BY server, flow_name, processor_name, processor_type, run_status\nORDER BY server, flow_name;\n```\n\n### 4. Track Processor Activity Over Time\n```sql\nSELECT \n    DATE_TRUNC('hour', snapshot_timestamp) as hour,\n    processor_name,\n    AVG(flow_files_out) as avg_flowfiles_per_5min,\n    SUM(tasks) as total_tasks\nFROM 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes\nWHERE snapshot_timestamp >= current_date() - INTERVAL 1 DAYS\n  AND processor_name = 'YourProcessorName'\nGROUP BY hour, processor_name\nORDER BY hour;\n```\n\n### 5. Compare Processors Across Environments\n```sql\nSELECT \n    server,\n    processor_type,\n    COUNT(DISTINCT processor_id) as processor_count,\n    SUM(flow_files_out) as total_flowfiles,\n    AVG(flow_files_out) as avg_flowfiles_per_snapshot\nFROM 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes\nWHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\nGROUP BY server, processor_type\nORDER BY server, total_flowfiles DESC;\n```\n\n## Unity Catalog Configuration\n\n- **Catalog**: `1dp_mfg_sbx`\n- **Schema**: `validation_test_eric`\n- **Table**: `nifi_processor_snapshots_full_attributes`\n- **Full path**: `1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes`\n\n## CSV Format\n\nYour flows CSV should look like:\n```\nid,flow_name\nd6193dbd-3358-1e5a-ad4b-ac6506f01403,Setup data_type\nabc-123-def-456-7890-abcdef123456,Production_Flow\n```\n\n## Server Identifier\n\nThe `server` field tracks data from multiple NiFi instances:\n- **Prod**: `us-chd01-prod-nifi.us-chd01.nxp.com:8443`\n- **Thailand**: `thbnk01hdpnp002.th-bnk01.nxp.com:8443`\n\nThis allows you to:\n- Compare processor usage across environments\n- Track different NiFi clusters\n- Aggregate metrics by server\n\n## Implementation Notes\n\n**Snapshot Frequency**: Run every 5-30 minutes for trend analysis\n\n**Schema Evolution**: Uses `mergeSchema=true` - new fields are automatically added\n\n**Performance**: One API call per flow (Status API with recursive=true)\n\n🤖 Generated with [Claude Code](https://claude.com/claude-code)"
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nifi_analyzer_databricks_multi_flow_prod",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}