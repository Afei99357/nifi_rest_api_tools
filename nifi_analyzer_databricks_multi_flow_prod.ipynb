{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6791bf8-1576-4719-aaac-8ffd4cbd4d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# NiFi Processor Usage Analyzer - Multi-Flow Edition\n",
    "\n",
    "This notebook analyzes NiFi processor execution counts across **multiple process groups** to identify unused or underutilized processors.\n",
    "\n",
    "**Features:**\n",
    "- Analyzes multiple flows from CSV input\n",
    "- Fast execution count analysis (~5-10 seconds per flow)\n",
    "- Snapshot mode with flow_name tracking\n",
    "- Delta Lake integration with timestamp\n",
    "- Standalone - no external files needed\n",
    "\n",
    "**Setup:**\n",
    "1. Upload CSV with flow definitions (id, flow_name)\n",
    "2. Edit the configuration in Cell 3\n",
    "3. Run all cells\n",
    "4. View results in Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adef03ec-294c-45f1-b52b-9b4bc98c7479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:36,530 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:37,530 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "%pip install requests rich --quiet\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea522c1a-91a6-46cf-b8c5-9958a4d56be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Libraries imported successfully!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:38,549 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Libraries\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn\n",
    "\n",
    "# Databricks-specific imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "# Disable SSL warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('nifi_analyzer')\n",
    "\n",
    "# Initialize Rich console\n",
    "console = Console()\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a83ba3-6c06-4fcc-9509-d19304cd414e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Configuration loaded!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Configuration loaded!\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  NiFi URL: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://us-chd01-prod-nifi.us-chd01.nxp.com:8443/nifi/</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  NiFi URL: \u001b[4;94mhttps://us-chd01-prod-nifi.us-chd01.nxp.com:8443/nifi/\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Username: nxg16670\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Username: nxg16670\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Server: prod\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Server: prod\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Flows CSV: <span style=\"color: #800080; text-decoration-color: #800080\">/Volumes/1dp_mfg_sbx/validation_test_eric/files/nifi_flow_status/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">nifi_group_ids_prod.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Flows CSV: \u001b[35m/Volumes/1dp_mfg_sbx/validation_test_eric/files/nifi_flow_status/\u001b[0m\u001b[95mnifi_group_ids_prod.csv\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Delta table: 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Delta table: 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Snapshots enabled: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Snapshots enabled: \u001b[3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Cell 3: Configuration\n",
    "# EDIT THESE VALUES FOR YOUR NIFI INSTANCE\n",
    "\n",
    "CONFIG = {\n",
    "    # NiFi Connection\n",
    "    'nifi_url': 'https://us-chd01-prod-nifi.us-chd01.nxp.com:8443/nifi/',\n",
    "    'username': 'nxg16670',\n",
    "    'password': '6be!x!_Ex855cXJ',  # ← EDIT THIS\n",
    "    'verify_ssl': False,\n",
    "    \n",
    "    # Server Identifier (for tracking multiple NiFi servers)\n",
    "    'server': 'prod',  # ← EDIT THIS (e.g., 'prod', 'dev', hostname)\n",
    "    \n",
    "    # Flow Definitions CSV\n",
    "    # CSV Format: id,flow_name\n",
    "    # Example:\n",
    "    #   8c8677c4-29d6-36...,Production_Flow_1\n",
    "    #   abc-123-def...,Development_Flow_2\n",
    "    'flows_csv_path': '/Volumes/1dp_mfg_sbx/validation_test_eric/files/nifi_flow_status/nifi_group_ids_prod.csv',  # ← Path to your CSV\n",
    "    \n",
    "    # Snapshot Storage (Unity Catalog - 3-level naming)\n",
    "    'enable_snapshots': True,\n",
    "    'delta_table_path': '1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes',  # catalog.schema.table\n",
    "}\n",
    "\n",
    "console.print(\"[green]✓ Configuration loaded![/green]\")\n",
    "console.print(f\"  NiFi URL: {CONFIG['nifi_url']}\")\n",
    "console.print(f\"  Username: {CONFIG['username']}\")\n",
    "console.print(f\"  Server: {CONFIG['server']}\")\n",
    "console.print(f\"  Flows CSV: {CONFIG['flows_csv_path']}\")\n",
    "console.print(f\"  Delta table: {CONFIG['delta_table_path']}\")\n",
    "console.print(f\"  Snapshots enabled: {CONFIG['enable_snapshots']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7358fe55-8364-4397-b48d-b6f9ef4327bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Cell 4: NiFi Client Class - SIMPLIFIED PROCESSOR-LEVEL\n\nclass NiFiClient:\n    \"\"\"Client for interacting with Apache NiFi REST API.\"\"\"\n    \n    def __init__(self, base_url: str, username: str, password: str, verify_ssl: bool = True):\n        self.base_url = base_url.rstrip('/')\n        if not self.base_url.endswith('/nifi'):\n            self.base_url += '/nifi'\n        self.api_url = f\"{self.base_url}-api\"\n        self.verify_ssl = verify_ssl\n        self.session = requests.Session()\n        self.token = None\n        self.username = username\n        self.password = password\n        self._authenticate(username, password)\n        \n    def _authenticate(self, username: str, password: str) -> None:\n        \"\"\"Authenticate with NiFi.\"\"\"\n        try:\n            response = requests.post(\n                f\"{self.api_url}/access/token\",\n                data={'username': username, 'password': password},\n                verify=self.verify_ssl\n            )\n            \n            if response.status_code == 201:\n                self.token = response.text\n                self.session.headers.update({'Authorization': f'Bearer {self.token}'})\n                logger.info(\"Successfully authenticated with token\")\n            else:\n                logger.warning(f\"Token auth failed with status {response.status_code}\")\n                logger.warning(\"Falling back to basic auth\")\n                from requests.auth import HTTPBasicAuth\n                self.session.auth = HTTPBasicAuth(username, password)\n        except Exception as e:\n            logger.warning(f\"Token auth error: {e}, falling back to basic auth\")\n            from requests.auth import HTTPBasicAuth\n            self.session.auth = HTTPBasicAuth(username, password)\n    \n    def _request(self, method: str, endpoint: str, **kwargs) -> requests.Response:\n        \"\"\"Make authenticated request with 401 retry.\"\"\"\n        url = f\"{self.api_url}/{endpoint.lstrip('/')}\"\n        kwargs.setdefault('verify', self.verify_ssl)\n        \n        response = self.session.request(method, url, **kwargs)\n        \n        # Handle 401 by re-authenticating once\n        if response.status_code == 401:\n            logger.warning(\"Received 401, attempting re-authentication\")\n            self._authenticate(self.username, self.password)\n            response = self.session.request(method, url, **kwargs)\n            if response.status_code == 401:\n                raise Exception(\"Authentication failed: Unauthorized\")\n        \n        response.raise_for_status()\n        return response\n    \n    def get_process_group(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get process group details including all processors.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}\")\n        return response.json()\n    \n    def list_processors(self, process_group_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Get all processors in a process group (recursive).\"\"\"\n        pg_data = self.get_process_group(process_group_id)\n        processors = pg_data[\"processGroupFlow\"][\"flow\"][\"processors\"]\n        \n        # Recursively get processors from child groups\n        child_groups = pg_data[\"processGroupFlow\"][\"flow\"][\"processGroups\"]\n        for child in child_groups:\n            processors.extend(self.list_processors(child[\"id\"]))\n        \n        return processors\n    \n    def get_process_group_status(self, group_id: str) -> Dict[str, Any]:\n        \"\"\"Get execution statistics for process group.\"\"\"\n        response = self._request(\"GET\", f\"/flow/process-groups/{group_id}/status\")\n        return response.json()\n    \n    def get_processor_statistics(self, group_id: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get processor-level statistics with IDs.\n        \n        SIMPLIFIED APPROACH:\n        1. Get processor list from Flow API (has IDs, names, types)\n        2. Get Status API and aggregate connections by processor name\n        3. Match by name to add IDs\n        \n        Returns:\n            List of processor dictionaries with IDs and aggregated metrics\n        \"\"\"\n        # Helper to safely convert to int\n        def safe_int(value, default=0):\n            \"\"\"Convert value to int, handling strings and None.\"\"\"\n            if value is None:\n                return default\n            try:\n                return int(value)\n            except (ValueError, TypeError):\n                return default\n        \n        # Step 1: Get processor list from Flow API (has IDs!)\n        processors = self.list_processors(group_id)\n        processor_lookup = {}\n        \n        for proc in processors:\n            component = proc.get('component', {})\n            proc_name = component.get('name')\n            proc_id = proc.get('id')\n            proc_type = component.get('type', '').split('.')[-1]\n            \n            processor_lookup[proc_name] = {\n                'id': proc_id,\n                'name': proc_name,\n                'type': proc_type\n            }\n        \n        logger.info(f\"Found {len(processor_lookup)} processors from Flow API\")\n        \n        # Step 2: Get Status API and aggregate by processor name\n        status_data = self.get_process_group_status(group_id)\n        processor_stats = {}\n        \n        def aggregate_connections(pg_status):\n            \"\"\"Recursively aggregate connection stats by source processor.\"\"\"\n            connection_statuses = pg_status.get(\"aggregateSnapshot\", {}).get(\"connectionStatusSnapshots\", [])\n            \n            for conn_status in connection_statuses:\n                conn_snap = conn_status.get(\"connectionStatusSnapshot\", {})\n                source_name = conn_snap.get('sourceName')\n                \n                if source_name:\n                    if source_name not in processor_stats:\n                        processor_stats[source_name] = {\n                            'flow_files_out': 0,\n                            'bytes_out': 0,\n                            'total_queued_count': 0,\n                            'total_queued_bytes': 0,\n                            'max_percent_use_count': 0\n                        }\n                    \n                    # Aggregate metrics with safe_int conversion\n                    processor_stats[source_name]['flow_files_out'] += safe_int(conn_snap.get('flowFilesOut', 0))\n                    processor_stats[source_name]['bytes_out'] += safe_int(conn_snap.get('bytesOut', 0))\n                    processor_stats[source_name]['total_queued_count'] += safe_int(conn_snap.get('queuedCount', 0))\n                    processor_stats[source_name]['total_queued_bytes'] += safe_int(conn_snap.get('queuedBytes', 0))\n                    processor_stats[source_name]['max_percent_use_count'] = max(\n                        processor_stats[source_name]['max_percent_use_count'],\n                        safe_int(conn_snap.get('percentUseCount', 0))\n                    )\n            \n            # Recurse into child groups\n            child_statuses = pg_status.get(\"processGroupStatus\", [])\n            for child_pg in child_statuses:\n                aggregate_connections(child_pg)\n        \n        pg_status = status_data.get(\"processGroupStatus\", {})\n        if pg_status:\n            aggregate_connections(pg_status)\n        \n        logger.info(f\"Aggregated stats for {len(processor_stats)} processors from Status API\")\n        \n        # Step 3: Merge - add IDs to stats\n        result = []\n        for proc_name, stats in processor_stats.items():\n            proc_info = processor_lookup.get(proc_name, {})\n            \n            result.append({\n                'processor_id': proc_info.get('id'),  # From Flow API\n                'processor_name': proc_name,\n                'processor_type': proc_info.get('type', 'Unknown'),\n                'flow_files_out': stats['flow_files_out'],\n                'bytes_out': stats['bytes_out'],\n                'total_queued_count': stats['total_queued_count'],\n                'total_queued_bytes': stats['total_queued_bytes'],\n                'max_percent_use_count': stats['max_percent_use_count']\n            })\n        \n        logger.info(f\"Returning {len(result)} processor records with IDs\")\n        return result\n    \n    def close(self):\n        \"\"\"Close session.\"\"\"\n        self.session.close()\n\nconsole.print(\"[green]✓ NiFiClient class defined (simplified processor-level)![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f3c653-5945-470c-a95c-0dbfb7b60e29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Cell 5: Multi-Flow Analyzer Class - SIMPLIFIED PROCESSOR-LEVEL\n\nclass MultiFlowAnalyzer:\n    \"\"\"Analyzes multiple NiFi flows and stores PROCESSOR-LEVEL results in Delta Lake.\"\"\"\n    \n    def __init__(self, client: NiFiClient, server: str = 'unknown'):\n        self.client = client\n        self.console = Console()\n        self.server = server\n        self.all_results = []\n        self.snapshot_timestamp = datetime.now()\n    \n    def analyze_flow(self, flow_id: str, flow_name: str) -> Dict:\n        \"\"\"Analyze a single flow - processor level.\"\"\"\n        flow_results = {\n            'flow_name': flow_name,\n            'flow_id': flow_id,\n            'processor_count': 0,\n            'processors': []\n        }\n        \n        try:\n            # Get processor-level statistics (SIMPLIFIED!)\n            processors = self.client.get_processor_statistics(flow_id)\n            flow_results['processor_count'] = len(processors)\n            \n            # Add metadata to each processor\n            for proc in processors:\n                flow_results['processors'].append({\n                    # Metadata (4 fields)\n                    'snapshot_timestamp': self.snapshot_timestamp,\n                    'server': self.server,\n                    'flow_name': flow_name,\n                    'process_group_id': flow_id,\n                    \n                    # Processor identity (3 fields)\n                    'processor_id': proc.get('processor_id'),\n                    'processor_name': proc.get('processor_name'),\n                    'processor_type': proc.get('processor_type'),\n                    \n                    # Aggregated metrics (4 fields)\n                    'flow_files_out': proc.get('flow_files_out', 0),\n                    'bytes_out': proc.get('bytes_out', 0),\n                    'total_queued_count': proc.get('total_queued_count', 0),\n                    'total_queued_bytes': proc.get('total_queued_bytes', 0),\n                    'max_percent_use_count': proc.get('max_percent_use_count', 0)\n                })\n            \n            return flow_results\n            \n        except Exception as e:\n            self.console.print(f\"[red]ERROR[/red] Failed to analyze {flow_name}: {e}\")\n            flow_results['error'] = str(e)\n            return flow_results\n    \n    def analyze_all_flows(self, flows_csv_path: str):\n        \"\"\"Analyze all flows from CSV.\"\"\"\n        self.console.print(f\"\\n[cyan]Multi-Flow Analysis Starting (PROCESSOR-LEVEL)...[/cyan]\")\n        self.console.print(f\"  Server: {self.server}\")\n        self.console.print(f\"  Timestamp: {self.snapshot_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        \n        # Read flows CSV\n        try:\n            flows_df = spark.read.csv(flows_csv_path, header=True)\n            flows = flows_df.collect()\n            \n            self.console.print(f\"[green]Found {len(flows)} flows to analyze[/green]\\n\")\n            \n        except Exception as e:\n            self.console.print(f\"[red]ERROR[/red] Failed to read CSV: {e}\")\n            raise\n        \n        # Analyze each flow\n        with Progress(\n            SpinnerColumn(),\n            TextColumn(\"[progress.description]{task.description}\"),\n            BarColumn(),\n            console=self.console\n        ) as progress:\n            task = progress.add_task(\"Analyzing flows...\", total=len(flows))\n            \n            for flow in flows:\n                flow_id = flow['id']\n                flow_name = flow['flow_name']\n                \n                progress.update(task, description=f\"Analyzing: {flow_name}\")\n                \n                flow_results = self.analyze_flow(flow_id, flow_name)\n                self.all_results.append(flow_results)\n                \n                # Display flow summary\n                if 'error' not in flow_results:\n                    self.console.print(\n                        f\"  [green]✓[/green] {flow_name}: {flow_results['processor_count']} processors\"\n                    )\n                else:\n                    self.console.print(\n                        f\"  [red]✗[/red] {flow_name}: {flow_results['error']}\"\n                    )\n                \n                progress.advance(task)\n        \n        # Display overall summary\n        self.display_summary()\n    \n    def display_summary(self):\n        \"\"\"Display analysis summary.\"\"\"\n        total_processors = sum(r['processor_count'] for r in self.all_results if 'error' not in r)\n        successful_flows = sum(1 for r in self.all_results if 'error' not in r)\n        failed_flows = sum(1 for r in self.all_results if 'error' in r)\n        \n        self.console.print(f\"\\n[cyan]Overall Summary:[/cyan]\")\n        self.console.print(f\"  Server: {self.server}\")\n        self.console.print(f\"  Total flows: {len(self.all_results)}\")\n        self.console.print(f\"  Successful: {successful_flows}\")\n        self.console.print(f\"  Failed: {failed_flows}\")\n        self.console.print(f\"  Total processors: {total_processors}\")\n        \n        # Create summary table\n        table = Table(title=\"\\nFlow Analysis Summary\")\n        table.add_column(\"Flow Name\", style=\"cyan\")\n        table.add_column(\"Processors\", justify=\"right\", style=\"yellow\")\n        table.add_column(\"Status\", style=\"green\")\n        \n        for result in self.all_results:\n            status = \"[red]Error[/red]\" if 'error' in result else \"[green]Success[/green]\"\n            table.add_row(\n                result['flow_name'],\n                str(result['processor_count']),\n                status\n            )\n        \n        self.console.print(table)\n    \n    def get_results_dataframe(self):\n        \"\"\"Convert all results to Spark DataFrame with SIMPLIFIED 11-field processor schema.\"\"\"\n        all_rows = []\n        \n        for flow_result in self.all_results:\n            if 'error' not in flow_result:\n                all_rows.extend(flow_result['processors'])\n        \n        if not all_rows:\n            return None\n        \n        # Helper function to safely convert to int\n        def safe_int(value, default=0):\n            \"\"\"Convert value to int, handling strings and None.\"\"\"\n            if value is None:\n                return default\n            try:\n                return int(value)\n            except (ValueError, TypeError):\n                return default\n        \n        # Convert to list of tuples (11 fields)\n        rows = [\n            (\n                row['snapshot_timestamp'],\n                row['server'],\n                row['flow_name'],\n                row['process_group_id'],\n                row['processor_id'],\n                row['processor_name'],\n                row['processor_type'],\n                safe_int(row['flow_files_out']),\n                safe_int(row['bytes_out']),\n                safe_int(row['total_queued_count']),\n                safe_int(row['max_percent_use_count'])\n            )\n            for row in all_rows\n        ]\n        \n        # Define SIMPLIFIED schema (11 fields - processor level)\n        schema = StructType([\n            # Metadata (4 fields)\n            StructField(\"snapshot_timestamp\", TimestampType(), False),\n            StructField(\"server\", StringType(), False),\n            StructField(\"flow_name\", StringType(), False),\n            StructField(\"process_group_id\", StringType(), False),\n            \n            # Processor identity (3 fields)\n            StructField(\"processor_id\", StringType(), True),\n            StructField(\"processor_name\", StringType(), False),\n            StructField(\"processor_type\", StringType(), True),\n            \n            # Aggregated metrics (4 fields)\n            StructField(\"flow_files_out\", LongType(), False),\n            StructField(\"bytes_out\", LongType(), False),\n            StructField(\"total_queued_count\", LongType(), False),\n            StructField(\"max_percent_use_count\", LongType(), False)\n        ])\n        \n        spark = SparkSession.builder.getOrCreate()\n        return spark.createDataFrame(rows, schema)\n\nconsole.print(\"[green]✓ MultiFlowAnalyzer class defined (simplified processor-level)![/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b0410c-171e-47c4-af2b-23e37969c8bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Starting Multi-Flow NiFi Analysis...</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mStarting Multi-Flow NiFi Analysis\u001b[0m\u001b[36m...\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Connecting to NiFi...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mConnecting to NiFi\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:39,532 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:40,166 - nifi_analyzer - INFO - Successfully authenticated with token\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">OK</span> Connected successfully\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mOK\u001b[0m Connected successfully\n",
       "\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Multi-Flow Analysis Starting...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mMulti-Flow Analysis Starting\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Server: prod\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Server: prod\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Timestamp: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:40</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Timestamp: \u001b[1;36m2026\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m09\u001b[0m \u001b[1;92m18:59:40\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:40,530 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:40,854 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Found </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">14</span><span style=\"color: #008000; text-decoration-color: #008000\"> flows to analyze</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mFound \u001b[0m\u001b[1;32m14\u001b[0m\u001b[32m flows to analyze\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6ed65758ce4e52b5413d3b3b1fed5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> STDF Unit Probe Ingest: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m STDF Unit Probe Ingest: \u001b[1;36m3\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> File Ingest: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m File Ingest: \u001b[1;36m4\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Final Test TX SAF - Spark3: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Final Test TX SAF - Spark3: \u001b[1;36m1\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Final Test Bin SAF and STDF - Spark3: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Final Test Bin SAF and STDF - Spark3: \u001b[1;36m2\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> FDC Data Edge To Hadoop/Kafka: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m FDC Data Edge To Hadoop/Kafka: \u001b[1;36m10\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:41,529 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> STDF Burn-in: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m STDF Burn-in: \u001b[1;36m18\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Final Test TX STDF - Spark3: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Final Test TX STDF - Spark3: \u001b[1;36m46\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> File Availability Metrics: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m File Availability Metrics: \u001b[1;36m2\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> ICN8 Track-out time based loading: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m ICN8 Track-out time based loading: \u001b[1;36m46\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> ICN8 BRS Feedback: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m ICN8 BRS Feedback: \u001b[1;36m40\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Saf Unit Probe: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Saf Unit Probe: \u001b[1;36m3\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> FDC Prod Data Processing: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m FDC Prod Data Processing: \u001b[1;36m7\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Master Tables: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Master Tables: \u001b[1;36m4\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #008000; text-decoration-color: #008000\">✓</span> Database Ingest: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> connections\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[32m✓\u001b[0m Database Ingest: \u001b[1;36m2\u001b[0m connections\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Overall Summary:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mOverall Summary:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Server: prod\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Server: prod\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Total flows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Total flows: \u001b[1;36m14\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Successful: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Successful: \u001b[1;36m14\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Failed: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Failed: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Total connections: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">188</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Total connections: \u001b[1;36m188\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                                </span>\n",
       "<span style=\"font-style: italic\">                     Flow Analysis Summary                      </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Flow Name                            </span>┃<span style=\"font-weight: bold\"> Connections </span>┃<span style=\"font-weight: bold\"> Status  </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> STDF Unit Probe Ingest               </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           3 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> File Ingest                          </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           4 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Final Test TX SAF - Spark3           </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           1 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Final Test Bin SAF and STDF - Spark3 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           2 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> FDC Data Edge To Hadoop/Kafka        </span>│<span style=\"color: #808000; text-decoration-color: #808000\">          10 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> STDF Burn-in                         </span>│<span style=\"color: #808000; text-decoration-color: #808000\">          18 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Final Test TX STDF - Spark3          </span>│<span style=\"color: #808000; text-decoration-color: #808000\">          46 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> File Availability Metrics            </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           2 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ICN8 Track-out time based loading    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">          46 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ICN8 BRS Feedback                    </span>│<span style=\"color: #808000; text-decoration-color: #808000\">          40 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Saf Unit Probe                       </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           3 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> FDC Prod Data Processing             </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           7 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Master Tables                        </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           4 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Database Ingest                      </span>│<span style=\"color: #808000; text-decoration-color: #808000\">           2 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Success </span>│\n",
       "└──────────────────────────────────────┴─────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                                \u001b[0m\n",
       "\u001b[3m                     Flow Analysis Summary                      \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mFlow Name                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnections\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStatus \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mSTDF Unit Probe Ingest              \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          3\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFile Ingest                         \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          4\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFinal Test TX SAF - Spark3          \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          1\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFinal Test Bin SAF and STDF - Spark3\u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          2\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFDC Data Edge To Hadoop/Kafka       \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m         10\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mSTDF Burn-in                        \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m         18\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFinal Test TX STDF - Spark3         \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m         46\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFile Availability Metrics           \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          2\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mICN8 Track-out time based loading   \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m         46\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mICN8 BRS Feedback                   \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m         40\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mSaf Unit Probe                      \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          3\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mFDC Prod Data Processing            \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          7\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mMaster Tables                       \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          4\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDatabase Ingest                     \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m          2\u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mSuccess\u001b[0m\u001b[32m \u001b[0m│\n",
       "└──────────────────────────────────────┴─────────────┴─────────┘\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ Multi-flow analysis complete!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[32m✓ Multi-flow analysis complete!\u001b[0m\n"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Cell 6: Run Multi-Flow Analysis\n",
    "\n",
    "console.print(\"\\n[cyan]Starting Multi-Flow NiFi Analysis...[/cyan]\\n\")\n",
    "\n",
    "# Connect to NiFi\n",
    "console.print(\"[yellow]Connecting to NiFi...[/yellow]\")\n",
    "client = NiFiClient(\n",
    "    base_url=CONFIG['nifi_url'],\n",
    "    username=CONFIG['username'],\n",
    "    password=CONFIG['password'],\n",
    "    verify_ssl=CONFIG['verify_ssl']\n",
    ")\n",
    "console.print(\"[green]OK[/green] Connected successfully\\n\")\n",
    "\n",
    "# Create analyzer and run analysis\n",
    "analyzer = MultiFlowAnalyzer(client=client, server=CONFIG['server'])\n",
    "analyzer.analyze_all_flows(CONFIG['flows_csv_path'])\n",
    "\n",
    "# Cleanup\n",
    "client.close()\n",
    "\n",
    "console.print(\"\\n[green]✓ Multi-flow analysis complete![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb83d48-0627-44f1-af76-bcdc0fcc0d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Saving snapshots to Delta Lake...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mSaving snapshots to Delta Lake\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:42,532 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Table exists, appending data to: 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mTable exists, appending data to: 1dp_mfg_sbx.validation_test_eric.nifi_processor_snapshots_full_attributes\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:43,530 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">OK</span> Data appended successfully\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mOK\u001b[0m Data appended successfully\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Timestamp: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:59:40</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">170613</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Timestamp: \u001b[1;36m2026\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m09\u001b[0m \u001b[1;92m18:59:40\u001b[0m.\u001b[1;36m170613\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:44,529 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Total rows written: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">188</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Total rows written: \u001b[1;36m188\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Sample data:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mSample data:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>snapshot_timestamp</th><th>server</th><th>flow_name</th><th>process_group_id</th><th>connection_id</th><th>connection_name</th><th>connection_group_id</th><th>source_id</th><th>source_name</th><th>destination_id</th><th>destination_name</th><th>flow_files_in</th><th>flow_files_out</th><th>bytes_in</th><th>bytes_out</th><th>input</th><th>output</th><th>queued_count</th><th>queued_bytes</th><th>queued</th><th>queued_size</th><th>percent_use_count</th><th>percent_use_bytes</th><th>stats_last_refreshed</th></tr></thead><tbody><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>STDF Unit Probe Ingest</td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>ea471957-dc64-3b05-a2ff-8e292e8bac9f</td><td></td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>null</td><td>From Ingest</td><td>null</td><td>input</td><td>4</td><td>4</td><td>83428</td><td>83428</td><td>4 (81.47 KB)</td><td>4 (81.47 KB)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>STDF Unit Probe Ingest</td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>ddde3800-a05a-1cb2-0000-00006a6159be</td><td></td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>null</td><td>to unit probe</td><td>null</td><td>from stdf ingest</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>STDF Unit Probe Ingest</td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>f198cfc4-f49a-3b94-98e2-071b04b9db2e</td><td></td><td>083296f7-e33e-3eaa-acbf-d3c3e42ac320</td><td>null</td><td>to updates</td><td>null</td><td>To Unit Probe</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>File Ingest</td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>1c79317b-0193-1000-0000-000047ed80c8</td><td></td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>null</td><td>to cp_tx Kudu</td><td>null</td><td>input_kudu</td><td>7</td><td>7</td><td>3453369</td><td>3453369</td><td>7 (3.29 MB)</td><td>7 (3.29 MB)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>File Ingest</td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>0e352fbe-0191-1000-0000-000007369a75</td><td></td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>null</td><td>to bin results (mdlp)</td><td>null</td><td>to bin results (mdlp)</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>File Ingest</td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>0e2da00a-0191-1000-0000-000043641421</td><td></td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>null</td><td>to EDL Wat</td><td>null</td><td>to EDL WAT</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>File Ingest</td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>db9d36d9-a68e-16ac-0000-000008ffd444</td><td></td><td>099bad4c-0191-1000-0000-000045e3c1a8</td><td>null</td><td>to CP TX</td><td>null</td><td>input</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>Final Test TX SAF - Spark3</td><td>7e7f1e93-c20b-3ab5-8791-9537827a965c</td><td>7a097d05-7f37-3022-87f6-92368ca58867</td><td></td><td>7e7f1e93-c20b-3ab5-8791-9537827a965c</td><td>null</td><td>routeToFTBinIngest</td><td>null</td><td>routeToFTBinIngest</td><td>2</td><td>2</td><td>48</td><td>48</td><td>2 (48 bytes)</td><td>2 (48 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>Final Test Bin SAF and STDF - Spark3</td><td>1b424d1e-3258-3283-b5bb-76a725bf1b11</td><td>da0b9bf4-d72d-3363-9a4f-9d57e1517262</td><td></td><td>1b424d1e-3258-3283-b5bb-76a725bf1b11</td><td>null</td><td>ftBinIngest</td><td>null</td><td>ftBinIngest</td><td>2</td><td>2</td><td>48</td><td>48</td><td>2 (48 bytes)</td><td>2 (48 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr><tr><td>2026-01-09T18:59:40.170613Z</td><td>prod</td><td>Final Test Bin SAF and STDF - Spark3</td><td>1b424d1e-3258-3283-b5bb-76a725bf1b11</td><td>5cbf1863-0aa0-3a31-8ba2-15fdfc872e6b</td><td></td><td>1b424d1e-3258-3283-b5bb-76a725bf1b11</td><td>null</td><td>toFTDataProduct</td><td>null</td><td>toFTDataProduct</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 (0 bytes)</td><td>0</td><td>0</td><td>0 (0 bytes)</td><td>0 bytes</td><td>0</td><td>0</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "STDF Unit Probe Ingest",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         "ea471957-dc64-3b05-a2ff-8e292e8bac9f",
         "",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         null,
         "From Ingest",
         null,
         "input",
         4,
         4,
         83428,
         83428,
         "4 (81.47 KB)",
         "4 (81.47 KB)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "STDF Unit Probe Ingest",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         "ddde3800-a05a-1cb2-0000-00006a6159be",
         "",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         null,
         "to unit probe",
         null,
         "from stdf ingest",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "STDF Unit Probe Ingest",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         "f198cfc4-f49a-3b94-98e2-071b04b9db2e",
         "",
         "083296f7-e33e-3eaa-acbf-d3c3e42ac320",
         null,
         "to updates",
         null,
         "To Unit Probe",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "File Ingest",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         "1c79317b-0193-1000-0000-000047ed80c8",
         "",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         null,
         "to cp_tx Kudu",
         null,
         "input_kudu",
         7,
         7,
         3453369,
         3453369,
         "7 (3.29 MB)",
         "7 (3.29 MB)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "File Ingest",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         "0e352fbe-0191-1000-0000-000007369a75",
         "",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         null,
         "to bin results (mdlp)",
         null,
         "to bin results (mdlp)",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "File Ingest",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         "0e2da00a-0191-1000-0000-000043641421",
         "",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         null,
         "to EDL Wat",
         null,
         "to EDL WAT",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "File Ingest",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         "db9d36d9-a68e-16ac-0000-000008ffd444",
         "",
         "099bad4c-0191-1000-0000-000045e3c1a8",
         null,
         "to CP TX",
         null,
         "input",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "Final Test TX SAF - Spark3",
         "7e7f1e93-c20b-3ab5-8791-9537827a965c",
         "7a097d05-7f37-3022-87f6-92368ca58867",
         "",
         "7e7f1e93-c20b-3ab5-8791-9537827a965c",
         null,
         "routeToFTBinIngest",
         null,
         "routeToFTBinIngest",
         2,
         2,
         48,
         48,
         "2 (48 bytes)",
         "2 (48 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "Final Test Bin SAF and STDF - Spark3",
         "1b424d1e-3258-3283-b5bb-76a725bf1b11",
         "da0b9bf4-d72d-3363-9a4f-9d57e1517262",
         "",
         "1b424d1e-3258-3283-b5bb-76a725bf1b11",
         null,
         "ftBinIngest",
         null,
         "ftBinIngest",
         2,
         2,
         48,
         48,
         "2 (48 bytes)",
         "2 (48 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ],
        [
         "2026-01-09T18:59:40.170613Z",
         "prod",
         "Final Test Bin SAF and STDF - Spark3",
         "1b424d1e-3258-3283-b5bb-76a725bf1b11",
         "5cbf1863-0aa0-3a31-8ba2-15fdfc872e6b",
         "",
         "1b424d1e-3258-3283-b5bb-76a725bf1b11",
         null,
         "toFTDataProduct",
         null,
         "toFTDataProduct",
         0,
         0,
         0,
         0,
         "0 (0 bytes)",
         "0 (0 bytes)",
         0,
         0,
         "0 (0 bytes)",
         "0 bytes",
         0,
         0,
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "snapshot_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "server",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "flow_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "process_group_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "connection_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "connection_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "connection_group_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "destination_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "destination_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "flow_files_in",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "flow_files_out",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bytes_in",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bytes_out",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "input",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "output",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "queued_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "queued_bytes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "queued",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "queued_size",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "percent_use_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "percent_use_bytes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "stats_last_refreshed",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     }
    }
   ],
   "source": [
    "# Cell 7: Save Snapshots to Delta Lake\n",
    "\n",
    "if CONFIG['enable_snapshots']:\n",
    "    console.print(\"\\n[yellow]Saving snapshots to Delta Lake...[/yellow]\")\n",
    "    \n",
    "    df = analyzer.get_results_dataframe()\n",
    "    \n",
    "    if df is not None:\n",
    "        table_name = CONFIG['delta_table_path']\n",
    "        \n",
    "        # Check if table exists\n",
    "        table_exists = spark.catalog._jcatalog.tableExists(table_name)\n",
    "        \n",
    "        if not table_exists:\n",
    "            # First run: Create table\n",
    "            console.print(f\"[yellow]Table doesn't exist, creating: {table_name}[/yellow]\")\n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(table_name)\n",
    "            console.print(f\"[green]OK[/green] Table created successfully with 24-field connection-level schema\")\n",
    "        else:\n",
    "            # Subsequent runs: Append data\n",
    "            console.print(f\"[yellow]Table exists, appending data to: {table_name}[/yellow]\")\n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .saveAsTable(table_name)\n",
    "            console.print(f\"[green]OK[/green] Data appended successfully\")\n",
    "        \n",
    "        console.print(f\"  Timestamp: {analyzer.snapshot_timestamp}\")\n",
    "        console.print(f\"  Total rows written: {df.count()}\")\n",
    "        \n",
    "        # Show sample\n",
    "        console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n",
    "        display(df.limit(10))\n",
    "    else:\n",
    "        console.print(\"[red]ERROR[/red] No data to save\")\n",
    "else:\n",
    "    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")\n",
    "\n",
    "# NOTE: If you need to manually drop the table to start fresh, run this in a separate cell:\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CONFIG['delta_table_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4708d0b-7e63-4d7c-94d2-ec64adb24c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">Querying connection-level snapshots...</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mQuerying connection-level snapshots\u001b[0m\u001b[36m...\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Snapshot count by server and flow:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mSnapshot count by server and flow:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:45,529 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:46,119 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------------------------------------+---------+-----------------+--------------------------+\n|server  |flow_name                           |snapshots|total_connections|last_snapshot             |\n+--------+------------------------------------+---------+-----------------+--------------------------+\n|prod    |Database Ingest                     |5        |10               |2026-01-09 18:59:40.170613|\n|prod    |FDC Data Edge To Hadoop/Kafka       |5        |50               |2026-01-09 18:59:40.170613|\n|prod    |FDC Prod Data Processing            |5        |35               |2026-01-09 18:59:40.170613|\n|prod    |File Availability Metrics           |5        |10               |2026-01-09 18:59:40.170613|\n|prod    |File Ingest                         |5        |20               |2026-01-09 18:59:40.170613|\n|prod    |Final Test Bin SAF and STDF - Spark3|5        |10               |2026-01-09 18:59:40.170613|\n|prod    |Final Test TX SAF - Spark3          |5        |5                |2026-01-09 18:59:40.170613|\n|prod    |Final Test TX STDF - Spark3         |5        |230              |2026-01-09 18:59:40.170613|\n|prod    |ICN8 BRS Feedback                   |5        |200              |2026-01-09 18:59:40.170613|\n|prod    |ICN8 Track-out time based loading   |5        |230              |2026-01-09 18:59:40.170613|\n|prod    |Master Tables                       |5        |20               |2026-01-09 18:59:40.170613|\n|prod    |STDF Burn-in                        |5        |90               |2026-01-09 18:59:40.170613|\n|prod    |STDF Unit Probe Ingest              |5        |15               |2026-01-09 18:59:40.170613|\n|prod    |Saf Unit Probe                      |5        |15               |2026-01-09 18:59:40.170613|\n|thailand|ACCT Dicer                          |5        |300              |2026-01-09 18:59:09.14744 |\n|thailand|Archive GTP landing zone            |5        |170              |2026-01-09 18:59:09.14744 |\n|thailand|Chipsort                            |5        |120              |2026-01-09 18:59:09.14744 |\n|thailand|DIDT                                |5        |60               |2026-01-09 18:59:09.14744 |\n|thailand|Dicer DC                            |5        |135              |2026-01-09 18:59:09.14744 |\n|thailand|Dicer DC Curated                    |5        |70               |2026-01-09 18:59:09.14744 |\n+--------+------------------------------------+---------+-----------------+--------------------------+\nonly showing top 20 rows\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Connections with queued flowfiles </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">backpressure</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mConnections with queued flowfiles \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mbackpressure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:46,530 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:46,662 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+---------------------------------+---------------------------------+-------------------------------+--------------------+----------------+----------------+\n|server|flow_name                        |source_name                      |destination_name               |max_queued_flowfiles|max_queued_bytes|max_percent_full|\n+------+---------------------------------+---------------------------------+-------------------------------+--------------------+----------------+----------------+\n|prod  |Final Test TX STDF - Spark3      |Try with more partitions and time|Catch errors                   |200                 |0               |1               |\n|prod  |Final Test TX STDF - Spark3      |Add new partition AWS Redshift   |Catch errors                   |70                  |0               |0               |\n|prod  |Final Test TX STDF - Spark3      |Split even/odd days              |Ingest stdf.stdf_ft_dp - Spark3|67                  |0               |1               |\n|prod  |ICN8 Track-out time based loading|Move file to \"processing\" folder |Dummy                          |23                  |0               |0               |\n|prod  |ICN8 BRS Feedback                |Funnel                           |Dummy                          |7                   |0               |0               |\n+------+---------------------------------+---------------------------------+-------------------------------+--------------------+----------------+----------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Connections approaching queue limits </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">&gt;</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">50</span><span style=\"color: #808000; text-decoration-color: #808000\">% full</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mConnections approaching queue limits \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33m>\u001b[0m\u001b[1;33m50\u001b[0m\u001b[33m% full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+---------+-----------+----------------+----------------+----------------+\n|server|flow_name|source_name|destination_name|max_percent_full|max_queued_count|\n+------+---------+-----------+----------------+----------------+----------------+\n+------+---------+-----------+----------------+----------------+----------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Inactive connections </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">no flowfiles for </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">7</span><span style=\"color: #808000; text-decoration-color: #808000\"> days</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mInactive connections \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mno flowfiles for \u001b[0m\u001b[1;33m7\u001b[0m\u001b[33m days\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:47,529 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:47,728 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+------------------------------------+------------------------------+------------------------------+---------------+-------------+\n|server|flow_name                           |source_name                   |destination_name              |delta_flowfiles|num_snapshots|\n+------+------------------------------------+------------------------------+------------------------------+---------------+-------------+\n|prod  |Database Ingest                     |from related tables trigger   |from tx trigger               |0              |5            |\n|prod  |Database Ingest                     |sleuth_ingest out             |sleuth_ingest                 |0              |5            |\n|prod  |FDC Data Edge To Hadoop/Kafka       |ATKH Bridge To Kafka          |ATKH File Receiver            |0              |5            |\n|prod  |FDC Data Edge To Hadoop/Kafka       |ATKH Kafka Output             |ATKH Bridge To Kafka          |0              |5            |\n|prod  |FDC Prod Data Processing            |Email Xinyu                   |Done Email                    |0              |5            |\n|prod  |FDC Prod Data Processing            |Email Xinyu                   |Failed, Resend in 30 min      |0              |5            |\n|prod  |FDC Prod Data Processing            |Failed, Resend in 30 min      |Email Xinyu                   |0              |5            |\n|prod  |FDC Prod Data Processing            |Out For Email                 |Email Xinyu                   |0              |5            |\n|prod  |FDC Prod Data Processing            |Out For Email Notification    |Email Xinyu                   |0              |5            |\n|prod  |FDC Prod Data Processing            |Out For Error Notification    |Email Xinyu                   |0              |5            |\n|prod  |File Availability Metrics           |to_efars_load                 |to_efars_load                 |0              |5            |\n|prod  |File Availability Metrics           |to_hdfs_load                  |to_hdfs_load                  |0              |5            |\n|prod  |File Ingest                         |to CP TX                      |input                         |0              |5            |\n|prod  |File Ingest                         |to EDL Wat                    |to EDL WAT                    |0              |5            |\n|prod  |File Ingest                         |to bin results (mdlp)         |to bin results (mdlp)         |0              |5            |\n|prod  |Final Test Bin SAF and STDF - Spark3|toFTDataProduct               |toFTDataProduct               |0              |5            |\n|prod  |Final Test TX STDF - Spark3         |Add new partition AWS Redshift|Catch errors                  |0              |5            |\n|prod  |Final Test TX STDF - Spark3         |Add new partition and refresh |Try after 30 mins             |0              |5            |\n|prod  |Final Test TX STDF - Spark3         |Add new partition and refresh |Add new partition AWS Redshift|0              |5            |\n|prod  |Final Test TX STDF - Spark3         |Funnel                        |Set partition                 |0              |5            |\n+------+------------------------------------+------------------------------+------------------------------+---------------+-------------+\nonly showing top 20 rows\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Inactive processors by flow </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">aggregated from connections</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mInactive processors by flow \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33maggregated from connections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:48,373 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------------------------------------+------------------------+\n|server  |flow_name                           |inactive_processor_count|\n+--------+------------------------------------+------------------------+\n|prod    |ICN8 BRS Feedback                   |28                      |\n|prod    |STDF Burn-in                        |15                      |\n|prod    |Final Test TX STDF - Spark3         |12                      |\n|prod    |ICN8 Track-out time based loading   |10                      |\n|prod    |FDC Prod Data Processing            |5                       |\n|prod    |Master Tables                       |4                       |\n|prod    |File Ingest                         |3                       |\n|prod    |Saf Unit Probe                      |2                       |\n|prod    |Database Ingest                     |2                       |\n|prod    |File Availability Metrics           |2                       |\n|prod    |FDC Data Edge To Hadoop/Kafka       |2                       |\n|prod    |STDF Unit Probe Ingest              |2                       |\n|prod    |Final Test Bin SAF and STDF - Spark3|1                       |\n|thailand|ACCT Dicer                          |23                      |\n|thailand|RPM v 2.0                           |22                      |\n|thailand|RPM v 2.1                           |21                      |\n|thailand|Chipsort                            |18                      |\n|thailand|Dicer Log v3                        |18                      |\n|thailand|WBAOI - POC                         |18                      |\n|thailand|Molding - POC                       |17                      |\n+--------+------------------------------------+------------------------+\nonly showing top 20 rows\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Queue depth trends </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">hourly averages</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mQueue depth trends \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mhourly averages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:48,530 - py4j.clientserver - INFO - Received command c on object id p0\n2026-01-09 18:59:48,898 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+------+---------------------------------+---------------------------------+-------------------------------+--------------------+--------------------+\n|hour               |server|flow_name                        |source_name                      |destination_name               |avg_queued_flowfiles|max_queued_flowfiles|\n+-------------------+------+---------------------------------+---------------------------------+-------------------------------+--------------------+--------------------+\n|2026-01-09 18:00:00|prod  |Final Test TX STDF - Spark3      |Try with more partitions and time|Catch errors                   |192.5               |200                 |\n|2026-01-09 18:00:00|prod  |Final Test TX STDF - Spark3      |Add new partition AWS Redshift   |Catch errors                   |70.0                |70                  |\n|2026-01-09 18:00:00|prod  |Final Test TX STDF - Spark3      |Split even/odd days              |Ingest stdf.stdf_ft_dp - Spark3|51.1                |67                  |\n|2026-01-09 18:00:00|prod  |ICN8 Track-out time based loading|Move file to \"processing\" folder |Dummy                          |23.0                |23                  |\n|2026-01-09 18:00:00|prod  |ICN8 BRS Feedback                |Funnel                           |Dummy                          |7.0                 |7                   |\n+-------------------+------+---------------------------------+---------------------------------+-------------------------------+--------------------+--------------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Flow balance </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input vs output by connection</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[33mFlow balance \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33minput vs output by connection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m:\u001b[0m\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:49,388 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+---------+-----------+----------------+------------------+-------------------+----------+\n|server|flow_name|source_name|destination_name|total_flowfiles_in|total_flowfiles_out|net_change|\n+------+---------+-----------+----------------+------------------+-------------------+----------+\n+------+---------+-----------+----------------+------------------+-------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Query Historical Snapshots (Connection-Level Analysis)\n",
    "\n",
    "if CONFIG['enable_snapshots']:\n",
    "    console.print(\"\\n[cyan]Querying connection-level snapshots...[/cyan]\\n\")\n",
    "    \n",
    "    table_name = CONFIG['delta_table_path']\n",
    "    \n",
    "    try:\n",
    "        # Show snapshots per flow and server\n",
    "        console.print(\"[yellow]Snapshot count by server and flow:[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                COUNT(DISTINCT snapshot_timestamp) as snapshots,\n",
    "                COUNT(*) as total_connections,\n",
    "                MAX(snapshot_timestamp) as last_snapshot\n",
    "            FROM {table_name}\n",
    "            GROUP BY server, flow_name\n",
    "            ORDER BY server, flow_name\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # NEW: Find connections with high queue depth (backpressure detection)\n",
    "        console.print(\"\\n[yellow]Connections with queued flowfiles (backpressure):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                source_name,\n",
    "                destination_name,\n",
    "                MAX(queued_count) as max_queued_flowfiles,\n",
    "                MAX(queued_bytes) as max_queued_bytes,\n",
    "                MAX(percent_use_count) as max_percent_full\n",
    "            FROM {table_name}\n",
    "            WHERE queued_count > 0\n",
    "            GROUP BY server, flow_name, source_name, destination_name\n",
    "            ORDER BY max_queued_flowfiles DESC\n",
    "            LIMIT 20\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # NEW: Identify connections approaching queue limits\n",
    "        console.print(\"\\n[yellow]Connections approaching queue limits (>50% full):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                source_name,\n",
    "                destination_name,\n",
    "                MAX(percent_use_count) as max_percent_full,\n",
    "                MAX(queued_count) as max_queued_count\n",
    "            FROM {table_name}\n",
    "            WHERE percent_use_count > 50\n",
    "            GROUP BY server, flow_name, source_name, destination_name\n",
    "            ORDER BY max_percent_full DESC\n",
    "            LIMIT 20\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # Find inactive connections (no flow for 7 days)\n",
    "        console.print(\"\\n[yellow]Inactive connections (no flowfiles for 7 days):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            WITH connection_activity AS (\n",
    "                SELECT \n",
    "                    server,\n",
    "                    flow_name,\n",
    "                    source_name,\n",
    "                    destination_name,\n",
    "                    MAX(flow_files_out) - MIN(flow_files_out) as delta_flowfiles,\n",
    "                    MIN(snapshot_timestamp) as first_snapshot,\n",
    "                    MAX(snapshot_timestamp) as last_snapshot,\n",
    "                    COUNT(DISTINCT snapshot_timestamp) as num_snapshots\n",
    "                FROM {table_name}\n",
    "                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "                GROUP BY server, flow_name, source_name, destination_name\n",
    "            )\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                source_name,\n",
    "                destination_name,\n",
    "                delta_flowfiles,\n",
    "                num_snapshots\n",
    "            FROM connection_activity\n",
    "            WHERE delta_flowfiles = 0\n",
    "            ORDER BY server, flow_name, source_name\n",
    "            LIMIT 50\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # Aggregate to processor level (still possible!)\n",
    "        console.print(\"\\n[yellow]Inactive processors by flow (aggregated from connections):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            WITH processor_activity AS (\n",
    "                SELECT \n",
    "                    server,\n",
    "                    flow_name,\n",
    "                    source_name as processor_name,\n",
    "                    MAX(flow_files_out) - MIN(flow_files_out) as delta_flowfiles\n",
    "                FROM {table_name}\n",
    "                WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "                GROUP BY server, flow_name, source_name\n",
    "            )\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                COUNT(*) as inactive_processor_count\n",
    "            FROM processor_activity\n",
    "            WHERE delta_flowfiles = 0\n",
    "            GROUP BY server, flow_name\n",
    "            ORDER BY server, inactive_processor_count DESC\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # NEW: Track queue growth over time\n",
    "        console.print(\"\\n[yellow]Queue depth trends (hourly averages):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                DATE_TRUNC('hour', snapshot_timestamp) as hour,\n",
    "                server,\n",
    "                flow_name,\n",
    "                source_name,\n",
    "                destination_name,\n",
    "                AVG(queued_count) as avg_queued_flowfiles,\n",
    "                MAX(queued_count) as max_queued_flowfiles\n",
    "            FROM {table_name}\n",
    "            WHERE snapshot_timestamp >= current_date() - INTERVAL 1 DAYS\n",
    "              AND queued_count > 0\n",
    "            GROUP BY hour, server, flow_name, source_name, destination_name\n",
    "            ORDER BY hour DESC, avg_queued_flowfiles DESC\n",
    "            LIMIT 20\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # NEW: Bidirectional flow analysis\n",
    "        console.print(\"\\n[yellow]Flow balance (input vs output by connection):[/yellow]\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                server,\n",
    "                flow_name,\n",
    "                source_name,\n",
    "                destination_name,\n",
    "                SUM(flow_files_in) as total_flowfiles_in,\n",
    "                SUM(flow_files_out) as total_flowfiles_out,\n",
    "                SUM(flow_files_in) - SUM(flow_files_out) as net_change\n",
    "            FROM {table_name}\n",
    "            WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "            GROUP BY server, flow_name, source_name, destination_name\n",
    "            HAVING ABS(SUM(flow_files_in) - SUM(flow_files_out)) > 100\n",
    "            ORDER BY ABS(net_change) DESC\n",
    "            LIMIT 20\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]ERROR[/red] Failed to query: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    console.print(\"\\n[yellow]Snapshots disabled[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baffe3ae-fe19-4192-b52d-ce7cc9760e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2026-01-09 18:59:49,558 - py4j.clientserver - INFO - Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# # Cell 9: Export Results to CSV by Flow\n",
    "\n",
    "# console.print(\"\\n[yellow]Exporting results to CSV...[/yellow]\")\n",
    "\n",
    "# timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# df = analyzer.get_results_dataframe()\n",
    "# if df is not None:\n",
    "#     pdf = df.toPandas()\n",
    "    \n",
    "#     # Export overall summary\n",
    "#     output_path = f\"/dbfs/nifi_analysis/all_flows_{timestamp_str}.csv\"\n",
    "#     pdf.to_csv(output_path, index=False)\n",
    "#     console.print(f\"[green]OK[/green] All flows exported to {output_path}\")\n",
    "    \n",
    "#     # Export per flow\n",
    "#     for flow_name in pdf['flow_name'].unique():\n",
    "#         flow_df = pdf[pdf['flow_name'] == flow_name]\n",
    "#         flow_path = f\"/dbfs/nifi_analysis/{flow_name}_{timestamp_str}.csv\"\n",
    "#         flow_df.to_csv(flow_path, index=False)\n",
    "#         console.print(f\"  [green]✓[/green] {flow_name}: {len(flow_df)} processors\")\n",
    "    \n",
    "#     console.print(f\"\\n[cyan]Sample data:[/cyan]\")\n",
    "#     display(pdf.head(10))\n",
    "# else:\n",
    "#     console.print(\"[red]ERROR[/red] No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4eb366b-247a-44ba-a9fa-fb29a19b9c73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Updated Delta Table Schema (Connection-Level)\n",
    "\n",
    "The Delta table now captures **ALL available fields** from NiFi Status API at the **connection level** (not processor level). This provides maximum granularity for analysis.\n",
    "\n",
    "### 24 Total Fields\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| **Metadata (4 fields)** | | |\n",
    "| `snapshot_timestamp` | Timestamp | When the snapshot was captured |\n",
    "| `server` | String | Server identifier (hostname, environment name) |\n",
    "| `flow_name` | String | Flow name from CSV |\n",
    "| `process_group_id` | String | NiFi process group ID |\n",
    "| **Connection Identity (3 fields)** | | |\n",
    "| `connection_id` | String | Connection UUID |\n",
    "| `connection_name` | String | Connection name (often empty or \"success\") |\n",
    "| `connection_group_id` | String | Parent process group ID |\n",
    "| **Source Processor (2 fields)** | | |\n",
    "| `source_id` | String | Source processor UUID |\n",
    "| `source_name` | String | Source processor name |\n",
    "| **Destination Processor (2 fields)** | | |\n",
    "| `destination_id` | String | Destination processor UUID |\n",
    "| `destination_name` | String | Destination processor name |\n",
    "| **Flow Metrics - 5-minute window (6 fields)** | | |\n",
    "| `flow_files_in` | Long | FlowFiles entering connection |\n",
    "| `flow_files_out` | Long | FlowFiles leaving connection |\n",
    "| `bytes_in` | Long | Bytes entering connection |\n",
    "| `bytes_out` | Long | Bytes leaving connection |\n",
    "| `input` | String | Formatted input stats (e.g., \"1,250 (50.8 KB)\") |\n",
    "| `output` | String | Formatted output stats |\n",
    "| **Queue Metrics - current state (4 fields)** | | |\n",
    "| `queued_count` | Long | FlowFiles currently queued |\n",
    "| `queued_bytes` | Long | Bytes currently queued |\n",
    "| `queued` | String | Formatted queue stats |\n",
    "| `queued_size` | String | Formatted queue size |\n",
    "| **Status Indicators (2 fields)** | | |\n",
    "| `percent_use_count` | Long | % of queue count threshold used |\n",
    "| `percent_use_bytes` | Long | % of queue bytes threshold used |\n",
    "| **Timestamps (1 field)** | | |\n",
    "| `stats_last_refreshed` | String | When stats were last updated |\n",
    "\n",
    "## Key Differences from Previous Version\n",
    "\n",
    "**Before:** 9 fields, processor-level aggregation\n",
    "- Only captured: flowFilesOut, bytesOut\n",
    "- Aggregated connections by source processor\n",
    "- Could not identify specific bottleneck connections\n",
    "- No queue monitoring capability\n",
    "\n",
    "**Now:** 24 fields, connection-level granularity\n",
    "- Captures ALL 15+ fields from NiFi Status API\n",
    "- Stores each connection separately\n",
    "- Can identify exact bottleneck points\n",
    "- Enables queue monitoring, backpressure detection, flow lineage\n",
    "\n",
    "**Impact:** ~2x more rows (typical NiFi flow has 1-2 connections per processor), but unlocks powerful new analysis capabilities.\n",
    "\n",
    "## New Analysis Capabilities\n",
    "\n",
    "### 1. Backpressure Detection\n",
    "Identify connections with high queue depth:\n",
    "```sql\n",
    "SELECT source_name, destination_name, MAX(queued_count) as max_queued\n",
    "FROM main.default.nifi_processor_snapshots\n",
    "WHERE queued_count > 100\n",
    "GROUP BY source_name, destination_name\n",
    "ORDER BY max_queued DESC;\n",
    "```\n",
    "\n",
    "### 2. Queue Limit Monitoring\n",
    "Find connections approaching capacity:\n",
    "```sql\n",
    "SELECT source_name, destination_name, MAX(percent_use_count) as max_percent_full\n",
    "FROM main.default.nifi_processor_snapshots\n",
    "WHERE percent_use_count > 80\n",
    "GROUP BY source_name, destination_name;\n",
    "```\n",
    "\n",
    "### 3. Bidirectional Flow Tracking\n",
    "Compare input vs output to find imbalances:\n",
    "```sql\n",
    "SELECT source_name,\n",
    "       SUM(flow_files_in) as total_in,\n",
    "       SUM(flow_files_out) as total_out,\n",
    "       SUM(flow_files_in) - SUM(flow_files_out) as net_change\n",
    "FROM main.default.nifi_processor_snapshots\n",
    "GROUP BY source_name\n",
    "HAVING ABS(net_change) > 100;\n",
    "```\n",
    "\n",
    "### 4. Queue Growth Trends\n",
    "Monitor queue depth over time:\n",
    "```sql\n",
    "SELECT DATE_TRUNC('hour', snapshot_timestamp) as hour,\n",
    "       source_name, destination_name,\n",
    "       AVG(queued_count) as avg_queued_flowfiles\n",
    "FROM main.default.nifi_processor_snapshots\n",
    "WHERE snapshot_timestamp >= current_date() - INTERVAL 1 DAYS\n",
    "GROUP BY hour, source_name, destination_name\n",
    "ORDER BY hour, avg_queued_flowfiles DESC;\n",
    "```\n",
    "\n",
    "### 5. Processor-Level Analysis (Still Possible!)\n",
    "Aggregate connections to processor-level when needed:\n",
    "```sql\n",
    "WITH processor_activity AS (\n",
    "    SELECT source_name,\n",
    "           MAX(flow_files_out) - MIN(flow_files_out) as delta\n",
    "    FROM main.default.nifi_processor_snapshots\n",
    "    WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "    GROUP BY source_name\n",
    ")\n",
    "SELECT * FROM processor_activity WHERE delta = 0;\n",
    "```\n",
    "\n",
    "### 6. Flow Path Lineage\n",
    "Track data movement through the flow:\n",
    "```sql\n",
    "SELECT source_name, destination_name, \n",
    "       SUM(flow_files_out) as total_flowfiles\n",
    "FROM main.default.nifi_processor_snapshots\n",
    "WHERE snapshot_timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "GROUP BY source_name, destination_name\n",
    "ORDER BY total_flowfiles DESC;\n",
    "```\n",
    "\n",
    "## Unity Catalog Configuration\n",
    "\n",
    "The notebook uses Unity Catalog with 3-level naming:\n",
    "- **Catalog**: `main` (default)\n",
    "- **Schema**: `default` (default)\n",
    "- **Table**: `nifi_processor_snapshots`\n",
    "- **Full path**: `main.default.nifi_processor_snapshots`\n",
    "\n",
    "You can customize this in Cell 3 by editing `delta_table_path`.\n",
    "\n",
    "## CSV Format\n",
    "\n",
    "Your `flows.csv` should look like:\n",
    "```\n",
    "id,flow_name\n",
    "8c8677c4-29d6-3607-a32e-1234567890ab,Production_Data_Pipeline\n",
    "abc-123-def-456-7890-abcdef123456,Development_Testing_Flow\n",
    "xyz-789-ghi-012-3456-7890abcdef12,QA_Validation_Flow\n",
    "```\n",
    "\n",
    "Upload it to: `/dbfs/nifi_analysis/flows.csv`\n",
    "\n",
    "## Server Identifier\n",
    "\n",
    "The `server` field helps track data from multiple NiFi instances:\n",
    "- Use hostname: `prod-nifi-01`, `dev-nifi-02`\n",
    "- Use environment: `prod`, `dev`, `qa`, `staging`\n",
    "- Use datacenter: `dc1-nifi`, `dc2-nifi`\n",
    "\n",
    "This allows you to:\n",
    "- Compare processor usage across environments\n",
    "- Track migration from one server to another\n",
    "- Aggregate metrics across multiple NiFi clusters\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Connection-Level Storage:**\n",
    "- Each connection is stored as a separate row\n",
    "- Preserves source → destination relationships\n",
    "- Enables fine-grained debugging and analysis\n",
    "- Can still aggregate to processor-level in queries\n",
    "\n",
    "**Snapshot-based Analysis:**\n",
    "- Each run captures a snapshot of flowfile counts at that moment\n",
    "- Take snapshots every 5 minutes over a week\n",
    "- Calculate deltas (MAX - MIN) to identify inactive connections\n",
    "- Connection with delta = 0 means no flowfiles processed in that time period\n",
    "\n",
    "**Why Connection-Level Instead of Processor-Level?**\n",
    "- Identify which specific connection is bottlenecked\n",
    "- Monitor queue depth per connection\n",
    "- Track flow paths (source → destination lineage)\n",
    "- More debugging capability with minimal storage overhead\n",
    "\n",
    "**5-Minute Window:**\n",
    "- NiFi Status API returns metrics aggregated over the last 5 minutes\n",
    "- Running snapshots every 5 minutes captures distinct time windows\n",
    "- Historical data retained for 24 hours (configurable in NiFi)\n",
    "\n",
    "**Queue Metrics:**\n",
    "- `queued_count`: Current number of FlowFiles waiting in connection\n",
    "- `percent_use_count`: How full the queue is (approaching backpressure threshold)\n",
    "- Helps identify bottlenecks before they cause performance issues\n",
    "\n",
    "## Migration Notes\n",
    "\n",
    "**IMPORTANT:** Running Cell 7 will DROP the existing table to start fresh with the new 24-field schema. This is necessary because:\n",
    "1. Schema changed from 9 fields to 24 fields\n",
    "2. Data model changed from processor-level to connection-level\n",
    "3. Cannot merge old and new data structures\n",
    "\n",
    "**Before running:** If you want to preserve old data, create a backup:\n",
    "```python\n",
    "spark.sql(\"CREATE TABLE main.default.nifi_processor_snapshots_backup AS SELECT * FROM main.default.nifi_processor_snapshots\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nifi_analyzer_databricks_multi_flow_prod",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}